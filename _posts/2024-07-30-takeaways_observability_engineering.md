---
layout: post
title:  ""
title:  "Takeaways from Observability Engineering Book"
categories: ["takeaways", "observability"]
tags: ["takeaways", "observability"]
---

## Abstracts

This post is about my key takeaways from the [Observability Engineering Book](https://www.oreilly.com/library/view/observability-engineering/9781492076438/) by Charity Majors, Liz Fong-Jones, George Miranda.

## What is Observability

For a software application to have observability, you must be able to do the following:

- Understand the inner workings and system state **solely** by observing and interrogating with external tools
- Can continually answer **open-ended** questions about the inner workings of
your applications to explain any anomalies, without hitting investigative dead ends

It is about how people interact with and try to understand their complex systems

## The Role of Cardinality

- In the database context, the cardinality refers to the **uniqueness** of the data values contained in a set.
- Cardinality matters for observability, because **high**-cardinality information (e.g *userId*, *requestId*) is almost always the most useful in **identifying** data for debugging or understanding a system.
- Unfortunately, **metrics**-based tooling systems can deal with only **low**-cardinality dimensions at any reasonable scale[^1].

## The Role of Dimensionality

- The dimensionality refers to the **keys** within that data.
- In observable systems, the telemetry data is generated as an arbitrarily wide(can contains thousands of **key-value pairs**) structured event.
- Imagine that you have an event schema that defines six high-cardinality dimensions per event: **time, app, host, user, endpoint, and status**. With those six dimensions, you can create queries that analyze any **combination** of dimensions to surface relevant patterns that may be contributing to anomalies.

## The Limitations of Metrics as a Building Block

The numerical values generated by a metric reflect an **aggregated** report of system state over a **predefined** period of time that pre-aggregated measure now becomes the **lowest possible level of granularity** for examining system state. That aggregation obscures many possible problems.

## Unstructured Log vs Structured Log

- Unstructured Log is **human readable** but **difficult for machines to process**.
- Structured Log is the **opposite** of unstructured log.

Unstructured logs example:

```
6:01:00 accepted connection on port 80 from 10.0.0.3:63349
6:01:03 basic authentication accepted for user foo
6:01:15 processing request for /super/slow/server
```

Structured log example:

```json
{
  "authority":"10.0.0.3:63349",
  "duration_ms":123,
  "level":"info",
  "msg":"Served HTTP request",
  "path":"/super/slow/server",
  "port":80,
  "service_name":"slowsvc",
  "status":200,
  "time":"2019-08-22T11:57:03-07:00",
  "trace.trace_id":"eafdf3123",
  "user":"foo"
}
```

## References

[^1]: https://grafana.com/blog/2022/02/15/what-are-cardinality-spikes-and-why-do-they-matter/
