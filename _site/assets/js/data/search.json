[
  
  {
    "title": "Fast File System",
    "url": "/posts/fast-file-system/",
    "categories": "os, persistence",
    "tags": "os, persistence",
    "date": "2024-02-25 00:00:00 +0800",
    





    
    "snippet": "Old Unix File SystemStructure  Super Block: information about the entire file system e.g. how big the volume is, how many inodes there are, a pointer to the head of a free list of blocks.  Inode Re...",
    "content": "Old Unix File SystemStructure  Super Block: information about the entire file system e.g. how big the volume is, how many inodes there are, a pointer to the head of a free list of blocks.  Inode Region  Data RegionPoor Performance  The main issue was that the old UNIX file system treated the disk like it was a random-access memory. The data was spread all over the place without regard to the fact that the medium holding the data was a disk, and thus had real and expensive positioning costs.  The free space is managed by free list that a logically contiguous file would be accessed by going back and forth across the disk, thus reducing performance dramatically.          disk defragmentation tool can help by reorganizing on-disk data to place files contiguously.      Fast File System: Disk AwarenessThe idea was to design the file system structures and allocation policies to be “disk aware” and thus improve performance.Organizing structure: the cylinder group  Cylinder is a set of tracks on different surfaces of a hard drive that are the same distance from the center of the drive.a  FFS aggregates N consecutive cylinders into a group, and thus the entire disk can thus be viewed as a collection of cylinder groups.  To improve performance, FFS puts files into the same group for reducing the seek time.  Each group include all file system structures including super block, bitmaps, inode region, and data region.  Super block is duplicated in each group for reliabiltiy.  Use data and inode bitmaps manage free space in a file system because it is easy to find a large chunk of free space and allocate it to a file, perhaps avoiding some of the fragmentation problems of the free list.Policies: How to Allocate Files and DirectoriesKeep related stuff together(same block group), keep unrelated stuff far apart(different block group).Placement of DirectoriesFind a group that  a lower number of allocated directories for balancing directiories across groups.  a high number of free inodes which implies can allocate a lot of files.Then put the inode and data block of the directory to that group.Placement of Files  Allocate the data blocks of a file in the same group as its inode.  Places all files that are in the same directory in the cylinder group of the directory they are in.The Large File exception  Without a different rule, a large file would entirely fill the block group it is first placed within such that it prevents subsequent “related” files from being placed within this block group, and thus may hurt file-access locality.  After some number of blocks are allocated into the first block group, the next large chunk of file will be placed into the next block group , and so on.  How to choose the chunk size? FFS decide it based on the inode structure. The first twelve direct blocks were placed in the same group as the inode. Each subsequent indirect block, and all the blocks it pointed to, were placed in a different group.Internal Fragmentation  FFS designed to to sub-blocks(512 bytes size blocks) to fix the internal fragmentation problem.  The sub-blocks will be allocated to a small file(smaller than 4KB) until the file acquires a full 4KB of data.  At that point, FFS will find a 4KB block, copy the sub-blocks into it, and free the sub-blocks for future use.  Since this introduces additional I/0s, buffer write is used to aviod the sub-block specialization entirely in most cases."
  },
  
  {
    "title": "Crash Consistency Problem",
    "url": "/posts/crash-consistency-problem/",
    "categories": "os, persistence",
    "tags": "os, persistence",
    "date": "2024-02-25 00:00:00 +0800",
    





    
    "snippet": "Crash Consistency Problem  The disk only serve one request at a time.  You have to update two on-disk structures.  If the system crashes or loses power after one write completes, the on-disk struct...",
    "content": "Crash Consistency Problem  The disk only serve one request at a time.  You have to update two on-disk structures.  If the system crashes or loses power after one write completes, the on-disk structure will be left in an inconsistent state.  How to update persistent data structures despite the presence of a power loss or system crash?File System Structure used in following example  Inode and data bitmaps for free space management  Inode Region  Data Region"
  },
  
  {
    "title": "Very Simple File System Implementation",
    "url": "/posts/file-system-implementation/",
    "categories": "os, persistence",
    "tags": "os, persistence",
    "date": "2024-02-24 00:00:00 +0800",
    





    
    "snippet": "A mental model of a File System  Data Structure: what types of on-disk structures are utilized by the file system to organize its data and metadata?  Access methods: How does it map the calls made ...",
    "content": "A mental model of a File System  Data Structure: what types of on-disk structures are utilized by the file system to organize its data and metadata?  Access methods: How does it map the calls made by a process, such as open(), read(), write(), etc., onto its structures?Overall Organization  Block: divide disk to a series of blocks.  Data Region: reserve a fixed portion of the disk for these blocks to store user data.  Inodes: a portion of the disk to store Inode. Inode is a structure for tracking things like which data blocks (in the data region) comprise a file, the size of the file, its owner and access rights, access and modify time.  Allocation Structure: tracks data or inodes blocks are allocated or not. The data structures like free list or bit map can be used.  Superblock: contains the information about this file system e.g. file system type, # of data/inodes blocks. When mounting a file system, the operating system will read the superblock first, to initialize various parameters, and then attach the volume to the file-system tree.InodeIndex nodeInode is short for index node, as the inode number is used to index into an array of on-disk inodes in order to find the inode of that number.Design of the InodeOne of the most important decisions in the design of the inode is how it refers to where data blocks are. One simple approach would be to have one or more direct pointers (disk addresses) inside the inode; each pointer refers to one disk block that belongs to the file.The multi-level indexTo support bigger files, file system designers have had to introduce different structures. One common idea is to have a special pointer known as an indirect pointer. Instead of pointing to a block that contains user data, it points to a block that contains more pointers, each of which point to user data.Extended Approach  A different approach is to use extents instead of pointers. An extent is simply a disk pointer plus a length (in blocks).  All one needs is a pointer and a length to specify the on-disk location of a file.  Just a single extent is limiting, as one may have trouble finding a contiguous chunk of on-disk free space when allocating a file.  Thus, extent-based file systems often allow for more than one extent, thus giving more freedom to the file system during file allocation.Directory Organization  A directory basically just contains a list of directory entries (contains e.g. entry name, inode number pairs).          we can also store entries in B-Tree form.        Each directory has two extra entries, . and ...  A directory has an inode, somewhere in the inode table which its type is directory instead of regular file.Free Space managementUse two bitmaps(one for inode blocks, one for data blocks) for free space management.Reading File from Disk  The file system has to traverse the path name from the root directory where the root directory inode number is “well known” when the file system was mounted.  Once the inode is read in, the FS can look inside of it to find pointers to data blocks, which contain the contents of the root directory. And then find the related entry from the contents.  Recursively traverse the pathname until the desired inode is found.  The FS then does a final permissions check, allocates a file descriptor for this process in the per-process open-file table, and returns it to the user.  Once open, the program can then issue a read() system call to read from the file.  The first read (at offset 0 unless lseek() has been called) will thus read in the first block of the file, consulting the inode to find the location of such a block; it may also update the inode with a new last-accessed time. The read will further update the in-memory open file table for this file descriptor, updating the file offset such that the next read will read the second file block, etc.  Deallocated file descriptor when the file is closed.Writing File to DiskCreate File  Open the file like how we do for reading a file.  One read to the inode bitmap (to find a free inode).  One write to the inode bitmap (to mark it allocated).  One write to the new inode itself (to initialize it).  One to the data of the directory (to link the high-level name of the file to its inode number).  One read and write to the directory inode to update it.Actual WriteFor each data blocl,  One read to the file inode.  One read and write to the data bitmap (to  find a free data block).  One write to the data block.  One write to the file inode (update pointer, # of blocks, etc).Caching and BufferingSpeed up ReadingThe first open may generate a lot of I/O traffic to read in directory inode and data, but subsequent file opens of that same file (or files in the same directory) will mostly hit in the cache and thus no I/O is needed.Static Partitioning  introduced a fixed-size cache to hold popular blocks  ensures each user receives some share of the resource  usually delivers more predictable performance  is often easier to implementDynamic Partiioning  integrate virtual memory pages and file system pages into a unified page cache  achieve better utilization, by letting resource-hungry users consume otherwise idle resources  more complex to implement  can be worse performance for users whose idle resources get consumed by others and then take a long time to reclaim when neededSpeed up WritingMost modern file systems buffer writes in memory for anywhere between five and thirty seconds.  If the system crashes before the updates have been propagated to disk, the updates are lost.  Keeping writes in memory longer, performance can be improved by batching, scheduling, and even avoiding writes.          Batching: e.g. One inode bitmap read write for multiple file creations.      Scheduling: reorder the writes for reducing seek time, rotation time.      Avoiding: e.g. if an process creates a file and then delete that created file, we dont have to write it to disk at all.      "
  },
  
  {
    "title": "File and Directory",
    "url": "/posts/file-and-directory/",
    "categories": "os, persistence",
    "tags": "os, persistence",
    "date": "2024-02-17 00:00:00 +0800",
    





    
    "snippet": "Storage AbstractionsStorage virtualization is based on two main ideas: the file and the directory.  File: A group of bytes that can be read or written. Each file has a low-level name in the form of...",
    "content": "Storage AbstractionsStorage virtualization is based on two main ideas: the file and the directory.  File: A group of bytes that can be read or written. Each file has a low-level name in the form of an inode number.  Directory: the user-readable name and the low-level name are listed in pairs.A file system consists of groups of directories and the files and other directories they contain.The directory hierarchy starts at the root directory and uses a separator to identify sub-directories until the desired file or directory is specified.File System InterfaceCreating, Reading, Writing, Closing FilesA program can create a new file by using open() and passing it the O_CREAT flag. The open() returns a file descriptor, which is useful. If you have permission to read or write to the file, you can use the file descriptor to read or write it using read() or write(). After reading or writing the file, the close() should be used to close the file descriptor so that it no longer refers to any file and can be reused.Reading and Writing, Non-SequentiallyThe OS keeps a current offset for each file a process opens. This tells us where the next read or write will begin. An open file has a current offset that can be modified:  Implicitly:  with each read or write of N bytes, or  Explicitly: With the lseek to reposition the read/write file offset.The offset is stored in the struct file and addressed by the struct proc. Below is a simplified definition of this:struct file {    int ref; // A reference counter    char readable;    char writable;    struct inode *ip; // If a file links to any other files through the struct inode pointer ip    uint off; // A file’s current offset};Shared File Table Entries  In many cases, the mapping of file descriptor to an entry in the open file table is a one-to-one mapping. If some other process reads the same file at the same time, each will have its own entry in the open file table. In this way, each logical reading or writing of a file is independent, and each has its own current offset while it accesses the given file.fork()  Each process’s private descriptor array, the shared open file table entry, and the reference from it to the underlying file-system inode.  When a file table entry is shared, its reference count is incremented; only when both processes close the file (or exit) will the entry be removed.int main(int argc, char *argv[]) {    int fd = open(\"file.txt\", O_RDONLY);     assert(fd &gt;= 0);    int rc = fork();    if (rc == 0) {        rc = lseek(fd, 10, SEEK_SET);        printf(\"child: offset %d\\n\", rc);    } else if (rc &gt; 0) {        (void) wait(NULL);        printf(\"parent: offset %d\\n\", (int) lseek(fd, 0, SEEK_CUR));    }    return 0; }dup()  The dup() call allows a process to create a new file descriptor that refers to the same underlying open file as an existing descriptor.  The dup() and dup2() call is useful when writing a UNIX shell and performing operations like output redirection. For example, if we want to rewrite the output from standard output to a file A, we can  dup_stdout = dup(1) to create a new file descriptor for saving a descriptor that refers to standard output.  close(1) to close the file descriptor 1 so that the file descriptor 1 can be reallocated to the file A.  fd_A = open(A) to open file A and the fd_A file descriptor should be 1 because the function return the smallest free file descriptor(assumed file descriptor 0 is in use).  Then when the process write things to file descriptor 1, it will write to file A.  Finally, we can use dup2(dup_stdout, 1) to make the file descriptor 1 refers to standard output since this is the one which dup_stdout refers to.Writing Immediately With fsync()  The file system will buffer such writes in memory for some time for performance issue. The write(s) will actually be issued to the storage device after few seconds.  To support the applications like DBMS, the interface fsync(int fd) is provided for forcing all dirty (i.e., not yet written) data to disk, for the file referred to by the specified file descriptor fd.File Metadata  Apart from the file content, the file system also stores the metadata of the file. To see the metadata for a certain file, we can use the stat() or fstat() system calls.  Each file system usually keeps this type of information in a on-disk structure called an inode.&gt; stat /dev/null  File: /dev/null  Size: 0               Blocks: 0          IO Block: 4096   character special fileDevice: 5h/5d   Inode: 5           Links: 1     Device type: 1,3Access: (0666/crw-rw-rw-)  Uid: (    0/    root)   Gid: (    0/    root)Access: 2024-02-17 22:49:17.248000219 +0800Modify: 2024-02-17 22:49:17.248000219 +0800Change: 2024-02-17 22:49:17.248000219 +0800 Birth: 2024-02-17 22:49:01.708000001 +0800DirectoriesSystem Calls  opendir()  readdir()  closedir()  rmdir(): the directory be empty (i.e., only has . and .. entries) before it is deleted.Data structurestruct dirent {  char d_name[256];         // filename  ino_t d_ino;              // inode number  off_t d_off;              // offset to the next dirent  unsigned short d_reclen;  // length of this record  unsigned char  d_type;    // type of file};Linkslink()The way link() works is that it creates another name in the directory you are creating the link to, and refers it to the same inode number (i.e., low-level name) of the original file. The file is not copied.unlink()When unlink() is called, it removes the “link” between the human-readable name (the file that is being deleted) to the given inode number, and decrements the reference count. When the reference count reaches zero does the file system also free the inode and related data blocks, and thus truly “delete” the file.Hard LinkHard links are somewhat limited as you can’t create one to a directory, for fear that you will create a cycle in the directory tree. You can’t hard link to files in other disk partitions, etc., because inode numbers are only unique within a particular file system, not across file systems.Symbolic Link  It has its own file type the file system knows about.  It is formed is by holding the pathname of the linked-to file as the data of the link file.  Unlike hard links, removing the original file causes the link to point to a pathname that no longer exists.Permission Bits  3 groups: owner, group, other  3 bits: read, write, execute  Execute Bit:          For regular files, its presence determines whether a program can be run or not.      For directories, it enables a user (or group, or everyone) to do things like change directories (i.e., cd) into the given directory, and, in combination with the writable bit, create files therein.      tim@tim-virtual-machine /tmp&gt; ls -alttotal 100drwxrwxrwt 21 root root 20480 Feb 24 12:05 .drw-rw-r--  2 tim  tim   4096 Feb 18 01:44 testtim@tim-virtual-machine /tmp&gt; cd testcd: Permission denied: “test”tim@tim-virtual-machine /tmp [1]&gt; chmod +x testtim@tim-virtual-machine /tmp&gt; cd testHow to assemble a full directory tree from many underlying file systems?Making File Systemmkfs is used to build a Linux filesystem on a device, usually a hard disk partition. The device argument is either the device name (e.g., /dev/hda1, /dev/sdb2), or a regular file that shall contain the filesystem. The size argument is the number of blocks to be used for the filesystem. and it simply writes an empty file system, starting with a root directory, onto that disk partition.MountOnce such a file system is created, it needs to be made accessible within the uniform file-system tree. This task is achieved via the mount program. For example, after we run mount -t ext3 /dev/sda1 /home/user, the path name /home/user refers to the root of the newly-mounted directory."
  },
  
  {
    "title": "Redundant Array Independent Disk (RAID)",
    "url": "/posts/raid/",
    "categories": "os, persistence",
    "tags": "os, persistence",
    "date": "2024-02-13 00:00:00 +0800",
    





    
    "snippet": "What is RAID?  A hardware RAID is a computer system designed to manage a set of disks.  RAID technology has three basic functions:          Block access to data is achieved by striping the data on ...",
    "content": "What is RAID?  A hardware RAID is a computer system designed to manage a set of disks.  RAID technology has three basic functions:          Block access to data is achieved by striping the data on the disk, reducing the mechanical seek time and increasing data access speed.      Reading multiple disks in an array reduces mechanical seek time and increases data access speed at the same time.      Mirroring and storing parity information helps in data protection.        RAIDs do their job transparent to the systems they are used on, so it looks like a single big disk representing itself as a linear array of blocks. It can replace a disk without changing a single line of software while the OS and user applications continue to work as before.Interface and InternalInterface  A file system sees a RAID as a single disk, a linear array of blocks that the file system or client can read or write to.  The RAID will issue one or more physical I/Os once it knows which disk (or disks) to access.Internal  Microcontrollers that run firmware to control the RAID  Volatile memory (DRAM) to buffer data blocks being read and written  Non-volatile memory (NVM) to buffer writes safely, and  Parity calculations might even be done with special logicEvaluation  Capacity - How much usable space is in N drives with B blocks each available to RAID clients?  Reliability - Tolerance for disk failures for specified design.  Performance - Performance is difficult to evaluate because it really depends on the workload(random, sequential) transmitted to the disk array.  Availability - How available is the system for actual use?RAID Levelshttps://en.wikipedia.org/wiki/Standard_RAID_levels"
  },
  
  {
    "title": "I/O Device",
    "url": "/posts/io-device/",
    "categories": "os, persistence",
    "tags": "os, persistence",
    "date": "2024-02-13 00:00:00 +0800",
    





    
    "snippet": "What is I/O Device?The objects that CPU handles that are not memory, and are connected to a computer system. e.g.  SSD  Keyboard  NetworkThe BusA bus is a system for transferring data between compo...",
    "content": "What is I/O Device?The objects that CPU handles that are not memory, and are connected to a computer system. e.g.  SSD  Keyboard  NetworkThe BusA bus is a system for transferring data between components inside a computer or between computers.Bus Hierarchy for I/O  Some devices, like graphics and high performance I/O devices are connected to the system using a general I/O(faster but higher cost) bus like a PCI.  Peripheral buses(slower but lower cost) like SCSI, SATA, and USB are used to connect slower devices like disks, mice, and keyboards.A Canonical DeviceInterface and Internal  Hardware, like software, has to use some kind of interface that lets the system software control its operation. All devices have a specific interface and protocol for normal interactions.  The internal structure of the device is implementation-specific, and it’s in charge of putting the device’s abstraction to work for the system. Like having a mini-computer dedicated to one task.The Canonical ProtocolInterface  Status Register - Is read to check the devices current status  Command Register - Used to tell the device to do a particular job  Data Register - Used to send/receive data to/from the deviceHow the OS interacts with a deviceWhile (deviceStatus == busy); // wait until device is free// Write some data to DATA register// Write some command to COMMAND register(start device and execute command)While (deviceStatus == busy); // wait until device is finishedLowering CPU Overhead With InterruptsInstead of constantly polling the device, the OS can use an interrupt to:  Send a request  Put the calling process to sleep, and  Switch the context to another job.When the I/O request is finished, it sends a hardware interrupt and makes the CPU go into the OS to a specific interrupt handler. This handler is OS code that will complete the request, wake up the process that’s waiting for I/O to finish, and lets it continue about its merry way.Interrupts aren’t always the best choice: We have to consider the time overhead of interrupt, handle the interrupt, and context switch.  If the device does it’s job fast, polling may be the better option.  If it is slow enough to allow for overlapping, interrupts might be your best bet.Moving Data With DMAProgrammed I/O (PIO)The CPU wastes time manually transporting data between devices using programmed I/O (PIO). Each data item transfer is initiated by an instruction in the program, involving the CPU for every transaction.Direct Memory Access (DMA)A Direct Memory Access (DMA) engine is a system component that handles transfers between devices and main memory without relying so much on the CPU.  The OS tells the DMA engine where and how much data to copy and what device to send data to.  Now, the OS is finished with the transfer and can go on to other tasks.  The DMA engine raises an interrupt when it’s finished, letting the OS know that the transfer is done.Methods Of Device Interaction  Having explicit I/O instructions that define how the OS delivers data to specific device registers.  Memory-mapped I/O:          This makes device registers available like they were memory addresses.      To access a specific register, the OS issues a load (to read) or store (to write) address.      The hardware then delivers the load/store to the device instead of main memory.      How to integrate devices with special interfaces into an OS that we want to make as general as possible?  A file system (or application) doesn’t care about disk class.  It just sends read and write requests to the generic block layer(POSIX: open, read, write, close, etc), which routes them to the appropriate driver(SCSI, ATA, etc).  Certain programs can read and write blocks directly without needing the file abstraction. Most systems support low-level storage management applications using this interface.References  https://en.wikipedia.org/wiki/Programmed_input%E2%80%93output  https://en.wikipedia.org/wiki/Memory-mapped_I/O_and_port-mapped_I/O"
  },
  
  {
    "title": "Hard Disk Drive",
    "url": "/posts/hard-disk-drive/",
    "categories": "os, persistence",
    "tags": "os, persistence",
    "date": "2024-02-13 00:00:00 +0800",
    





    
    "snippet": "What is Hard Disk Drive?Hard disk drives are persistent storage devices for computers.Interface and Internal  Interface: read and write.          includes several sectors (512 -byte blocks) that ea...",
    "content": "What is Hard Disk Drive?Hard disk drives are persistent storage devices for computers.Interface and Internal  Interface: read and write.          includes several sectors (512 -byte blocks) that each one can be read and write.      It’s like an array of n sectors, with an address space ranging from 0 to n−1.      Some types of interfaces: SCSI, SATA, and SAS.        Internal:          A controller: this exports the interface and controls the operation of each request given to the device.      Mechanics: Disk platters, Arm, Head, etc.      Basic Geometry  A disk can have one or more platters.  Platter is a hard circular surface where data is permanently stored by causing magnetic variations.  The platters are connected around a spindle coupled to a motor that spins them at a constant speed measured in rotations per minute (RPM).  Each surface has data encoded in nested circles called tracks.  Clusters(two or more sectors) are subdivided portions of these tracks.  The disk head, one per surface of the drive, does the reading and writing by sensing (i.e., read) or creating a change in (i.e., write) the magnetic patterns on the disk.  One disk arm is attached to the disk head that moves across the surface to put it over the track we want.Read/Write operations  Seek - positioning the read/write head over desired track  Rotation (Rotational Delay) - Waiting for the target sector to rotate under the head  Transfer - performing the read/writePerformance Evaluation  Disk Access Time = Seek Time + Rotation Time + Transfer Time  Disk Response Time = Queue Time + Disk Access TimeTypes of workloads  Random workloads read small (4KB) blocks of data from the disk at random. Database management systems, in particular, use random workloads.  Sequential workloads read several sectors from the disk in a linear fashion. Sequential access patterns are pretty common.I/O schedulingThe disk scheduling algorithm affects the efficiency of our service.  FIFO  Shortest Seek Time First prioritizes requests by their seek time.  Shortest Access Time First prioritizes requests by how close they are to the head’s current location."
  },
  
  {
    "title": "Multi CPU Scheduling",
    "url": "/posts/multi-cpu-scheduling/",
    "categories": "os, concurrency",
    "tags": "os, concurrency",
    "date": "2024-02-10 00:00:00 +0800",
    





    
    "snippet": "Key Questions  How to Schedule Jobs on Multiple CPUs? Strategies and mechanisms for effectively distributing tasks across multiple processing units.  Emerging Challenges in Multiprocessor Schedulin...",
    "content": "Key Questions  How to Schedule Jobs on Multiple CPUs? Strategies and mechanisms for effectively distributing tasks across multiple processing units.  Emerging Challenges in Multiprocessor Scheduling: Identifying and addressing unique issues that arise when transitioning from single to multiple CPU systems.  Load Balancing in Multi-Queue Multiprocessor Schedulers: Methods to achieve optimal workload distribution among CPUs, ensuring efficient utilization of resources.Implications for Applications  Conventional Applications: Standard applications, like a C program, typically utilize only one CPU. Therefore, adding more CPUs doesn’t inherently boost their performance.  Multi-Threaded Applications: These applications can distribute their workload across multiple CPUs, achieving significant performance gains.Role of Caches in Multiprocessor Systems  Caches are small, fast memory units that store copies of data from the system’s main memory, crucial for enhancing processing speed.  Each processor has its own chache but several processors share a single main memory.Challenges in Multiprocessor CachingHow data is shared and cached across multiple CPUs, affecting data access patterns and efficiency.Cache Coherence ProblemThe problem of cache coherence becomes evident when processes migrate between CPUs or when multiple CPUs access the same memory block.  Process Migration: If a program initially running on CPU A switches to CPU B, the latter CPU lacks immediate access to the previously cached data in CPU A’s cache.  Inefficient Data Access: CPU B, lacking the needed data in its cache, must access the main memory, leading to inefficiencies and potential data inconsistencies.  Inconsistency Risks: The situation becomes even more complicated if both CPUs attempt to modify the same memory location, leading to potential data inconsistencies between their caches and the main memory.Addressing Cache Coherence  Modern multiprocessor systems implement hardware-level mechanisms to ensure cache coherence. These mechanisms can detect and manage data that is simultaneously accessed or modified by multiple CPUs.  Operating systems and applications can also be designed to minimize cache coherence issues, for instance, by managing process allocation to CPUs or optimizing memory access patterns.Synchronization in Multiprocessor SystemsEven with cache coherence protocols in place, challenges persist in ensuring consistent and correct data manipulation.Case Study: Shared Linked ListImagine a scenario where two CPU threads concurrently execute a routine to remove an element from a shared linked list:  Thread 1 executes the first line, storing the current head value in its tmp variable.  Thread 2 also executes the same line, capturing the same head value in its separate tmp variable (since tmp is stack-allocated and thus private to each thread).Both threads attempt to remove the same element, leading to potential data corruption or other unintended consequences.To prevent the concurrent access issues, we use mutex locks for the critical section (data manipulation part).Cache Affinity  A multiprocessor scheduler should consider Cache affinity when scheduling, and thus if possible keeping a process on the same CPU.  When a process runs on the same CPU repeatedly, benefiting from the state information accumulated in that CPU’s caches  When a process is frequently shifted between different CPUs, it faces a performance hit. Each CPU switch forces the process to reload its state into a new cache, which is a slower operation.Single-Queue Multiprocessor Scheduling (SQMS)SQMS involves placing all jobs in a single queue, adopting a structure similar to single-processor scheduling.  The use of locks to access the single queue can lead to performance bottlenecks, especially as the number of CPUs increases. The competition for the single lock results in increased overhead and reduced job execution time.  In SQMS, jobs tend to move between CPUs, which goes against the principle of cache affinity.Multi-Queue Multiprocessor Scheduling (MQMS)MQMS addresses the limitations of SQMS by assigning each CPU its own scheduling queue.  Each queue operates independently, significantly reducing the need for synchronization and information sharing across CPUs.  Jobs are more likely to remain on the same CPU across executions, allowing them to benefit from cache affinity  While MQMS supports cache affinity, it requires effective load balancing strategies to ensure that no single CPU is overwhelmed or underutilized.          A technique that move jobs around CPUs which we call migration.      Work stealing is one basic method the system used to decide how to perform such a migration.      Work Stealing  When work-stealing takes place, a queue that is low on jobs occasionally peeks into another queue, to determine how full it is.  The source to “steal” one or more jobs from a target queue to balance the load but this strategy creates friction.  Overlooking other queues causes significant overhead and scaling issues, which was the whole point of introducing MQMS.  Conversely, if you don’t glance at other queues often, you risk significant load imbalances.  Finding the proper threshold is a dark art in system policy design."
  },
  
  {
    "title": "Event Based Concurrency",
    "url": "/posts/event-based-concurrency/",
    "categories": "os, concurrency",
    "tags": "os, concurrency",
    "date": "2024-02-10 00:00:00 +0800",
    





    
    "snippet": "What is Event Based Concurrency?The method of constructing the concurrent servers without relying on threads is event-based concurrency.The Essence of Event-Based Concurrency  Event Monitoring: The...",
    "content": "What is Event Based Concurrency?The method of constructing the concurrent servers without relying on threads is event-based concurrency.The Essence of Event-Based Concurrency  Event Monitoring: The server waits for specific events (like I/O requests) and reacts when they occur.  Minimal Work Execution: Upon an event’s occurrence, the server identifies the event type and performs only the necessary actions, which may include initiating further I/O requests or scheduling future events.Advantages of Event-Based Concurrency  Event-based concurrency sidesteps the concurrency issues like missing locks or deadlocks by handling tasks in response to specific events.  Unlike multi-threaded systems where the operating system manages thread scheduling, event-based concurrency offers developers more control over task execution.A canonical event-based server: event loopPseudo CodeA handler’s processing of an event is the system’s only activity.while (1){  event=getEvents();  for (something in event)    processEvent(e);}How can an event server detect if a message has arrived for it?We can use the select() or poll() system call as API’s when receiving the event. Run man select to check more.The Challenge of Blocking System Calls  System calls such as open() and read() can cause delays, especially if the required data isn’t immediately available in memory. These calls can initiate slow I/O requests to the storage system.  With only a single event loop in action, any event handler making a blocking call causes the entire server to stall.  To maintain efficiency and avoid stalling the entire server, event-based systems must adhere to a crucial rule: avoid making any blocking calls.Asynchronous I/OUnderstanding Asynchronous I/OAsynchronous I/O allows applications to issue I/O requests to the disk system without waiting for the operation to complete.Checking I/O Completion  Frequent Checks: Continuously polling to check the completion of I/O operations can be cumbersome, especially when numerous operations are ongoing.  Interrupt: Using signals to inform applications of completed I/O operations. This method contrasts with the traditional polling approach and is often debated in the context of device management.Implications for Event-Based SystemsIt necessitates a balance between efficient event handling and the complexities of managing asynchronous I/O operations, including the decision between polling and interrupt-driven approaches.State Management and ChallengesManual Stack Management  In thread-based applications, the state is maintained on the thread’s stack. In contrast, event-based systems require explicit state management, often called “manual stack management.”  To handle events like asynchronous I/O, event-based systems use ‘continuations’ - structures that store necessary information for event processing. This approach requires careful management of these continuations to ensure correct program execution.Example Scenario: Handling Asynchronous I/OThread-based ServerIn a thread-based server, reading from a file and writing to a socket is straightforward, with the socket descriptor (sd) readily available on the thread’s stack.int rc = read(fd, buffer, size);rc = write(sd, buffer, size);Event-based ServerIn an event-based server, this operation requires additional steps. After performing an asynchronous read, the server uses a data structure (like a hash table) to link the file descriptor (fd) with the socket descriptor (sd). When the read completes, the server retrieves sd using fd from the data structure and then proceeds with writing to the socket.Other issues on Event Based Concurrency  Scaling on Multi-CPU Systems: Running multiple event handlers concurrently reintroduces synchronization challenges typical in multi-threaded environments.  Incompatibility with System Activities: Event-based methods struggle with activities that inherently block, such as paging. For instance, a page fault in an event handler can halt the entire server, undermining the non-blocking nature of event-based systems."
  },
  
  {
    "title": "Concurrency Bugs",
    "url": "/posts/concurrency-bugs/",
    "categories": "os, concurrency",
    "tags": "os, concurrency",
    "date": "2024-02-10 00:00:00 +0800",
    





    
    "snippet": "Types of Bugs  Non-Deadlock          Atomicity violation bugs.      Order violation bugs.        DeadlockAtomicity violation bugsA code region is intended to be atomic but the atomicity is not enfo...",
    "content": "Types of Bugs  Non-Deadlock          Atomicity violation bugs.      Order violation bugs.        DeadlockAtomicity violation bugsA code region is intended to be atomic but the atomicity is not enforced during execution. For example,Thread 1::if (thd-&gt;proc_info) {    fputs(thd-&gt;proc_info, ...);}Thread 2::thd-&gt;proc_info = NULL;Execution Order:  Thread 1: thd-&gt;proc_info  Thread 2: thd-&gt;proc_info = NULL;  Thread 1: fputs(thd-&gt;proc_info, ...);  Thread 1: crash as a NULL pointer will be dereferenced by fputsThe fix to this type of bug is generally to acquire this lock before accesses the shared structure.Order violation bugsThe desired order between two (groups of) memory accesses is flipped. For example,Thread 1::void init() {      mThread = PR_CreateThread(mMain, ...);}Thread 2::void mMain(...) {    mState = mThread-&gt;State;}Execution Order:  Thread 2: mState = mThread-&gt;State;  Thread 2: crash with NULL-pointer dereferenceThe fix to this type of bug is generally to enforce the order(e.g. use condition variables).DeadlockConditions for deadlock  Mutual exclusion: Threads claim exclusive control of resources that they require (e.g., a thread grabs a lock).  Hold-and-wait: Threads hold resources allocated to them(e.g., locks that they have already acquired) while waiting for additional resources (e.g., locks that they wish to acquire).  No preemption: Resources (e.g., locks) cannot be forcibly removed from threads that are holding them.  Circular wait: There exists a circular chain of threads such that each thread holds one or more resources (e.g., locks) that are being requested by the next thread in the chain.If any of these four conditions are not met, deadlock cannot occur.PreventionCircular wait  To prevent curcular wait, Write your locking code such that you never induce a circular wait.  One way to do that is provide a total ordering on lock acquisition(e.g. always acquire lock 1 before lock 2).  In complext system that involves many locks, partial ordering is useful as well.  But odering just a convention that programmer can be ingored and it requires deep understanding to the code base.Hold-and-wait  To prevent hold-and-wait, acquiring all locks at once, atomically.  It requires that any thread grabs a lock any time it first acquires the global prevention lock.  This approach requires us to know exactly which locks must be held and to acquire them ahead of time.  This technique also is likely to decrease concurrency as all locks must be acquired early on (at once) instead of when they are truly needed.No preemption  To prevent no preemption, we either grabs the lock (if it is available) and returns success or returns an error code indicating the lock is held; in the latter case, you can try again later if you want to grab that lock.  The try-lock approach to allow a developer to back out of lock ownership (i.e., preempt their own ownership) in a graceful way.top:    pthread_mutex_lock(L1);    if (pthread_mutex_trylock(L2) != 0) {        pthread_mutex_unlock(L1);        goto top;    }  But livelock can be occurred that two threads could both be repeatedly attempting this sequence and repeatedly failing to acquire both locks. For example, the livelock is happended if the following execution order keep repeat.  Thread 1: lock(L1)  Thread 2: lock(L2)  Thread 1: try_lock(L2) != 0  Thread 2: try_lock(L1) != 0  Thread 1: unlock(L1)  Thread 2: unlock(L2)  To address the livelock problem, one could add a random delay before looping back and trying the entire thing over again, thus decreasing the odds of repeated interference among competing threads.Mutual Exclusion  To prevent mutual exclusion, we avoid the need for mutual exclusion at all.  E.g. design data structures without lock at all(lock-free or wait-free) with the help of powerful hardware instructions.  But design a lock-freee data structure is non-trival.Avoidance via SchedulingAvoidance requires some global knowledge of which locks various threads might grab during their execution, and subsequently, schedules said threads in a way as to guarantee no deadlock can occur.Detection And Recovery  Allow deadlocks to occur occasionally, and then take some action once such a deadlock has been detected.  Many database systems employ deadlock detection and recovery techniques. A deadlock detector runs periodically, building a resource graph and checking it for cycles. In the event of a cycle (deadlock), the system needs to be restarted.  If more intricate repair of data structures is first required, a human being may be involved to ease the process."
  },
  
  {
    "title": "Semaphore",
    "url": "/posts/semaphore/",
    "categories": "os, concurrency",
    "tags": "os, concurrency",
    "date": "2024-02-09 00:00:00 +0800",
    





    
    "snippet": "What is Semaphore?Semaphores serve as a unified primitive, adept at handling synchronization tasks traditionally managed by both locks and condition variables.Essence of a Semaphore  Semaphores ope...",
    "content": "What is Semaphore?Semaphores serve as a unified primitive, adept at handling synchronization tasks traditionally managed by both locks and condition variables.Essence of a Semaphore  Semaphores operate on a integer value, manipulated by two pivotal routines:          sem_wait(): Decreases the semaphore value by one. If this action would lead to a negative value, the thread must wait.      sem_post(): Increments the semaphore value by one, with the potential to wake a waiting thread if such a thread exists.        A semaphore is initialized with a value that will dictate its forthcoming behavior e.g 1 is promising mutual exclusivity. For threads of a single process, the second argument to remains 0, keeping the semaphore’s scope localized in its process.  It is an atomic operation by leveraging locks and condition variables.Semaphores For OrderingA parent thread must wait for a child thread to completesem_t s;void *child(void *arg) {    printf(\"child\\n\");    sem_post(&amp;s); // Signal: child is done    return NULL;}int main(int argc, char *argv[]) {    sem_init(&amp;s, 0, 0); // Initialize semaphore to 0    printf(\"parent: begin\\n\");    pthread_t c;    Pthread_create(&amp;c, NULL, child, NULL);    sem_wait(&amp;s); // Parent waits for the child    printf(\"parent: end\\n\");    return 0;}Evaluations  If the parent thread executes sem_wait(&amp;s) before the child runs, the semaphore decrement will cause the parent to wait, given its initial value of 0.  If the child complete first and invoke sem_post(&amp;s), the semaphore’s value becomes 1, allowing the parent to bypass waiting and continue execution upon reaching sem_wait(&amp;s).  In both cases, the semaphore’s initial value of 0 ensures that the parent thread will only proceed after the child has signaled its completion.The producer/consumer or bounded buffer problemSemaphores can act as versatile tools to handle this problem, functioning as locks when initialized to 1 and as signaling mechanisms when initialized to 0(a thread called sem_wait() must have to wait another thread to call sem_post() to wake it up). The initialization value of a semaphore typically reflects the count of resources available for distribution at startup.Requirements  Mutual Exclusion: Only one producer can write to a buffer slot at a time, and only one consumer can read from a buffer slot at a time.  Correct Ordering: Producers must wait for available space, and consumers must wait for available data.C++ Implementation#include &lt;stdio.h&gt;#include &lt;pthread.h&gt;#include &lt;semaphore.h&gt;#define MAX 10 // Define the size of the buffer#define loops 20 // Define the number of iterationsint buffer[MAX]; // Shared bufferint fill = 0; // Index for the next item to be producedint use = 0; // Index for the next item to be consumedsem_t empty; // Semaphore indicating the number of empty slotssem_t full; // Semaphore indicating the number of full slotssem_t mutex; // Binary semaphore used as a mutexvoid put(int value) {    buffer[fill] = value;    fill = (fill + 1) % MAX;}int get() {    int tmp = buffer[use];    use = (use + 1) % MAX;    return tmp;}void *producer(void *arg) {    for (int i = 0; i &lt; loops; i++) {        sem_wait(&amp;empty); // Wait for an empty slot        sem_wait(&amp;mutex); // Acquire mutex        put(i); // Produce an item        sem_post(&amp;mutex); // Release mutex        sem_post(&amp;full); // Signal that a new item is available    }    return NULL;}void *consumer(void *arg) {    for (int i = 0; i &lt; loops; i++) {        sem_wait(&amp;full); // Wait for a full slot        sem_wait(&amp;mutex); // Acquire mutex        int tmp = get(); // Consume an item        sem_post(&amp;mutex); // Release mutex        sem_post(&amp;empty); // Signal that a slot is now empty        printf(\"Consumed: %d\\n\", tmp);    }    return NULL;}int main(int argc, char *argv[]) {    // Initialize semaphores    sem_init(&amp;empty, 0, MAX);    sem_init(&amp;full, 0, 0);    sem_init(&amp;mutex, 0, 1);    // Create producer and consumer threads    pthread_t p, c;    pthread_create(&amp;p, NULL, producer, NULL);    pthread_create(&amp;c, NULL, consumer, NULL);    // Wait for threads to complete    pthread_join(p, NULL);    pthread_join(c, NULL);    // Clean up    sem_destroy(&amp;empty);    sem_destroy(&amp;full);    sem_destroy(&amp;mutex);    return 0;}Evaluations  The mutex semaphore is used to protect critical sections within put() and get() for preventing simultaneous access by multiple producers or consumers.  The mutex semaphore is acquired only after successfully passing the empty or full semaphore wait for avoiding the deadlock issue.  If the mutex semaphore is acquired before passing the empty or full semaphore wait, the deadlock occurs when the consumer acquires the mutex and is then blocked waiting for a full signal. Meanwhile, the producer is blocked that it cannot put data and signal full because it cannot acquire the mutex held by the consumer.  The semaphores full and empty are still used to signal the availability of data and space in the buffer.Reader-Writer LockThe need of reader-writer lock  operations like data insertions alter the structure and necessitate exclusive access.  while read operations can often proceed in parallel without such restrictions.C++ Implementation#include &lt;stdio.h&gt;#include &lt;pthread.h&gt;#include &lt;semaphore.h&gt;typedef struct _rwlock_t {    sem_t lock; // Binary semaphore (basic lock)    sem_t writelock; // Allow ONE writer/MANY readers    int readers; // Number of readers in critical section} rwlock_t;void rwlock_init(rwlock_t *rw) {    rw-&gt;readers = 0;    sem_init(&amp;rw-&gt;lock, 0, 1);    sem_init(&amp;rw-&gt;writelock, 0, 1);}void rwlock_acquire_readlock(rwlock_t *rw) {    sem_wait(&amp;rw-&gt;lock);    rw-&gt;readers++;    if (rw-&gt;readers == 1) // First reader gets writelock        sem_wait(&amp;rw-&gt;writelock);    sem_post(&amp;rw-&gt;lock);    printf(\"Reader acquired read lock\\n\");}void rwlock_release_readlock(rwlock_t *rw) {    sem_wait(&amp;rw-&gt;lock);    rw-&gt;readers--;    if (rw-&gt;readers == 0) // Last reader lets it go        sem_post(&amp;rw-&gt;writelock);    sem_post(&amp;rw-&gt;lock);    printf(\"Reader released read lock\\n\");}void rwlock_acquire_writelock(rwlock_t *rw) {    sem_wait(&amp;rw-&gt;writelock);    printf(\"Writer acquired write lock\\n\");}void rwlock_release_writelock(rwlock_t *rw) {    sem_post(&amp;rw-&gt;writelock);    printf(\"Writer released write lock\\n\");}void *reader(void *arg) {    rwlock_t *rw = (rwlock_t *)arg;    rwlock_acquire_readlock(rw);    // Simulate reading operation    rwlock_release_readlock(rw);    return NULL;}void *writer(void *arg) {    rwlock_t *rw = (rwlock_t *)arg;    rwlock_acquire_writelock(rw);    // Simulate writing operation    rwlock_release_writelock(rw);    return NULL;}int main() {    rwlock_t rwlock;    rwlock_init(&amp;rwlock);    pthread_t r1, r2, w1, w2;    pthread_create(&amp;r1, NULL, reader, &amp;rwlock);    pthread_create(&amp;r2, NULL, reader, &amp;rwlock);    pthread_create(&amp;w1, NULL, writer, &amp;rwlock);    pthread_create(&amp;w2, NULL, writer, &amp;rwlock);    pthread_join(r1, NULL);    pthread_join(r2, NULL);    pthread_join(w1, NULL);    pthread_join(w2, NULL);    return 0;}Evaluations  a risk that readers could continually access the data structure, starving writers who are waiting for an opportunity to acquire the write lock.  don’t always result in performance improvement over simpler, faster locking primitives.Thread throttlingThread throttling is a semaphore application in concurrency control, where the goal is to limit the number of threads running simultaneously to prevent system slowdowns.Semaphore as a Solutionsem_t semaphore;sem_init(&amp;semaphore, 0, threshold); // threshold is the max allowed threadsvoid memory_intensive_operation() {    sem_wait(&amp;semaphore); // Enter region    // Perform memory-intensive work    sem_post(&amp;semaphore); // Leave region}Implementing SemaphoresUnlike traditional semaphores, where the negative value indicates the number of waiting threads, in this implementation, the semaphore value never goes below zero. This approach simplifies the implementation and aligns with current practices like those in Linux.typedef struct __Zem_t {    int value;    pthread_cond_t cond;    pthread_mutex_t lock;} Zem_t;// Initialize the semaphorevoid Zem_init(Zem_t *s, int value) {    s-&gt;value = value;    pthread_cond_init(&amp;s-&gt;cond, NULL);    pthread_mutex_init(&amp;s-&gt;lock, NULL);}// Semaphore wait operationvoid Zem_wait(Zem_t *s) {    pthread_mutex_lock(&amp;s-&gt;lock);    while (s-&gt;value &lt;= 0)        pthread_cond_wait(&amp;s-&gt;cond, &amp;s-&gt;lock);    s-&gt;value--;    pthread_mutex_unlock(&amp;s-&gt;lock);}// Semaphore post operationvoid Zem_post(Zem_t *s) {    pthread_mutex_lock(&amp;s-&gt;lock);    s-&gt;value++;    pthread_cond_signal(&amp;s-&gt;cond);    pthread_mutex_unlock(&amp;s-&gt;lock);}Use Zem_t to throttle threadsvoid *memory_intensive_operation(void *arg) {    Zem_t *sem = (Zem_t *)arg;    Zem_wait(sem);    printf(\"Thread entered memory-intensive region\\n\");    sleep(1);    // Simulate memory-intensive work    Zem_post(sem);    printf(\"Thread exited memory-intensive region\\n\");    return NULL;}int main() {    const int MAX_THREADS = 5; // Maximum number of concurrent threads in the region    Zem_t sem;    Zem_init(&amp;sem, 3); // Allowing 3 threads to enter the region simultaneously    pthread_t threads[MAX_THREADS];    for (int i = 0; i &lt; MAX_THREADS; i++) {        pthread_create(&amp;threads[i], NULL, memory_intensive_operation, &amp;sem);    }    for (int i = 0; i &lt; MAX_THREADS; i++) {        pthread_join(threads[i], NULL);    }    return 0;}"
  },
  
  {
    "title": "Condition Variables in Threading",
    "url": "/posts/condition-variables-in-threading/",
    "categories": "os, concurrency",
    "tags": "os, concurrency",
    "date": "2024-02-09 00:00:00 +0800",
    





    
    "snippet": "What is Condition Variables?Condition variables in threading allow threads to wait for certain conditions to be true. When the condition changes, threads can be woken up by signaling the condition ...",
    "content": "What is Condition Variables?Condition variables in threading allow threads to wait for certain conditions to be true. When the condition changes, threads can be woken up by signaling the condition variable.Why we need Condition Variables?It allows a thread to wait for a specific condition without wasting CPU cycles.Mesa vs. Hoare Semantics  In Mesa semantics, signaling indicates a state change without guaranteeing the state upon the thread’s activation.  Hoare semantics ensures immediate action post-signaling but is harder to implement. Most systems opt for Mesa semantics.The Producer/Consumer (Bounded Buffer) ProblemThis problem involves producer threads creating data items and storing them in a share buffer, and consumer threads taking these items from the buffer for use.Synchronization requirementProducers only put data into the buffer when count is buffer is empty and consumers only retrieve data when count is buffer is full.Initial Implementation in C++int loops; // Initialized elsewherecond_t cond;mutex_t mutex;void *producer(void *arg) { for (int i = 0; i &lt; loops; i++) {  Pthread_mutex_lock(&amp;mutex);   if (count == 1)   Pthread_cond_wait(&amp;cond, &amp;mutex);  put(i);  Pthread_cond_signal(&amp;cond);  Pthread_mutex_unlock(&amp;mutex); }}void *consumer(void *arg) { for (int i = 0; i &lt; loops; i++) {  Pthread_mutex_lock(&amp;mutex);   if (count == 0)   Pthread_cond_wait(&amp;cond, &amp;mutex);  int tmp = get();  Pthread_cond_signal(&amp;cond);  Pthread_mutex_unlock(&amp;mutex);  printf(\"%d\\n\", tmp); }}Problem of Initial ImplementationThe issue stems from the buffer state changing after Consumer thread Tc1 is signaled but before it acts since the system use Mesa semantics.Assume there are 1 producer Tp and two consumers Tc1 and Tc2.  Tc1 checked count = 0 so it waits.  Tp puts item since count = 0 and then signals Tc1 to Ready state.  Before Tc1 can consume, Tc2 intervenes and consumes the buffer value.  When Tc1 resumes, it finds an empty buffer, leading to an assertion failure.Implementing a While Loop for Condition VariablesReplacing if statements with while loops in both producer and consumer functions. This ensures that whenever a thread wakes up, it rechecks the shared variable’s state.Unresolved Issue: DeadLock  Both consumers go to sleep as the buffer is empty.  Tp fills the buffer and signals, waking Tc1.  Tp producer keeps take control the CPU and try to add more data, it finds the buffer full and sleeps.  Tc1 consumes the buffer and signals, but if it accidentally wakes Tc2.  Tc2 finds an empty buffer and sleeps again.  Tp also asleep, leaves the system in a deadlock.Implementing Dual Condition Variables  The root of the problem lies in using only one condition variable cond for both full and empty states of the buffer. This can lead to incorrect signaling and subsequent deadlocks in a multi-threaded environment.  So we use two condition variables instead: empty and fill.  Producer threads wait on the empty condition. Once a buffer space is available (empty), they proceed to fill the buffer and then signal fill, indicating the buffer is no longer empty.  Consumer threads wait for the fill condition, meaning the buffer has data. After consuming, they signal empty, indicating space availability in the buffer.Multiple buffer slotsTo further optimize for concurrency and efficiency, we introduce multiple buffer slots. This enhancement allows for multiple values to be produced and consumed without frequent sleeping, reducing context switches and boosting efficiency.Enhanced C++ Implementation#include &lt;stdio.h&gt;#include &lt;pthread.h&gt;int buffer[MAX];int fill_ptr = 0; // Points to where to fill nextint use_ptr = 0;  // Points to where to use nextint count = 0;    // Number of items in the buffervoid put(int value) { buffer[fill_ptr] = value; fill_ptr = (fill_ptr + 1) % MAX; // Circular increment count++;}int get() { int tmp = buffer[use_ptr]; use_ptr = (use_ptr + 1) % MAX; // Circular increment count--; return tmp;}cond_t empty, fill; // Condition variables for synchronizationmutex_t mutex; // Mutex for protecting shared datavoid *producer(void *arg) {    int i;    for (i = 0; i &lt; loops; i++) {        Pthread_mutex_lock(&amp;mutex); // p1: Acquire the mutex for exclusive access        while (count == MAX) // p2: Check if the buffer is full (use a while loop to handle spurious wake-ups)            Pthread_cond_wait(&amp;empty, &amp;mutex); // p3: Wait for the buffer to have space        put(i); // p4: Put the value into the buffer        Pthread_cond_signal(&amp;fill); // p5: Signal that the buffer is no longer empty        Pthread_mutex_unlock(&amp;mutex); // p6: Release the mutex    }}void *consumer(void *arg) {    int i;    for (i = 0; i &lt; loops; i++) {        Pthread_mutex_lock(&amp;mutex); // c1: Acquire the mutex for exclusive access        while (count == 0) // c2: Check if the buffer is empty (use a while loop to handle spurious wake-ups)            Pthread_cond_wait(&amp;fill, &amp;mutex); // c3: Wait for the buffer to have data        int tmp = get(); // c4: Get a value from the buffer        Pthread_cond_signal(&amp;empty); // c5: Signal that the buffer is no longer full        Pthread_mutex_unlock(&amp;mutex); // c6: Release the mutex        printf(\"%d\\n\", tmp); // Print the retrieved value    }}"
  },
  
  {
    "title": "Concurrent Data Structures",
    "url": "/posts/concurrent-data-structure/",
    "categories": "os, concurrency",
    "tags": "os, concurrency",
    "date": "2024-02-07 00:00:00 +0800",
    





    
    "snippet": "Challenges of Concurrency in Data Structures  What are the strategies for adding locks to a data structure to ensure correct, concurrent access?  How can locks be applied in a way that maintains th...",
    "content": "Challenges of Concurrency in Data Structures  What are the strategies for adding locks to a data structure to ensure correct, concurrent access?  How can locks be applied in a way that maintains the speed and allows simultaneous access by multiple threads?Ideal Scenario: Perfect ScalingPerfect scaling is achieved when the time taken for threads to complete tasks on multiple processors is as fast as on a single processor, despite the increase in workload.Concurrent Counter Data Structure by adding lockC++ Implementation#include &lt;stdio.h&gt;#include &lt;pthread.h&gt;// Counter with locktypedef struct __counter_t {    int value;    pthread_mutex_t lock;} counter_t;void init(counter_t *c) {    c-&gt;value = 0;    pthread_mutex_init(&amp;c-&gt;lock, NULL);}void increment(counter_t *c) {    pthread_mutex_lock(&amp;c-&gt;lock);    c-&gt;value++;    printf(\"Incremented: %d\\n\", c-&gt;value);    pthread_mutex_unlock(&amp;c-&gt;lock);}void decrement(counter_t *c) {    pthread_mutex_lock(&amp;c-&gt;lock);    c-&gt;value--;    printf(\"Decremented: %d\\n\", c-&gt;value);    pthread_mutex_unlock(&amp;c-&gt;lock);}int get(counter_t *c) {    pthread_mutex_lock(&amp;c-&gt;lock);    int rc = c-&gt;value;    pthread_mutex_unlock(&amp;c-&gt;lock);    return rc;}void* threadFunction(void* arg) {    counter_t *c = (counter_t*)arg;    for (int i = 0; i &lt; 5; ++i) {        increment(c);    }    for (int i = 0; i &lt; 3; ++i) {        decrement(c);    }    printf(\"Final value in thread: %d\\n\", get(c));    return NULL;}int main() {    counter_t counter;    init(&amp;counter);    pthread_t threads[2];    for (int i = 0; i &lt; 2; ++i) {        pthread_create(&amp;threads[i], NULL, threadFunction, &amp;counter);    }    for (int i = 0; i &lt; 2; ++i) {        pthread_join(threads[i], NULL);    }    printf(\"Final value in main: %d\\n\", get(&amp;counter));    return 0;}Evaluations  Single Lock Bottleneck: A single lock might not be sufficient for high-performance needs. Further optimizations may be required, which will be the focus of the rest of the chapter.  Sufficiency for Basic Needs: If performance isn’t a critical issue, a simple lock might suffice, and no further complexity is necessary.The Approximation Counter Approach  The logical counter is represented by a global counter and one local counter for each CPU core.  Each local counter is synchronized with its own local lock, and a global lock is used for the global counter.  When a thread wants to increment the counter, it updates its corresponding local counter, which is efficient due to reduced contention across CPUs.  Regularly, the value from a local counter is transferred to the global counter by acquiring the global lock. This process involves incrementing the global counter based on the local counter’s value and then resetting the local counter to zero.  The frequency of local-to-global updates is determined by a threshold S. A smaller S makes the counter behave more like a non-scalable counter, while a larger S enhances scalability at the expense of accuracy in the global count.Concurrent Operations in a Linked ListC++ Implementation#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;pthread.h&gt;// Define the structure for a linked list nodetypedef struct node {    int key;    struct node *next;} node_t;// Define the structure for the linked listtypedef struct list {    node_t *head;    pthread_mutex_t lock;} list_t;// Initialize the linked listvoid List_Init(list_t *L) {    L-&gt;head = NULL;    pthread_mutex_init(&amp;L-&gt;lock, NULL);}// Insert a new node with the given key at the beginning of the listvoid List_Insert(list_t *L, int key) {    // Allocate memory for a new node    node_t *new = malloc(sizeof(node_t));    if (new == NULL) {        perror(\"malloc\");        return;    }    new-&gt;key = key;    // Lock the critical section    pthread_mutex_lock(&amp;L-&gt;lock);    // Update the pointers to insert the new node at the beginning    new-&gt;next = L-&gt;head;    L-&gt;head = new;    // Unlock the critical section    pthread_mutex_unlock(&amp;L-&gt;lock);}// Lookup a key in the linked list and return 0 if found, -1 if not foundint List_Lookup(list_t *L, int key) {    int rv = -1;    // Lock the critical section    pthread_mutex_lock(&amp;L-&gt;lock);    // Traverse the linked list to find the key    node_t *curr = L-&gt;head;    while (curr) {        if (curr-&gt;key == key) {            rv = 0; // Key found            break;        }        curr = curr-&gt;next;    }    // Unlock the critical section    pthread_mutex_unlock(&amp;L-&gt;lock);    return rv; // Return 0 for success and -1 for failure}Evaluation  The lock is only held around the critical region of the insert or lookup operations. This makes the implementation more robust.  Assumed malloc() is thread-safe that can be called without a lock, reducing the duration for which the lock is held.  The lookup function has a single exit path for decreasing the likelihood of errors like forgetting to unlock before returning.Hand-Over-Hand Locking in Linked Lists  This approach involves adding a lock to each node of the list. As one traverses the list, they acquire the next node’s lock before releasing the current one.  Conceptually, hand-over-hand locking increases parallelism in list operations. However, in practice, the overhead of locking each node can be prohibitive, often making it less efficient than a single lock, especially for extensive lists and numerous threads. A hybrid method where a lock is acquired for every few nodes might be a more practical solution.Concurrent Queues  A queue involves two separate locks: one for the head and another for the tail.  Allows enqueue operations to primarily use the tail lock and dequeue operations to use the head lock, enabling concurrent execution of these functions.  A dummy node, allocated during the queue’s initialization, separates the head and tail operations, further enhancing concurrency.Concurrent Non-resizable Hash Table  Each hash bucket, represented by a list, has its own lock. This differs from using a single lock for the entire table.  By allocating a lock per hash bucket, the hash table allows multiple operations to occur simultaneously, significantly enhancing its performance."
  },
  
  {
    "title": "Threads Locks",
    "url": "/posts/threads-locks/",
    "categories": "os, concurrency",
    "tags": "os, concurrency",
    "date": "2024-02-05 00:00:00 +0800",
    





    
    "snippet": "What is a LockLock is a tool for allowing running critical section code atomically by only allow one thread to be execute that part of code.How does a lock work?  Lock Variable: e.g. mutex. It hold...",
    "content": "What is a LockLock is a tool for allowing running critical section code atomically by only allow one thread to be execute that part of code.How does a lock work?  Lock Variable: e.g. mutex. It holds the lock state.  It indicates whether the lock is free or acquired by a thread.  Acquiring a Lock: When a thread executes lock(&amp;mutex), it acquires the lock if it’s free, allowing the thread to enter the critical section.  Waiting Threads: If there are threads waiting (blocked in the lock() function), the lock won’t be set to free immediately. Instead, one of the waiting threads will enter the critical section.  Releasing a Lock: Using unlock(&amp;mutex) releases the lock, making it available for other threads.Supports for implementing lock  Hardware: special hardware instructions for implementing locks that are both efficient and effective.  OS: complements these hardware capabilities by providing the necessary context and controls to ensure that locks are used appropriately within the system.Key Criteria for Assessing Lock Efficacy  Basic Functionality: The primary function of a lock is to ensure mutual exclusion, keeping multiple threads out of a critical section simultaneously.  Equal Opportunity: Fairness in locks means each competing thread should have a reasonable chance of acquiring the lock once it’s available.  Avoiding Starvation: Assess whether any thread consistently fails to acquire the lock, leading to starvation.  Overhead Assessment: Consider the time overheads incurred by using the lock in different scenarios:          Single CPU, Single Thread      Single CPU, Multiple Threads      Multiple CPUs, Multiple Threads      Achieving mutual exclusion on single-processor systems by controlling interrupts.Idea  Before entering a critical section, interrupts are disabled to prevent interruption.  After exiting the critical section, interrupts are re-enabled.ProsSimple.Cons  Trust the programs.  Cant ensure mutual exclusion on multi-processor systems.  Prolonged disabling of interrupts might lead to missed signals, causing system issues.  Interrupt masking and unmasking operations can be slow on modern CPUs.Implementing a Flag-Based LockLock Structure and Functionstypedef struct __lock_t { int flag; } lock_t;void init(lock_t *mutex) {    // 0 -&gt; lock is available, 1 -&gt; held    mutex-&gt;flag = 0;}void lock(lock_t *mutex) {    while (mutex-&gt;flag == 1) // TEST the flag        ; // spin-wait (do nothing)    mutex-&gt;flag = 1; // now SET it!}void unlock(lock_t *mutex) {    mutex-&gt;flag = 0;}Issues  Potential for Race Conditions: a critical window between the check while (mutex-&gt;flag == 1) and the set mutex-&gt;flag = 1; so that more than one thread could acquire the lock. For example,          Thread 1: unlock()      Thread 2: mutex-&gt;flag == 1 is false      Thread 3: mutex-&gt;flag == 1 is false      Thread 2: mutex-&gt;flag = 1      Thread 3: mutex-&gt;flag = 1        Spin-wait: consume CPU with do nothing.Spin Lock with Test-And-SetTest-And-SetThe test-and-set instruction is a fundamental piece of hardware support for locking which is the atomic instruction.int TestAndSet(int *old_ptr, int new) {    // logical implementation that should be executed atomically in reality    int old = *old_ptr; // Fetch old value    *old_ptr = new;     // Store 'new' into old_ptr    return old;         // Return the old value}Lock Acquisition  If a thread calls lock() and no other thread holds the lock, TestAndSet(flag, 1) will return 0 (the old value), and the thread acquires the lock while setting the flag to 1.  If another thread already holds the lock (flag is 1), TestAndSet(flag, 1) will return 1, causing the thread to enter a spin-wait loop until the lock is released.Releasing the LockThe unlocking thread sets the flag back to 0, allowing other threads to acquire the lock.Evaluation  achieve mutual exclustion  potential for starvation  waste CPU cycles in the spin-wait loopSpin Lock with Compare-And-SwapCompare-And-Swap (CAS), known as Compare-And-Exchange on x86 architectures, is a hardware primitive provided by some systems to aid in concurrent programming.CAS Functionality in C Pseudocodeint CompareAndSwap(int *ptr, int expected, int new) {    int original = *ptr;    if (original == expected)        *ptr = new;    return original;}  CAS returns the original value at ptr, allowing the calling code to determine whether the update was successful.  atomically updates ptr with new if the value at the address specified by ptr equals expected.CAS Lock Implementionvoid lock(lock_t *lock) {    while (CompareAndSwap(&amp;lock-&gt;flag, 0, 1) == 1)        ; // Spin-wait}Compare with Test-And-Set  Lock acquisition and releasing are the same.  CAS is more flexible than test-and-set, opening possibilities for more advanced synchronization techniques like lock-free synchronization.Spin Lock with Load-Linked and Store-ConditionalLoad-Linked and Store-ConditionalLoad-Linked (LL) and Store-Conditional (SC) are pairs of instructions used on platforms like MIPS, ARM, and PowerPC to create locks and other concurrent structures  Load-Linked (LL): This instruction loads a value from a memory location into a register. It prepares for a subsequent conditional store.  Store-Conditional (SC): This instruction attempts to store a value to the memory location if no updates have been made to that location since the last load-linked operation. It returns 0 on failure, leaving the value unchanged.Lock Implementation in Pseudo C Codevoid lock(lock_t *lock) {    while (LoadLinked(&amp;lock-&gt;flag) ||            !StoreConditional(&amp;lock-&gt;flag, 1))        ; // Spin-wait}When two threads attempt to acquire the lock simultaneously after an LL operation, only one will succeed with the SC. The other thread’s SC will fail, forcing it to retry.Ticket Lock with Fetch-And-AddFetch-And-Add Representation in CThe fetch-and-add instruction atomically increments a value at a specific address while returning the old value.int FetchAndAdd(int *ptr) {    int old = *ptr;    *ptr = old + 1;    return old;}Ticket Lock Representation in C++class TicketLock {  private:    std::atomic&lt;int&gt; ticketCounter;    std::atomic&lt;int&gt; turn;  public:    TicketLock() : ticketCounter(0), turn(0) {}    void lock() {        int myTicket = ticketCounter.fetch_add(1); // Fetch-And-Add        while (turn.load(std::memory_order_relaxed) != myTicket) {            ; // Spin-wait        }    }    void unlock() {        turn.fetch_add(1); // Move to next ticket    }};  A thread acquires a ticket through an atomic fetch-and-add on the ticket counter. This ticket number (ticketCOunter) represents the thread’s place in the queue.  A thread enters the critical section when its ticket number matches the current turn (ticketCounter == turn).  To release the lock, the thread simply increments the turn variable, allowing the next thread in line to enter the critical section.Advantages of Ticket Lock  Guarantee Progress: ensures that all threads make progress. Each thread is assigned a ticket, and they enter the critical section in the order of their tickets.  Fairness: guarantees that each thread will eventually get its turn to enter the critical section.Yield to Avoid Spin WaitingBasic IdeaInstead of spinning, a thread could yield the CPU to another thread through a basic yield() system call, where the yielding thread de-schedules itself, changing its state from running to ready.Challenges  In a system with many threads competing for a lock, yielding can lead to frequent context switches. If one thread acquires the lock and is preempted, the other threads will sequentially find the lock held, yield, and enter a run-and-yield cycle.  The starvation problem is unaddressed.Sleep and Waiting QueueSolaris Implementation in C// Define a structure for the locktypedef struct __lock_t {    int flag;       // 0 if lock is available, 1 if locked    int guard;      // To prevent concurrent guard lock    queue_t *q;     // Queue to hold waiting threads} lock_t;// Initialize the lockvoid lock_init(lock_t *m) {    m-&gt;flag = 0;     // Lock is initially available    m-&gt;guard = 0;    // Guard is initially free    queue_init(m-&gt;q); // Initialize the queue for waiting threads}// Acquire the lockvoid lock(lock_t *m) {    while (TestAndSet(&amp;m-&gt;guard, 1) == 1)        ; // Acquire guard lock by spinning    if (m-&gt;flag == 0) {        m-&gt;flag = 1;   // Lock is acquired        m-&gt;guard = 0;  // Release the guard lock    } else {        queue_add(m-&gt;q, gettid()); // Add current thread to the waiting queue        m-&gt;guard = 0;              // Release the guard lock        park();                     // Put the thread to sleep    }}// Release the lockvoid unlock(lock_t *m) {    while (TestAndSet(&amp;m-&gt;guard, 1) == 1)        ; // Acquire guard lock by spinning    if (queue_empty(m-&gt;q))        m-&gt;flag = 0; // Release the lock; no one wants it    else        unpark(queue_remove(m-&gt;q)); // Wake up the next waiting thread    m-&gt;guard = 0; // Release the guard lock}Addressing Race ConditionsThe race condition can occurs just before the park call. For example,  Thread-1: m-&gt;guard = 0;  Thread-2: unlock(Thread-1);  Thread-1: park();To ensure unpark is called before park, the thread does not sleep unnecessarily by adding setpark() before m-&gt;guard = 0.queue_add(m-&gt;q, gettid());setpark(); // new codem-&gt;guard = 0;Or place the guard directly within the kernel, allowing for atomic operations in lock release and thread dequeuing."
  },
  
  {
    "title": "Thread API",
    "url": "/posts/thread-api/",
    "categories": "os, concurrency",
    "tags": "os, concurrency",
    "date": "2024-02-03 00:00:00 +0800",
    





    
    "snippet": "Thread Creation in POSIXFunctions  create thread: pthread_create  wait a thread to complete: pthread_joinExample Code#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;pthread.h&gt;// De...",
    "content": "Thread Creation in POSIXFunctions  create thread: pthread_create  wait a thread to complete: pthread_joinExample Code#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;pthread.h&gt;// Define structures for argument and return valuestypedef struct {    int a;    int b;} myarg_t;typedef struct {    int x;    int y;} myret_t;// Thread function to perform a task and return valuesvoid *mythread(void *arg) {    myret_t *rvals = (myret_t *)malloc(sizeof(myret_t));  // Allocate memory for return values    // Simulate some work and set return values    rvals-&gt;x = 1;    rvals-&gt;y = 2;    return (void *)rvals;  // Return the allocated struct}int main(int argc, char *argv[]) {    pthread_t p;          // Thread object    myret_t *rvals;       // Pointer to store returned values    myarg_t args = {10, 20};  // Example argument values    // Create a new thread with the specified function and arguments    pthread_create(&amp;p, NULL, mythread, &amp;args);    // Wait for the thread to finish and retrieve its return values    pthread_join(p, (void **) &amp;rvals);    // Print the returned values    printf(\"returned %d %d\\n\", rvals-&gt;x, rvals-&gt;y);    free(rvals);  // Free the allocated memory    return 0;}Mutual Exclusion with POSIX ThreadsMutexes are essential for protecting critical sections of code to ensure correct operation.Usage Examplepthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER; // Static Initialization to set mutex to default state.// int rc = pthread_mutex_init(&amp;lock, NULL); // Dynamic Initialization with NULL indicating default attributes.// assert(rc == 0); // Check for successful initializationpthread_mutex_lock(&amp;lock); // Acquire the lock. If the lock is already held by another thread, the calling thread will block until it can acquire the lock.x = x + 1; // Critical sectionpthread_mutex_unlock(&amp;lock); // Release the lockpthread_mutex_destroy(&amp;lock); // Clean upAdvanced Mutex Operations  Timed Locks: Acquire a lock with a timeout, useful in scenarios where avoiding deadlock is crucial.  Try Locks: Non-blocking lock attempts that can be useful in certain advanced programming scenarios.Condition variablesThey are used when threads need to signal each other to proceed with their tasks.Example C++ Code#include &lt;iostream&gt;#include &lt;pthread.h&gt;#include &lt;unistd.h&gt; // For sleep()pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER; // Mutexpthread_cond_t cond = PTHREAD_COND_INITIALIZER;  // Condition variable// Shared variablebool ready = false;// Thread function that waits for the conditionvoid* wait_thread(void* arg) {    pthread_mutex_lock(&amp;lock);    while (!ready) { // Using while loop for spurious wake-ups        std::cout &lt;&lt; \"Waiting thread is waiting for the condition...\" &lt;&lt; std::endl;        pthread_cond_wait(&amp;cond, &amp;lock);    }    std::cout &lt;&lt; \"Waiting thread received the signal.\" &lt;&lt; std::endl;    pthread_mutex_unlock(&amp;lock);    return NULL;}// Thread function that signals the conditionvoid* signal_thread(void* arg) {    sleep(1); // Sleep for demonstration purposes    pthread_mutex_lock(&amp;lock);    ready = true;    std::cout &lt;&lt; \"Signaling thread is signaling the condition...\" &lt;&lt; std::endl;    pthread_cond_signal(&amp;cond);    pthread_mutex_unlock(&amp;lock);    return NULL;}int main() {    pthread_t waitThread, signalThread;    // Create threads    pthread_create(&amp;waitThread, NULL, wait_thread, NULL);    pthread_create(&amp;signalThread, NULL, signal_thread, NULL);    // Wait for threads to finish    pthread_join(waitThread, NULL);    pthread_join(signalThread, NULL);    // Clean up    pthread_mutex_destroy(&amp;lock);    pthread_cond_destroy(&amp;cond);    return 0;}  Hold Lock During Signaling: Always hold the lock when signaling or modifying the global condition variable to avoid race conditions.  Lock Handling in Wait and Signal: The wait function requires the lock as it releases it when putting the thread to sleep. The lock is re-acquired before pthread_cond_wait returns. The signal function only needs the condition variable.  Rechecking the Condition: Use a while loop rather than an if statement to recheck the condition. This is because some implementations may wake up threads spuriously, without the condition actually being met.Caution Against Using Flags for Synchronization// Waiting codewhile (ready == 0) ; // Spin// Signaling codeready = 1;This approach can lead to excessive CPU usage (busy-waiting) and is prone to synchronization errors."
  },
  
  {
    "title": "Concurrency and Threads",
    "url": "/posts/concurrency-and-threads/",
    "categories": "os, concurrency",
    "tags": "os, concurrency",
    "date": "2024-02-03 00:00:00 +0800",
    





    
    "snippet": "Threads  Each thread is like an independent worker, but they all share the same memory space.  Each thread has its own Program Counter(PC) to keeps track of where a thread is in its execution.  Jus...",
    "content": "Threads  Each thread is like an independent worker, but they all share the same memory space.  Each thread has its own Program Counter(PC) to keeps track of where a thread is in its execution.  Just like a process has a control block to keep track of its state, each thread has its own Thread Control Block(TCB).  Each thread has its own stack. This means each thread can handle its own functions and data, keeping them in its personal stack.  Context switch is allowed by saving and loading different sets of registers for each thread.Why use threads?They are useful on parallelism and handling I/O (Input/Output) operations.ParallelismIn multi-processor system, we can speed up the process by dividing the task(e.g. adding numbers in a huge array) among multiple threads, with each processor handling a part of the job.Avoid delays caused by I/O operationsIf a program only used one thread and had to wait for an I/O operation to complete, it couldn’t do anything else during that time. By using multiple threads, one thread can handle the I/O operation while others continue processing or initiating more I/O requests.Why not just use multiple processes instead of threads?Threads have a special advantage: they share the same address space. This makes data sharing between threads straightforward and efficient, especially suitable for tasks that are closely related.Race Condition  When multiple threads access and modify the same data, unpredictable outcomes can occur because of the uncontrollable threads scheduling.  For example, in order to increment an counter, 3 machine codes will be generated by compiler.          load counter value from memory to register(e.g. eax).      increment the register.      store back the updated register value to counter memory location.        Two threads (Thread 1 and Thread 2) executing this sequence concurrently so that the code executed twice, but the counter incremented only once.          Thread 1: loading the counter value(10) into eax and increments it to 11.      Context Switch to Thread 2.      Thread 2: loading the counter value(10) into eax and increments it to 11, store its eax value(11) back to memory.      Context Switch to Thread 1.      Thread 1: store its eax value(11) back to memory.      Critical SectionFrom the above code sequence example, we can see that increment an counter is an critical section that the code that accesses a shared resource and should not be executed by more than one thread at a time.Mutual ExclusionTo prevent race conditions, we need mutual exclusion in critical sections. If one thread operates within the critical part, the others are stopped.Atomicity  In multi-threading, a major challenge is ensuring that certain operations are executed without interruption, maintaining consistency in shared data.  add instruction is one of an atomic instructions.  In practice, we often don’t have such atomic instructions for complex operations in regular instruction set and we have to break it down to multiple instructions.  Since we can’t rely on hardware for atomicity, we turn to synchronization primitives."
  },
  
  {
    "title": "VMS Lazy Optimizations",
    "url": "/posts/vms-lazy-optimizations/",
    "categories": "os, memory",
    "tags": "os, memory",
    "date": "2024-02-02 00:00:00 +0800",
    





    
    "snippet": "Demand Page Zeroing  Consider adding a page to the address space, say in the heap.  The OS have to zero the page for security reason so that the process cannot know how this page used for.  Instead...",
    "content": "Demand Page Zeroing  Consider adding a page to the address space, say in the heap.  The OS have to zero the page for security reason so that the process cannot know how this page used for.  Instead of the OS zeros the page before maps it into your address space, the OS  the OS just adds an entry to the page table marking it unavailable.  When the process reads or writes the page, it traps the OS. A demand-zero page is identified by the OS using certain bits designated in the “reserved for OS” portion of the page table entry.  The OS then finds a physical page, zeroes it, and maps it into the process’s address space.  This task(zeros the page) is avoided if the process never accesses the page.Copy-On-Write  Instead of copying a page from one address space to another, the OS can map it into the target address space and declare it read-only in both address spaces.  If both address spaces just read the page, no data is moved.  A page write attempt from one of the address spaces will be logged into the OS. The OS then allocate a new page, fill it with data, and map it into the address space of the faulting process.  So it saves unnecessary copying.References  VMS - https://en.wikipedia.org/wiki/OpenVMS"
  },
  
  {
    "title": "Page Swapping",
    "url": "/posts/page-swapping/",
    "categories": "os, memory",
    "tags": "os, memory",
    "date": "2024-01-30 00:00:00 +0800",
    





    
    "snippet": "What is page swappingSwap pages to disk so that the running programs to use more RAM than is physically accessible.Swap Space  Swap space is reserved space on the disk for moving pages between memo...",
    "content": "What is page swappingSwap pages to disk so that the running programs to use more RAM than is physically accessible.Swap Space  Swap space is reserved space on the disk for moving pages between memory and the file system.  This assumes the OS can read and write to swap space in page-sized units.The free CommandThe free command displays amount of free and used memory in the system.               total        used        free      shared  buff/cache   availableMem:         8086120     2908832      577556       56540     4599732     4815504Swap:        2097148          12     2097136  In the “Mem” or memory row, there is more “available” space than “free” because there are pages the system knows it can get rid of if needed.  The “Swap” row which reports the usage of the swap space as distinct from your memory.Swapping Mechanism  In a software-managed TLB architecture, the OS determines if a page exists in physical memory using a new piece of information in each page-table entry called the present bit.  A page fault occurs when a program accesses a page that isn’t in physical memory.  A page fault will require the OS to swap in a page from disk.          use the PTE’s data bits, like the page’s PFN, to store a disk address.      Once the page is located on the disk, it is swapped into memory via I/O. The process will be blocked while the I/O is running, so the OS can run other ready processes while the page fault is handled.      The OS will update the page table to reflect the new page, update the PFN field of the page-table entry (PTE) to reflect the new page’s address in memory, and retry the instruction.      When to swap out(swap page to disk)?High/Low WatermarkWhen the OS detects that there are more pages in memory than the high watermark (HW), a background process called the swap daemon  starts to evict pages from memory until the number of pages is less than the low watermark (LW). The daemon then sleeps until the HW is reached again.Invoke by ProcessThe swap can also be awoken by a process if there are no free pages available; Once the daemon has freed up some pages, it will re-awaken the original thread, which will then be able to page in the appropriate page and continue working.Performancement OptimizationMany systems, will cluster or group a number of pages and write them out to the swap partition all at once.Other useful commands  vmstatImplementing LRUProblemScanning a wide array of times to discover the least-recently-used page is expensive.Approximating LRU  When a page is referenced (read or written), the hardware sets the use bit to 1.  The system’s pages organized in a circle.  Initially, a clock hand points to any page.  When replacing a page, the OS checks if the use bit is 1 or 0.          If 1, page P was recently used. The usage bit for P is cleared (set to 0), and the clock hand is advanced one page (P + 1).      If the use bit is set to 0, the page is evicted (in the worst case, all pages have been recently used and we have now searched through the entire set of pages, clearing all the bits).      Dirty Pages  The clock algorithm may be altered to look for pages that are both unused and clean to evict first; if those aren’t found, then look for unused dirty pages, and so on.  Because if a page has been updated and is thus unclean, it must be evicted by writing it back to disk, which is costly.  The eviction is free if it has not been updated; the physical frame can simply be reused for other purposes without further I/O.  A modified/dirty bit should be included in the hardware to accommodate this behavior.Thrashing  Thrashing is used to describe the system is continuously paging because the memory demands of the operating processes simply outnumber the physical memory available.  The methods to address thrashing          not to execute a subset of them in the hopes that the pages of the reduced set of processes will fit in memory, allowing progress.      launch an out-of-memory killer; this daemon selects a memory-intensive process and kills it.      "
  },
  
  {
    "title": "Memory Space Management with paging",
    "url": "/posts/memory-space-management-with-paging/",
    "categories": "os, memory",
    "tags": "os, memory",
    "date": "2024-01-28 00:00:00 +0800",
    





    
    "snippet": "What is PagingPaging is another memory space management approach that dividing memory into fixed size of chuncks called pages. In contrast to segmentation, paging does not have external fragmentati...",
    "content": "What is PagingPaging is another memory space management approach that dividing memory into fixed size of chuncks called pages. In contrast to segmentation, paging does not have external fragmentation and support the abstraction of an address space effectively, regardless of how a process uses the address space since it won’t make assumptions about the way the heap and stack grow and how they are use.Address TranslationTo translate the virtual address the process generates:  We have to break the resulting virtual address into two parts:          The virtual page number (VPN) and      The offset within the page.        Using our VPN, we can now index our page table and find out which physical frame virtual page lives in.Page Table  The page table is a data structure that maps virtual addresses (or virtual page numbers) into physical addresses (physical frame numbers).  Each process has its own page table.Linear Page TableLinear Page table is an array.  VPN is an index of the array.  Each page table entry(PTE) contains PFN and other useful bits.The steps of address translation by hardware// Extract the VPN from the virtual addressVPN = (VirtualAddress &amp; VPN_MASK) &gt;&gt; SHIFT// Form the address of the page-table entry (PTE)PTEAddr = PTBR + (VPN * sizeof(PTE))// Fetch the PTEPTE = AccessMemory(PTEAddr)// Check if process can access the pageif (PTE.Valid == False)    RaiseException(SEGMENTATION_FAULT)else if (CanAccess(PTE.ProtectBits) == False)    RaiseException(PROTECTION_FAULT)else    // Access is OK: form physical address and fetch itoffset = VirtualAddress &amp; OFFSET_MASKPhysAddr = (PTE.PFN &lt;&lt; PFN_SHIFT) | offsetRegister = AccessMemory(PhysAddr)"
  },
  
  {
    "title": "Advanced Page Table",
    "url": "/posts/advanced-page-table/",
    "categories": "os, memory",
    "tags": "os, memory",
    "date": "2024-01-28 00:00:00 +0800",
    





    
    "snippet": "Problem of Linear Page Table  The size of the page table is too big.  We need to allocate the physical memory for the page table entries that there is no physical frame as well.Larger Page size  On...",
    "content": "Problem of Linear Page Table  The size of the page table is too big.  We need to allocate the physical memory for the page table entries that there is no physical frame as well.Larger Page size  One way to make the page table smaller is make the page size larger because it makes the number of page table entries to be decreased.  The major drawback to this strategy is that large pages result in waste within each page, a problem known as internal fragmentation. Applications allocate pages but only use small portions of each, and memory quickly fills up with these excessively large pages.Combining Paging and segmentation  The goal of combining paging and segmentation is removing the erroneous entries between the heap, and stack segments(unallocated pages between the stack and the heap are no longer needed) to reduce the size of page table.  Instead of one page table for the process’s whole address space, we have one page table for each segments(code, heap, and stack) so we have three page tables.  The base register holds the physical address of the segment’s page table.  The limits register indicates the page table’s end (i.e., how many valid pages it has).  During a context switch, these registers must be updated to reflect the new process’s page tables.  On a TLB miss (assuming a hardware-managed TLB), the hardware utilizes the segment bits (SN) to identify which base and bounds pair to use. The hardware then combines the physical address with the VPN to generate the page table entry (PTE) address.SN           = (VirtualAddress &amp; SEG_MASK) &gt;&gt; SN_SHIFTVPN          = (VirtualAddress &amp; VPN_MASK) &gt;&gt; VPN_SHIFTAddressOfPTE = Base[SN] + (VPN * sizeof(PTE))Virtual Address:|****SN*****|*****VPN******|*****OFFSET******|  The downsides of this approach:          Use segmentation so assumes a fixed address space utilization pattern; a huge but sparsely used heap      External fragmentation: page tables can now be any size (in multiples of PTEs). Finding memory space for them is more difficult.      Multi-Level Page Tables  This is approach to get rid of all those incorrect sections in the page table without using segmentation.  It first divide the page table into page-sized units.  If all page-table entries (PTEs) of that page are invalid, then do not assign that page of the page table at all.  So, it’s generally compact and supports sparse address spaces.  To know the memory location of the pages of the page table and their validities, it use the new data structure called page directory.  When the OS wants to allocate or grow a page table, it may simply grab the next free page-sized unit(the size is much smaller than the size of page table).  But, on a TLB miss, two loads from memory are necessary to acquire the proper translation information from the page table (one for the page directory, and one for the PTE itself).  For 2-level page table, to find out the page table entry, we can use base pointer + PD.index * sizeof(page directory) to find out the address of page-sized unit = PD.PFN , then we use PD.PFN + PT.index * sizeof(PTE) to find out the PTE address.|******** VPN **********************|****** Offset *********|| 13 | 12 | 11 | 10 | 9 | 8 | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0 ||**** PD.Index *****|** PT.index ***|Inverted Page Table  Rather than having many page tables (one for each system process), we have a single page table with an item for each physical page of the system.  This entry indicates which process uses this page and which virtual page of that process corresponds to this physical page.  A hash table is frequently added on top of the underlying structure to speed up lookups.Swapping the Page Tables to DiskSome systems store page tables in kernel virtual memory, allowing the system to swap portions of these page tables to disk if memory becomes scarce."
  },
  
  {
    "title": "Transaction Lookaside Buffer",
    "url": "/posts/Transaction-Loodaside-Buffer/",
    "categories": "os, memory",
    "tags": "os, memory",
    "date": "2024-01-28 00:00:00 +0800",
    





    
    "snippet": "Problem of PagingPaging needs an additional memory lookup in order to translate each virtual address, but it takes too long to obtain translation information before every instruction fetch, load, o...",
    "content": "Problem of PagingPaging needs an additional memory lookup in order to translate each virtual address, but it takes too long to obtain translation information before every instruction fetch, load, or store.What is Transaction Lookaside Buffer?In order to speed up the process of address translation, we use the hardware cache for the address translation. This cache is called Transaction Lookaside Buffer(TLF) which is part of the MMU.TLB EntryTLBs generally include 32 or 64 of TLB entries. A few are for the OS (using the G bit). The OS can set a wired register to instruct the hardware how many TLB slots to reserve for it. The OS uses these reserved mappings for code and data it needs to access during key moments when a TLB miss would be troublesome (e.g., in the TLB miss handler).TLB Entry:  Virtual Page Number (VPN)  Process ID (PID) or Address Space ID(ASID)  Page Frame Number (PFN)  V bit: Valid bit - indicates whether the entry has a valid translation or not  G bit: Global bit - If set, TLB does not check PID for translation  …What if TLB missHardware approach  If the virtual address does not in the TLB entries, we have to check the page table to find the translation.  The hardware has to know the exact location of the page tables in memory (through a page-table base register)OS approach  The hardware mimics an exception, pausing the current instruction stream, switching to kernel mode, and jumping to a trap handler.  Returning from a TLB miss-handling trap causes the hardware to retry the instruction, resulting in a TLB hit.  OS must avoid creating endless loops of TLB misses by keeping the TLB miss handler in physical memory.          reserve some TLB entries for always valid transaction. Or      unmapped and not subject to address translation.        OS can use any data structure it wants to implement the page table.Array Access ExampleA memory array of 10 4-byte integers.The page size is 16 bytes.            VPN      Offset 0-4      Offset 5-8      Offset 9-12      Offset 13-16                  VPN 0                           arr[0]              VPN 1      arr[1]      arr[2]      arr[3]      arr[4]              VPN 2      arr[5]      arr[6]      arr[7]      arr[8]              VPN 3      arr[9]                           The TLB hit rate the first time the array is accessedThe hit rate is 60%.  arr[0]: Miss (VPN 0 stored in TLB)  arr[1]: Miss (VPN 1 stored in TLB)  arr[2]: Hit (VPN 1)  arr[3]: Hit (VPN 1)  arr[4]: Hit (VPN 1)  arr[5]: Miss (VPN 2 stored in TLB)  arr[6]: Hit (VPN 2)  arr[7]: Hit (VPN 2)  arr[8]: Hit (VPN 2)  arr[9]: Miss (VPN 3 stored in TLB)The TLB hit rate the second time the array is accessedThe hit rate is 100% because VPN 0-4 stored in TLB already in the first time access.Context SwitchingHow to make sure the process does not reuse the TLB entries of the old process?  flushing: clears the TLB by setting all valid bits to 0.  ASID: TLBs include an address space identifier (ASID) field. The ASID is a Process ID (PID) with less bits. So the TLB can hold several processes’ translations.Two entries for two processes with two VPNs point to the same physical pageWhen two processes share a page (for example, a code page), this can occur. Also, it reduces memory overheads by reducing the number of physical pages needed.            VPN      PFN      ASID      Prot-bit      Valid-bit                  VPN 0      PFN 100      1      r-x      1              VPN 5      PFN 100      2      r-x      1      TLB Replacement Policy  LRU  Random"
  },
  
  {
    "title": "Memory Segmentation",
    "url": "/posts/memory-segmentation/",
    "categories": "os, memory",
    "tags": "os, memory",
    "date": "2024-01-27 00:00:00 +0800",
    





    
    "snippet": "What is Memory Segmentation?  It allows the segments(code, stack, heap) of the address space can be stored in different physical memory locations so that we do not need to allocate the physical mem...",
    "content": "What is Memory Segmentation?  It allows the segments(code, stack, heap) of the address space can be stored in different physical memory locations so that we do not need to allocate the physical memory for the “free” segment.  Each segment has its own base/bound registers.Which segment the virtual memory address related to?Explicit Approach  we divide the address space into segments based on the first few bits of the virtual address.  the top 2 most bits represent which segment the address corresponds to.  the other bits represent the offset.// get top 2 bits of 14-bit VASegment = (VirtualAddress &amp; SEG_MASK) &gt;&gt; SEG_SHIFT// now get offsetOffset  = VirtualAddress &amp; OFFSET_MASKif (Offset &gt;= Bounds[Segment])  RaiseException(PROTECTION_FAULT)else  PhysAddr = Base[Segment] + Offset  Register = AccessMemory(PhysAddr)Implicit Approach  determines the segment by examining the address.  If the address came from the program counter (i.e., an instruction fetch), it’s in the code segment.  If it came from the stack or base pointer, it’s in the stack segment.  All others are in the heap.How to handle stack  The difference between the stack and the other segment is it now grows backwards (towards lower addresses).  We need more hardware support so that the hardware knows the segment grows positive or negative from the base address.  We can get the correct physical address by base address + offset - max segment size.            Segment      Base      Size (Max 4K)      Grows Positive?                  Code      32K      2K      1              Heap      34K      3K      1              Code      28K      2K      0      Segmentation presents new challenges for the OS  The segment registers must be saved and restored becase each process has its own virtual address space for context switch.  Able to update the segment size register to the new (larger/smaller) size.  Able to find physical memory space for new address spaces.  handle external fragmentation: physical memory soon fills up with pockets of free space, making it impossible to assign new segments or expand old ones."
  },
  
  {
    "title": "Why Memory Virtualisation?",
    "url": "/posts/why-memory-virtualisation/",
    "categories": "os, memory",
    "tags": "os, memory",
    "date": "2024-01-26 00:00:00 +0800",
    





    
    "snippet": "  Transparency: user program dont need to worry about the physical memory address.  Protection: each process only can manipulate its own address space.",
    "content": "  Transparency: user program dont need to worry about the physical memory address.  Protection: each process only can manipulate its own address space."
  },
  
  {
    "title": "Multilevel Feedback Queue Scheduling",
    "url": "/posts/mlfq-scheduling-policy/",
    "categories": "os, process",
    "tags": "os, process",
    "date": "2024-01-25 00:00:00 +0800",
    





    
    "snippet": "Example RunEnter the number of processes: 3Enter duration for process 1: 10Process 1 enqueued in queue with time quantum 2Enter duration for process 2: 20Process 2 enqueued in queue with time quant...",
    "content": "Example RunEnter the number of processes: 3Enter duration for process 1: 10Process 1 enqueued in queue with time quantum 2Enter duration for process 2: 20Process 2 enqueued in queue with time quantum 2Enter duration for process 3: 30Process 3 enqueued in queue with time quantum 2Process 1 dequeued from queue with time quantum 2Process 1 is running in high priority queueProcess 1 enqueued in queue with time quantum 4Process 2 dequeued from queue with time quantum 2Process 2 is running in high priority queueProcess 2 enqueued in queue with time quantum 4Process 3 dequeued from queue with time quantum 2Process 3 is running in high priority queueProcess 3 enqueued in queue with time quantum 4Process 1 dequeued from queue with time quantum 4Process 1 is running in medium priority queueProcess 1 enqueued in queue with time quantum 8Process 2 dequeued from queue with time quantum 4Process 2 is running in medium priority queueProcess 2 enqueued in queue with time quantum 8Process 3 dequeued from queue with time quantum 4Process 3 is running in medium priority queueProcess 3 enqueued in queue with time quantum 8Process 1 dequeued from queue with time quantum 8Process 1 is running in low priority queueProcess 1 finished executionProcess 2 dequeued from queue with time quantum 8Process 2 is running in low priority queueProcess 2 enqueued in queue with time quantum 8Process 3 dequeued from queue with time quantum 8Process 3 is running in low priority queueProcess 3 enqueued in queue with time quantum 8Process 2 dequeued from queue with time quantum 8Process 2 is running in low priority queueProcess 2 finished executionProcess 3 dequeued from queue with time quantum 8Process 3 is running in low priority queueProcess 3 enqueued in queue with time quantum 8Process 3 dequeued from queue with time quantum 8Process 3 is running in low priority queueProcess 3 finished executionProcess Duration        Waiting Time    Turnaround Time1       10      12      222       20      24      443       30      30      60Example C Code#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;typedef struct {  int id;  int duration;  int remaining_time;  int waiting_time;  int turnaround_time;} Process;typedef struct {  Process* processes;  int front;  int rear;  int time_quantum;} Queue;void init_queues(Queue* high_q, Queue* mid_q, Queue* low_q){  high_q-&gt;processes = malloc(10*sizeof(Process));  mid_q-&gt;processes = malloc(10*sizeof(Process));  low_q-&gt;processes = malloc(10*sizeof(Process));  high_q-&gt;time_quantum = 2;  mid_q-&gt;time_quantum = 4;  low_q-&gt;time_quantum = 8;  high_q-&gt;front = -1;  mid_q-&gt;front = -1;  low_q-&gt;front = -1;  high_q-&gt;rear = -1;  mid_q-&gt;rear = -1;  low_q-&gt;rear = -1;}void enqueue(Queue* q, Process* p){  printf(\"Process %d enqueued in queue with time quantum %d\\n\", p-&gt;id, q-&gt;time_quantum);  if(q-&gt;front &gt; 9) { return; }  q-&gt;front += 1;  q-&gt;processes[q-&gt;front].id = p-&gt;id;  q-&gt;processes[q-&gt;front].duration = p-&gt;duration;  q-&gt;processes[q-&gt;front].remaining_time = p-&gt;remaining_time;  q-&gt;processes[q-&gt;front].waiting_time = p-&gt;waiting_time;  q-&gt;processes[q-&gt;front].turnaround_time = p-&gt;turnaround_time;}Process* dequeue(Queue* q){  if(q-&gt;rear &gt;= q-&gt;front) {     q-&gt;rear = -1;    q-&gt;front = -1;    return NULL;  }  q-&gt;rear += 1;  printf(\"Process %d dequeued from queue with time quantum %d\\n\", q-&gt;processes[q-&gt;rear].id, q-&gt;time_quantum);  return &amp;q-&gt;processes[q-&gt;rear];}void mlfq(Process* processes, int n, Queue* high_q, Queue* mid_q, Queue* low_q) {  Process* current_p;  int total_turnaround_time = 0;  if (n&lt;= 0) { return; }  while(1){    current_p = dequeue(high_q);    if(current_p != NULL){      printf(\"Process %d is running in high priority queue\\n\", current_p-&gt;id);      if (current_p-&gt;duration &gt; high_q-&gt;time_quantum) {        current_p-&gt;duration -= high_q-&gt;time_quantum;        enqueue(mid_q, current_p);        total_turnaround_time += high_q-&gt;time_quantum;      } else {        total_turnaround_time += current_p-&gt;duration;        processes[current_p-&gt;id-1].turnaround_time = total_turnaround_time;        processes[current_p-&gt;id-1].waiting_time = total_turnaround_time - processes[current_p-&gt;id-1].duration;      }      continue;    }    current_p = dequeue(mid_q);    if(current_p != NULL){      printf(\"Process %d is running in medium priority queue\\n\", current_p-&gt;id);      if (current_p-&gt;duration &gt; mid_q-&gt;time_quantum) {        current_p-&gt;duration -= mid_q-&gt;time_quantum;        enqueue(low_q, current_p);        total_turnaround_time += mid_q-&gt;time_quantum;      }       else {        total_turnaround_time += current_p-&gt;duration;        processes[current_p-&gt;id-1].turnaround_time = total_turnaround_time;        processes[current_p-&gt;id-1].waiting_time = total_turnaround_time - processes[current_p-&gt;id-1].duration;        printf(\"Process %d finished execution\\n\", current_p-&gt;id);      }      continue;    }    current_p = dequeue(low_q);      if(current_p != NULL){      printf(\"Process %d is running in low priority queue\\n\", current_p-&gt;id);      if (current_p-&gt;duration &gt; low_q-&gt;time_quantum) {        current_p-&gt;duration -= low_q-&gt;time_quantum;        enqueue(low_q, current_p);        total_turnaround_time += low_q-&gt;time_quantum;      }       else {        total_turnaround_time += current_p-&gt;duration;        processes[current_p-&gt;id-1].turnaround_time = total_turnaround_time;        processes[current_p-&gt;id-1].waiting_time = total_turnaround_time - processes[current_p-&gt;id-1].duration;        printf(\"Process %d finished execution\\n\", current_p-&gt;id);      }      continue;    }    printf(\"\\n\");    break;    }}int main(){  int n;  float total_waiting_time=0, total_turnaround_time=0;  Queue* high_q = malloc(sizeof(Queue));  Queue* mid_q = malloc(sizeof(Queue));  Queue* low_q = malloc(sizeof(Queue));    printf(\"Enter the number of processes: \");  scanf(\"%d\", &amp;n);  Process* processes = malloc(n*sizeof(Process));    init_queues(high_q, mid_q, low_q);  for(int i=0;i&lt;n;i++){    processes[i].id = i+1;    printf(\"Enter duration for process %d: \", i+1);    scanf(\"%d\", &amp;processes[i].duration);    enqueue(high_q, &amp;processes[i]);  }  mlfq(processes, n, high_q, mid_q, low_q);  printf(\"Process\\tDuration\\tWaiting Time\\tTurnaround Time\\n\");  for(int i=0; i&lt;n; i++){    printf(\"%d\\t%d\\t%d\\t%d\\n\", processes[i].id, processes[i].duration, processes[i].waiting_time, processes[i].turnaround_time);    total_turnaround_time += processes[i].turnaround_time;    total_waiting_time += processes[i].waiting_time;  }  free(processes);  return 0;}"
  }
  
]

