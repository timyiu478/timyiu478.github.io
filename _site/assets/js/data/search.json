[
  
  {
    "title": "Page Swapping",
    "url": "/posts/page-swapping/",
    "categories": "OS",
    "tags": "OS",
    "date": "2024-01-30 00:00:00 +0800",
    





    
    "snippet": "What is page swappingSwap pages to disk so that the running programs to use more RAM than is physically accessible.Swap Space  Swap space is reserved space on the disk for moving pages between memo...",
    "content": "What is page swappingSwap pages to disk so that the running programs to use more RAM than is physically accessible.Swap Space  Swap space is reserved space on the disk for moving pages between memory and the file system.  This assumes the OS can read and write to swap space in page-sized units.The free CommandThe free command displays amount of free and used memory in the system.               total        used        free      shared  buff/cache   availableMem:         8086120     2908832      577556       56540     4599732     4815504Swap:        2097148          12     2097136  In the “Mem” or memory row, there is more “available” space than “free” because there are pages the system knows it can get rid of if needed.  The “Swap” row which reports the usage of the swap space as distinct from your memory.Swapping Mechanism  In a software-managed TLB architecture, the OS determines if a page exists in physical memory using a new piece of information in each page-table entry called the present bit.  A page fault occurs when a program accesses a page that isn’t in physical memory.  A page fault will require the OS to swap in a page from disk.          use the PTE’s data bits, like the page’s PFN, to store a disk address.      Once the page is located on the disk, it is swapped into memory via I/O. The process will be blocked while the I/O is running, so the OS can run other ready processes while the page fault is handled.      The OS will update the page table to reflect the new page, update the PFN field of the page-table entry (PTE) to reflect the new page’s address in memory, and retry the instruction.      When to swap out(swap page to disk)?High/Low WatermarkWhen the OS detects that there are more pages in memory than the high watermark (HW), a background process called the swap daemon  starts to evict pages from memory until the number of pages is less than the low watermark (LW). The daemon then sleeps until the HW is reached again.Invoke by ProcessThe swap can also be awoken by a process if there are no free pages available; Once the daemon has freed up some pages, it will re-awaken the original thread, which will then be able to page in the appropriate page and continue working.Performancement OptimizationMany systems, will cluster or group a number of pages and write them out to the swap partition all at once.Other useful commands  vmstatImplementing LRUProblemScanning a wide array of times to discover the least-recently-used page is expensive.Approximating LRU  When a page is referenced (read or written), the hardware sets the use bit to 1.  The system’s pages organized in a circle.  Initially, a clock hand points to any page.  When replacing a page, the OS checks if the use bit is 1 or 0.          If 1, page P was recently used. The usage bit for P is cleared (set to 0), and the clock hand is advanced one page (P + 1).      If the use bit is set to 0, the page is evicted (in the worst case, all pages have been recently used and we have now searched through the entire set of pages, clearing all the bits).      Dirty Pages  The clock algorithm may be altered to look for pages that are both unused and clean to evict first; if those aren’t found, then look for unused dirty pages, and so on.  Because if a page has been updated and is thus unclean, it must be evicted by writing it back to disk, which is costly.  The eviction is free if it has not been updated; the physical frame can simply be reused for other purposes without further I/O.  A modified/dirty bit should be included in the hardware to accommodate this behavior.Thrashing  Thrashing is used to describe the system is continuously paging because the memory demands of the operating processes simply outnumber the physical memory available.  The methods to address thrashing          not to execute a subset of them in the hopes that the pages of the reduced set of processes will fit in memory, allowing progress.      launch an out-of-memory killer; this daemon selects a memory-intensive process and kills it.      "
  },
  
  {
    "title": "Memory Space Management with paging",
    "url": "/posts/memory-space-management-with-paging/",
    "categories": "OS",
    "tags": "OS",
    "date": "2024-01-28 00:00:00 +0800",
    





    
    "snippet": "What is PagingPaging is another memory space management approach that dividing memory into fixed size of chuncks called pages. In contrast to segmentation, paging does not have external fragmentati...",
    "content": "What is PagingPaging is another memory space management approach that dividing memory into fixed size of chuncks called pages. In contrast to segmentation, paging does not have external fragmentation and support the abstraction of an address space effectively, regardless of how a process uses the address space since it won’t make assumptions about the way the heap and stack grow and how they are use.Address TranslationTo translate the virtual address the process generates:  We have to break the resulting virtual address into two parts:          The virtual page number (VPN) and      The offset within the page.        Using our VPN, we can now index our page table and find out which physical frame virtual page lives in.Page Table  The page table is a data structure that maps virtual addresses (or virtual page numbers) into physical addresses (physical frame numbers).  Each process has its own page table.Linear Page TableLinear Page table is an array.  VPN is an index of the array.  Each page table entry(PTE) contains PFN and other useful bits.The steps of address translation by hardware// Extract the VPN from the virtual addressVPN = (VirtualAddress &amp; VPN_MASK) &gt;&gt; SHIFT// Form the address of the page-table entry (PTE)PTEAddr = PTBR + (VPN * sizeof(PTE))// Fetch the PTEPTE = AccessMemory(PTEAddr)// Check if process can access the pageif (PTE.Valid == False)    RaiseException(SEGMENTATION_FAULT)else if (CanAccess(PTE.ProtectBits) == False)    RaiseException(PROTECTION_FAULT)else    // Access is OK: form physical address and fetch itoffset = VirtualAddress &amp; OFFSET_MASKPhysAddr = (PTE.PFN &lt;&lt; PFN_SHIFT) | offsetRegister = AccessMemory(PhysAddr)"
  },
  
  {
    "title": "Advanced Page Table",
    "url": "/posts/advanced-page-table/",
    "categories": "OS",
    "tags": "OS",
    "date": "2024-01-28 00:00:00 +0800",
    





    
    "snippet": "Problem of Linear Page Table  The size of the page table is too big.  We need to allocate the physical memory for the page table entries that there is no physical frame as well.Larger Page size  On...",
    "content": "Problem of Linear Page Table  The size of the page table is too big.  We need to allocate the physical memory for the page table entries that there is no physical frame as well.Larger Page size  One way to make the page table smaller is make the page size larger because it makes the number of page table entries to be decreased.  The major drawback to this strategy is that large pages result in waste within each page, a problem known as internal fragmentation. Applications allocate pages but only use small portions of each, and memory quickly fills up with these excessively large pages.Combining Paging and segmentation  The goal of combining paging and segmentation is removing the erroneous entries between the heap, and stack segments(unallocated pages between the stack and the heap are no longer needed) to reduce the size of page table.  Instead of one page table for the process’s whole address space, we have one page table for each segments(code, heap, and stack) so we have three page tables.  The base register holds the physical address of the segment’s page table.  The limits register indicates the page table’s end (i.e., how many valid pages it has).  During a context switch, these registers must be updated to reflect the new process’s page tables.  On a TLB miss (assuming a hardware-managed TLB), the hardware utilizes the segment bits (SN) to identify which base and bounds pair to use. The hardware then combines the physical address with the VPN to generate the page table entry (PTE) address.SN           = (VirtualAddress &amp; SEG_MASK) &gt;&gt; SN_SHIFTVPN          = (VirtualAddress &amp; VPN_MASK) &gt;&gt; VPN_SHIFTAddressOfPTE = Base[SN] + (VPN * sizeof(PTE))Virtual Address:|****SN*****|*****VPN******|*****OFFSET******|  The downsides of this approach:          Use segmentation so assumes a fixed address space utilization pattern; a huge but sparsely used heap      External fragmentation: page tables can now be any size (in multiples of PTEs). Finding memory space for them is more difficult.      Multi-Level Page Tables  This is approach to get rid of all those incorrect sections in the page table without using segmentation.  It first divide the page table into page-sized units.  If all page-table entries (PTEs) of that page are invalid, then do not assign that page of the page table at all.  So, it’s generally compact and supports sparse address spaces.  To know the memory location of the pages of the page table and their validities, it use the new data structure called page directory.  When the OS wants to allocate or grow a page table, it may simply grab the next free page-sized unit(the size is much smaller than the size of page table).  But, on a TLB miss, two loads from memory are necessary to acquire the proper translation information from the page table (one for the page directory, and one for the PTE itself).  For 2-level page table, to find out the page table entry, we can use base pointer + PD.index * sizeof(page directory) to find out the address of page-sized unit = PD.PFN , then we use PD.PFN + PT.index * sizeof(PTE) to find out the PTE address.|******** VPN **********************|****** Offset *********|| 13 | 12 | 11 | 10 | 9 | 8 | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0 ||**** PD.Index *****|** PT.index ***|Inverted Page Table  Rather than having many page tables (one for each system process), we have a single page table with an item for each physical page of the system.  This entry indicates which process uses this page and which virtual page of that process corresponds to this physical page.  A hash table is frequently added on top of the underlying structure to speed up lookups.Swapping the Page Tables to DiskSome systems store page tables in kernel virtual memory, allowing the system to swap portions of these page tables to disk if memory becomes scarce."
  },
  
  {
    "title": "Transaction Lookaside Buffer",
    "url": "/posts/Transaction-Loodaside-Buffer/",
    "categories": "OS",
    "tags": "OS",
    "date": "2024-01-28 00:00:00 +0800",
    





    
    "snippet": "Problem of PagingPaging needs an additional memory lookup in order to translate each virtual address, but it takes too long to obtain translation information before every instruction fetch, load, o...",
    "content": "Problem of PagingPaging needs an additional memory lookup in order to translate each virtual address, but it takes too long to obtain translation information before every instruction fetch, load, or store.What is Transaction Lookaside Buffer?In order to speed up the process of address translation, we use the hardware cache for the address translation. This cache is called Transaction Lookaside Buffer(TLF) which is part of the MMU.TLB EntryTLBs generally include 32 or 64 of TLB entries. A few are for the OS (using the G bit). The OS can set a wired register to instruct the hardware how many TLB slots to reserve for it. The OS uses these reserved mappings for code and data it needs to access during key moments when a TLB miss would be troublesome (e.g., in the TLB miss handler).TLB Entry:  Virtual Page Number (VPN)  Process ID (PID) or Address Space ID(ASID)  Page Frame Number (PFN)  V bit: Valid bit - indicates whether the entry has a valid translation or not  G bit: Global bit - If set, TLB does not check PID for translation  …What if TLB missHardware approach  If the virtual address does not in the TLB entries, we have to check the page table to find the translation.  The hardware has to know the exact location of the page tables in memory (through a page-table base register)OS approach  The hardware mimics an exception, pausing the current instruction stream, switching to kernel mode, and jumping to a trap handler.  Returning from a TLB miss-handling trap causes the hardware to retry the instruction, resulting in a TLB hit.  OS must avoid creating endless loops of TLB misses by keeping the TLB miss handler in physical memory.          reserve some TLB entries for always valid transaction. Or      unmapped and not subject to address translation.        OS can use any data structure it wants to implement the page table.Array Access ExampleA memory array of 10 4-byte integers.The page size is 16 bytes.            VPN      Offset 0-4      Offset 5-8      Offset 9-12      Offset 13-16                  VPN 0                           arr[0]              VPN 1      arr[1]      arr[2]      arr[3]      arr[4]              VPN 2      arr[5]      arr[6]      arr[7]      arr[8]              VPN 3      arr[9]                           The TLB hit rate the first time the array is accessedThe hit rate is 60%.  arr[0]: Miss (VPN 0 stored in TLB)  arr[1]: Miss (VPN 1 stored in TLB)  arr[2]: Hit (VPN 1)  arr[3]: Hit (VPN 1)  arr[4]: Hit (VPN 1)  arr[5]: Miss (VPN 2 stored in TLB)  arr[6]: Hit (VPN 2)  arr[7]: Hit (VPN 2)  arr[8]: Hit (VPN 2)  arr[9]: Miss (VPN 3 stored in TLB)The TLB hit rate the second time the array is accessedThe hit rate is 100% because VPN 0-4 stored in TLB already in the first time access.Context SwitchingHow to make sure the process does not reuse the TLB entries of the old process?  flushing: clears the TLB by setting all valid bits to 0.  ASID: TLBs include an address space identifier (ASID) field. The ASID is a Process ID (PID) with less bits. So the TLB can hold several processes’ translations.Two entries for two processes with two VPNs point to the same physical pageWhen two processes share a page (for example, a code page), this can occur. Also, it reduces memory overheads by reducing the number of physical pages needed.            VPN      PFN      ASID      Prot-bit      Valid-bit                  VPN 0      PFN 100      1      r-x      1              VPN 5      PFN 100      2      r-x      1      TLB Replacement Policy  LRU  Random"
  },
  
  {
    "title": "Memory Segmentation",
    "url": "/posts/memory-segmentation/",
    "categories": "OS",
    "tags": "OS",
    "date": "2024-01-27 00:00:00 +0800",
    





    
    "snippet": "What is Memory Segmentation?  It allows the segments(code, stack, heap) of the address space can be stored in different physical memory locations so that we do not need to allocate the physical mem...",
    "content": "What is Memory Segmentation?  It allows the segments(code, stack, heap) of the address space can be stored in different physical memory locations so that we do not need to allocate the physical memory for the “free” segment.  Each segment has its own base/bound registers.Which segment the virtual memory address related to?Explicit Approach  we divide the address space into segments based on the first few bits of the virtual address.  the top 2 most bits represent which segment the address corresponds to.  the other bits represent the offset.// get top 2 bits of 14-bit VASegment = (VirtualAddress &amp; SEG_MASK) &gt;&gt; SEG_SHIFT// now get offsetOffset  = VirtualAddress &amp; OFFSET_MASKif (Offset &gt;= Bounds[Segment])  RaiseException(PROTECTION_FAULT)else  PhysAddr = Base[Segment] + Offset  Register = AccessMemory(PhysAddr)Implicit Approach  determines the segment by examining the address.  If the address came from the program counter (i.e., an instruction fetch), it’s in the code segment.  If it came from the stack or base pointer, it’s in the stack segment.  All others are in the heap.How to handle stack  The difference between the stack and the other segment is it now grows backwards (towards lower addresses).  We need more hardware support so that the hardware knows the segment grows positive or negative from the base address.  We can get the correct physical address by base address + offset - max segment size.            Segment      Base      Size (Max 4K)      Grows Positive?                  Code      32K      2K      1              Heap      34K      3K      1              Code      28K      2K      0      Segmentation presents new challenges for the OS  The segment registers must be saved and restored becase each process has its own virtual address space for context switch.  Able to update the segment size register to the new (larger/smaller) size.  Able to find physical memory space for new address spaces.  handle external fragmentation: physical memory soon fills up with pockets of free space, making it impossible to assign new segments or expand old ones."
  },
  
  {
    "title": "Why Memory Virtualisation?",
    "url": "/posts/why-memory-virtualisation/",
    "categories": "OS",
    "tags": "OS",
    "date": "2024-01-26 00:00:00 +0800",
    





    
    "snippet": "  Transparency: user program dont need to worry about the physical memory address.  Protection: each process only can manipulate its own address space.",
    "content": "  Transparency: user program dont need to worry about the physical memory address.  Protection: each process only can manipulate its own address space."
  },
  
  {
    "title": "Multilevel Feedback Queue Scheduling",
    "url": "/posts/mlfq-scheduling-policy/",
    "categories": "OS",
    "tags": "OS",
    "date": "2024-01-25 00:00:00 +0800",
    





    
    "snippet": "Example RunEnter the number of processes: 3Enter duration for process 1: 10Process 1 enqueued in queue with time quantum 2Enter duration for process 2: 20Process 2 enqueued in queue with time quant...",
    "content": "Example RunEnter the number of processes: 3Enter duration for process 1: 10Process 1 enqueued in queue with time quantum 2Enter duration for process 2: 20Process 2 enqueued in queue with time quantum 2Enter duration for process 3: 30Process 3 enqueued in queue with time quantum 2Process 1 dequeued from queue with time quantum 2Process 1 is running in high priority queueProcess 1 enqueued in queue with time quantum 4Process 2 dequeued from queue with time quantum 2Process 2 is running in high priority queueProcess 2 enqueued in queue with time quantum 4Process 3 dequeued from queue with time quantum 2Process 3 is running in high priority queueProcess 3 enqueued in queue with time quantum 4Process 1 dequeued from queue with time quantum 4Process 1 is running in medium priority queueProcess 1 enqueued in queue with time quantum 8Process 2 dequeued from queue with time quantum 4Process 2 is running in medium priority queueProcess 2 enqueued in queue with time quantum 8Process 3 dequeued from queue with time quantum 4Process 3 is running in medium priority queueProcess 3 enqueued in queue with time quantum 8Process 1 dequeued from queue with time quantum 8Process 1 is running in low priority queueProcess 1 finished executionProcess 2 dequeued from queue with time quantum 8Process 2 is running in low priority queueProcess 2 enqueued in queue with time quantum 8Process 3 dequeued from queue with time quantum 8Process 3 is running in low priority queueProcess 3 enqueued in queue with time quantum 8Process 2 dequeued from queue with time quantum 8Process 2 is running in low priority queueProcess 2 finished executionProcess 3 dequeued from queue with time quantum 8Process 3 is running in low priority queueProcess 3 enqueued in queue with time quantum 8Process 3 dequeued from queue with time quantum 8Process 3 is running in low priority queueProcess 3 finished executionProcess Duration        Waiting Time    Turnaround Time1       10      12      222       20      24      443       30      30      60Example C Code#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;typedef struct {  int id;  int duration;  int remaining_time;  int waiting_time;  int turnaround_time;} Process;typedef struct {  Process* processes;  int front;  int rear;  int time_quantum;} Queue;void init_queues(Queue* high_q, Queue* mid_q, Queue* low_q){  high_q-&gt;processes = malloc(10*sizeof(Process));  mid_q-&gt;processes = malloc(10*sizeof(Process));  low_q-&gt;processes = malloc(10*sizeof(Process));  high_q-&gt;time_quantum = 2;  mid_q-&gt;time_quantum = 4;  low_q-&gt;time_quantum = 8;  high_q-&gt;front = -1;  mid_q-&gt;front = -1;  low_q-&gt;front = -1;  high_q-&gt;rear = -1;  mid_q-&gt;rear = -1;  low_q-&gt;rear = -1;}void enqueue(Queue* q, Process* p){  printf(\"Process %d enqueued in queue with time quantum %d\\n\", p-&gt;id, q-&gt;time_quantum);  if(q-&gt;front &gt; 9) { return; }  q-&gt;front += 1;  q-&gt;processes[q-&gt;front].id = p-&gt;id;  q-&gt;processes[q-&gt;front].duration = p-&gt;duration;  q-&gt;processes[q-&gt;front].remaining_time = p-&gt;remaining_time;  q-&gt;processes[q-&gt;front].waiting_time = p-&gt;waiting_time;  q-&gt;processes[q-&gt;front].turnaround_time = p-&gt;turnaround_time;}Process* dequeue(Queue* q){  if(q-&gt;rear &gt;= q-&gt;front) {     q-&gt;rear = -1;    q-&gt;front = -1;    return NULL;  }  q-&gt;rear += 1;  printf(\"Process %d dequeued from queue with time quantum %d\\n\", q-&gt;processes[q-&gt;rear].id, q-&gt;time_quantum);  return &amp;q-&gt;processes[q-&gt;rear];}void mlfq(Process* processes, int n, Queue* high_q, Queue* mid_q, Queue* low_q) {  Process* current_p;  int total_turnaround_time = 0;  if (n&lt;= 0) { return; }  while(1){    current_p = dequeue(high_q);    if(current_p != NULL){      printf(\"Process %d is running in high priority queue\\n\", current_p-&gt;id);      if (current_p-&gt;duration &gt; high_q-&gt;time_quantum) {        current_p-&gt;duration -= high_q-&gt;time_quantum;        enqueue(mid_q, current_p);        total_turnaround_time += high_q-&gt;time_quantum;      } else {        total_turnaround_time += current_p-&gt;duration;        processes[current_p-&gt;id-1].turnaround_time = total_turnaround_time;        processes[current_p-&gt;id-1].waiting_time = total_turnaround_time - processes[current_p-&gt;id-1].duration;      }      continue;    }    current_p = dequeue(mid_q);    if(current_p != NULL){      printf(\"Process %d is running in medium priority queue\\n\", current_p-&gt;id);      if (current_p-&gt;duration &gt; mid_q-&gt;time_quantum) {        current_p-&gt;duration -= mid_q-&gt;time_quantum;        enqueue(low_q, current_p);        total_turnaround_time += mid_q-&gt;time_quantum;      }       else {        total_turnaround_time += current_p-&gt;duration;        processes[current_p-&gt;id-1].turnaround_time = total_turnaround_time;        processes[current_p-&gt;id-1].waiting_time = total_turnaround_time - processes[current_p-&gt;id-1].duration;        printf(\"Process %d finished execution\\n\", current_p-&gt;id);      }      continue;    }    current_p = dequeue(low_q);      if(current_p != NULL){      printf(\"Process %d is running in low priority queue\\n\", current_p-&gt;id);      if (current_p-&gt;duration &gt; low_q-&gt;time_quantum) {        current_p-&gt;duration -= low_q-&gt;time_quantum;        enqueue(low_q, current_p);        total_turnaround_time += low_q-&gt;time_quantum;      }       else {        total_turnaround_time += current_p-&gt;duration;        processes[current_p-&gt;id-1].turnaround_time = total_turnaround_time;        processes[current_p-&gt;id-1].waiting_time = total_turnaround_time - processes[current_p-&gt;id-1].duration;        printf(\"Process %d finished execution\\n\", current_p-&gt;id);      }      continue;    }    printf(\"\\n\");    break;    }}int main(){  int n;  float total_waiting_time=0, total_turnaround_time=0;  Queue* high_q = malloc(sizeof(Queue));  Queue* mid_q = malloc(sizeof(Queue));  Queue* low_q = malloc(sizeof(Queue));    printf(\"Enter the number of processes: \");  scanf(\"%d\", &amp;n);  Process* processes = malloc(n*sizeof(Process));    init_queues(high_q, mid_q, low_q);  for(int i=0;i&lt;n;i++){    processes[i].id = i+1;    printf(\"Enter duration for process %d: \", i+1);    scanf(\"%d\", &amp;processes[i].duration);    enqueue(high_q, &amp;processes[i]);  }  mlfq(processes, n, high_q, mid_q, low_q);  printf(\"Process\\tDuration\\tWaiting Time\\tTurnaround Time\\n\");  for(int i=0; i&lt;n; i++){    printf(\"%d\\t%d\\t%d\\t%d\\n\", processes[i].id, processes[i].duration, processes[i].waiting_time, processes[i].turnaround_time);    total_turnaround_time += processes[i].turnaround_time;    total_waiting_time += processes[i].waiting_time;  }  free(processes);  return 0;}"
  },
  
  {
    "title": "Welcome to Jekyll!",
    "url": "/posts/hellworld/",
    "categories": "HELLOWORLD",
    "tags": "HELLOWORLD",
    "date": "2024-01-21 00:00:00 +0800",
    





    
    "snippet": "WelcomeHello world, this is my first Jekyll blog post.I hope you like it!",
    "content": "WelcomeHello world, this is my first Jekyll blog post.I hope you like it!"
  }
  
]

