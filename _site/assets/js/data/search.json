[
  
  {
    "title": "Concurrent Data Structures",
    "url": "/posts/concurrent-data-structure/",
    "categories": "OS",
    "tags": "OS",
    "date": "2024-02-07 00:00:00 +0800",
    





    
    "snippet": "Challenges of Concurrency in Data Structures  What are the strategies for adding locks to a data structure to ensure correct, concurrent access?  How can locks be applied in a way that maintains th...",
    "content": "Challenges of Concurrency in Data Structures  What are the strategies for adding locks to a data structure to ensure correct, concurrent access?  How can locks be applied in a way that maintains the speed and allows simultaneous access by multiple threads?Ideal Scenario: Perfect ScalingPerfect scaling is achieved when the time taken for threads to complete tasks on multiple processors is as fast as on a single processor, despite the increase in workload.Concurrent Counter Data Structure by adding lockC++ Implementation#include &lt;stdio.h&gt;#include &lt;pthread.h&gt;// Counter with locktypedef struct __counter_t {    int value;    pthread_mutex_t lock;} counter_t;void init(counter_t *c) {    c-&gt;value = 0;    pthread_mutex_init(&amp;c-&gt;lock, NULL);}void increment(counter_t *c) {    pthread_mutex_lock(&amp;c-&gt;lock);    c-&gt;value++;    printf(\"Incremented: %d\\n\", c-&gt;value);    pthread_mutex_unlock(&amp;c-&gt;lock);}void decrement(counter_t *c) {    pthread_mutex_lock(&amp;c-&gt;lock);    c-&gt;value--;    printf(\"Decremented: %d\\n\", c-&gt;value);    pthread_mutex_unlock(&amp;c-&gt;lock);}int get(counter_t *c) {    pthread_mutex_lock(&amp;c-&gt;lock);    int rc = c-&gt;value;    pthread_mutex_unlock(&amp;c-&gt;lock);    return rc;}void* threadFunction(void* arg) {    counter_t *c = (counter_t*)arg;    for (int i = 0; i &lt; 5; ++i) {        increment(c);    }    for (int i = 0; i &lt; 3; ++i) {        decrement(c);    }    printf(\"Final value in thread: %d\\n\", get(c));    return NULL;}int main() {    counter_t counter;    init(&amp;counter);    pthread_t threads[2];    for (int i = 0; i &lt; 2; ++i) {        pthread_create(&amp;threads[i], NULL, threadFunction, &amp;counter);    }    for (int i = 0; i &lt; 2; ++i) {        pthread_join(threads[i], NULL);    }    printf(\"Final value in main: %d\\n\", get(&amp;counter));    return 0;}Evaluations  Single Lock Bottleneck: A single lock might not be sufficient for high-performance needs. Further optimizations may be required, which will be the focus of the rest of the chapter.  Sufficiency for Basic Needs: If performance isn’t a critical issue, a simple lock might suffice, and no further complexity is necessary.The Approximation Counter Approach  The logical counter is represented by a global counter and one local counter for each CPU core.  Each local counter is synchronized with its own local lock, and a global lock is used for the global counter.  When a thread wants to increment the counter, it updates its corresponding local counter, which is efficient due to reduced contention across CPUs.  Regularly, the value from a local counter is transferred to the global counter by acquiring the global lock. This process involves incrementing the global counter based on the local counter’s value and then resetting the local counter to zero.  The frequency of local-to-global updates is determined by a threshold S. A smaller S makes the counter behave more like a non-scalable counter, while a larger S enhances scalability at the expense of accuracy in the global count.Concurrent Operations in a Linked ListC++ Implementation#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;pthread.h&gt;// Define the structure for a linked list nodetypedef struct node {    int key;    struct node *next;} node_t;// Define the structure for the linked listtypedef struct list {    node_t *head;    pthread_mutex_t lock;} list_t;// Initialize the linked listvoid List_Init(list_t *L) {    L-&gt;head = NULL;    pthread_mutex_init(&amp;L-&gt;lock, NULL);}// Insert a new node with the given key at the beginning of the listvoid List_Insert(list_t *L, int key) {    // Allocate memory for a new node    node_t *new = malloc(sizeof(node_t));    if (new == NULL) {        perror(\"malloc\");        return;    }    new-&gt;key = key;    // Lock the critical section    pthread_mutex_lock(&amp;L-&gt;lock);    // Update the pointers to insert the new node at the beginning    new-&gt;next = L-&gt;head;    L-&gt;head = new;    // Unlock the critical section    pthread_mutex_unlock(&amp;L-&gt;lock);}// Lookup a key in the linked list and return 0 if found, -1 if not foundint List_Lookup(list_t *L, int key) {    int rv = -1;    // Lock the critical section    pthread_mutex_lock(&amp;L-&gt;lock);    // Traverse the linked list to find the key    node_t *curr = L-&gt;head;    while (curr) {        if (curr-&gt;key == key) {            rv = 0; // Key found            break;        }        curr = curr-&gt;next;    }    // Unlock the critical section    pthread_mutex_unlock(&amp;L-&gt;lock);    return rv; // Return 0 for success and -1 for failure}Evaluation  The lock is only held around the critical region of the insert or lookup operations. This makes the implementation more robust.  Assumed malloc() is thread-safe that can be called without a lock, reducing the duration for which the lock is held.  The lookup function has a single exit path for decreasing the likelihood of errors like forgetting to unlock before returning.Hand-Over-Hand Locking in Linked Lists  This approach involves adding a lock to each node of the list. As one traverses the list, they acquire the next node’s lock before releasing the current one.  Conceptually, hand-over-hand locking increases parallelism in list operations. However, in practice, the overhead of locking each node can be prohibitive, often making it less efficient than a single lock, especially for extensive lists and numerous threads. A hybrid method where a lock is acquired for every few nodes might be a more practical solution.Concurrent Queues  A queue involves two separate locks: one for the head and another for the tail.  Allows enqueue operations to primarily use the tail lock and dequeue operations to use the head lock, enabling concurrent execution of these functions.  A dummy node, allocated during the queue’s initialization, separates the head and tail operations, further enhancing concurrency.Concurrent Non-resizable Hash Table  Each hash bucket, represented by a list, has its own lock. This differs from using a single lock for the entire table.  By allocating a lock per hash bucket, the hash table allows multiple operations to occur simultaneously, significantly enhancing its performance."
  },
  
  {
    "title": "Threads Locks",
    "url": "/posts/threads-locks/",
    "categories": "OS",
    "tags": "OS",
    "date": "2024-02-05 00:00:00 +0800",
    





    
    "snippet": "What is a LockLock is a tool for allowing running critical section code atomically by only allow one thread to be execute that part of code.How does a lock work?  Lock Variable: e.g. mutex. It hold...",
    "content": "What is a LockLock is a tool for allowing running critical section code atomically by only allow one thread to be execute that part of code.How does a lock work?  Lock Variable: e.g. mutex. It holds the lock state.  It indicates whether the lock is free or acquired by a thread.  Acquiring a Lock: When a thread executes lock(&amp;mutex), it acquires the lock if it’s free, allowing the thread to enter the critical section.  Waiting Threads: If there are threads waiting (blocked in the lock() function), the lock won’t be set to free immediately. Instead, one of the waiting threads will enter the critical section.  Releasing a Lock: Using unlock(&amp;mutex) releases the lock, making it available for other threads.Supports for implementing lock  Hardware: special hardware instructions for implementing locks that are both efficient and effective.  OS: complements these hardware capabilities by providing the necessary context and controls to ensure that locks are used appropriately within the system.Key Criteria for Assessing Lock Efficacy  Basic Functionality: The primary function of a lock is to ensure mutual exclusion, keeping multiple threads out of a critical section simultaneously.  Equal Opportunity: Fairness in locks means each competing thread should have a reasonable chance of acquiring the lock once it’s available.  Avoiding Starvation: Assess whether any thread consistently fails to acquire the lock, leading to starvation.  Overhead Assessment: Consider the time overheads incurred by using the lock in different scenarios:          Single CPU, Single Thread      Single CPU, Multiple Threads      Multiple CPUs, Multiple Threads      Achieving mutual exclusion on single-processor systems by controlling interrupts.Idea  Before entering a critical section, interrupts are disabled to prevent interruption.  After exiting the critical section, interrupts are re-enabled.ProsSimple.Cons  Trust the programs.  Cant ensure mutual exclusion on multi-processor systems.  Prolonged disabling of interrupts might lead to missed signals, causing system issues.  Interrupt masking and unmasking operations can be slow on modern CPUs.Implementing a Flag-Based LockLock Structure and Functionstypedef struct __lock_t { int flag; } lock_t;void init(lock_t *mutex) {    // 0 -&gt; lock is available, 1 -&gt; held    mutex-&gt;flag = 0;}void lock(lock_t *mutex) {    while (mutex-&gt;flag == 1) // TEST the flag        ; // spin-wait (do nothing)    mutex-&gt;flag = 1; // now SET it!}void unlock(lock_t *mutex) {    mutex-&gt;flag = 0;}Issues  Potential for Race Conditions: a critical window between the check while (mutex-&gt;flag == 1) and the set mutex-&gt;flag = 1; so that more than one thread could acquire the lock. For example,          Thread 1: unlock()      Thread 2: mutex-&gt;flag == 1 is false      Thread 3: mutex-&gt;flag == 1 is false      Thread 2: mutex-&gt;flag = 1      Thread 3: mutex-&gt;flag = 1        Spin-wait: consume CPU with do nothing.Spin Lock with Test-And-SetTest-And-SetThe test-and-set instruction is a fundamental piece of hardware support for locking which is the atomic instruction.int TestAndSet(int *old_ptr, int new) {    // logical implementation that should be executed atomically in reality    int old = *old_ptr; // Fetch old value    *old_ptr = new;     // Store 'new' into old_ptr    return old;         // Return the old value}Lock Acquisition  If a thread calls lock() and no other thread holds the lock, TestAndSet(flag, 1) will return 0 (the old value), and the thread acquires the lock while setting the flag to 1.  If another thread already holds the lock (flag is 1), TestAndSet(flag, 1) will return 1, causing the thread to enter a spin-wait loop until the lock is released.Releasing the LockThe unlocking thread sets the flag back to 0, allowing other threads to acquire the lock.Evaluation  achieve mutual exclustion  potential for starvation  waste CPU cycles in the spin-wait loopSpin Lock with Compare-And-SwapCompare-And-Swap (CAS), known as Compare-And-Exchange on x86 architectures, is a hardware primitive provided by some systems to aid in concurrent programming.CAS Functionality in C Pseudocodeint CompareAndSwap(int *ptr, int expected, int new) {    int original = *ptr;    if (original == expected)        *ptr = new;    return original;}  CAS returns the original value at ptr, allowing the calling code to determine whether the update was successful.  atomically updates ptr with new if the value at the address specified by ptr equals expected.CAS Lock Implementionvoid lock(lock_t *lock) {    while (CompareAndSwap(&amp;lock-&gt;flag, 0, 1) == 1)        ; // Spin-wait}Compare with Test-And-Set  Lock acquisition and releasing are the same.  CAS is more flexible than test-and-set, opening possibilities for more advanced synchronization techniques like lock-free synchronization.Spin Lock with Load-Linked and Store-ConditionalLoad-Linked and Store-ConditionalLoad-Linked (LL) and Store-Conditional (SC) are pairs of instructions used on platforms like MIPS, ARM, and PowerPC to create locks and other concurrent structures  Load-Linked (LL): This instruction loads a value from a memory location into a register. It prepares for a subsequent conditional store.  Store-Conditional (SC): This instruction attempts to store a value to the memory location if no updates have been made to that location since the last load-linked operation. It returns 0 on failure, leaving the value unchanged.Lock Implementation in Pseudo C Codevoid lock(lock_t *lock) {    while (LoadLinked(&amp;lock-&gt;flag) ||            !StoreConditional(&amp;lock-&gt;flag, 1))        ; // Spin-wait}When two threads attempt to acquire the lock simultaneously after an LL operation, only one will succeed with the SC. The other thread’s SC will fail, forcing it to retry.Ticket Lock with Fetch-And-AddFetch-And-Add Representation in CThe fetch-and-add instruction atomically increments a value at a specific address while returning the old value.int FetchAndAdd(int *ptr) {    int old = *ptr;    *ptr = old + 1;    return old;}Ticket Lock Representation in C++class TicketLock {  private:    std::atomic&lt;int&gt; ticketCounter;    std::atomic&lt;int&gt; turn;  public:    TicketLock() : ticketCounter(0), turn(0) {}    void lock() {        int myTicket = ticketCounter.fetch_add(1); // Fetch-And-Add        while (turn.load(std::memory_order_relaxed) != myTicket) {            ; // Spin-wait        }    }    void unlock() {        turn.fetch_add(1); // Move to next ticket    }};  A thread acquires a ticket through an atomic fetch-and-add on the ticket counter. This ticket number (ticketCOunter) represents the thread’s place in the queue.  A thread enters the critical section when its ticket number matches the current turn (ticketCounter == turn).  To release the lock, the thread simply increments the turn variable, allowing the next thread in line to enter the critical section.Advantages of Ticket Lock  Guarantee Progress: ensures that all threads make progress. Each thread is assigned a ticket, and they enter the critical section in the order of their tickets.  Fairness: guarantees that each thread will eventually get its turn to enter the critical section.Yield to Avoid Spin WaitingBasic IdeaInstead of spinning, a thread could yield the CPU to another thread through a basic yield() system call, where the yielding thread de-schedules itself, changing its state from running to ready.Challenges  In a system with many threads competing for a lock, yielding can lead to frequent context switches. If one thread acquires the lock and is preempted, the other threads will sequentially find the lock held, yield, and enter a run-and-yield cycle.  The starvation problem is unaddressed.Sleep and Waiting QueueSolaris Implementation in C// Define a structure for the locktypedef struct __lock_t {    int flag;       // 0 if lock is available, 1 if locked    int guard;      // To prevent concurrent guard lock    queue_t *q;     // Queue to hold waiting threads} lock_t;// Initialize the lockvoid lock_init(lock_t *m) {    m-&gt;flag = 0;     // Lock is initially available    m-&gt;guard = 0;    // Guard is initially free    queue_init(m-&gt;q); // Initialize the queue for waiting threads}// Acquire the lockvoid lock(lock_t *m) {    while (TestAndSet(&amp;m-&gt;guard, 1) == 1)        ; // Acquire guard lock by spinning    if (m-&gt;flag == 0) {        m-&gt;flag = 1;   // Lock is acquired        m-&gt;guard = 0;  // Release the guard lock    } else {        queue_add(m-&gt;q, gettid()); // Add current thread to the waiting queue        m-&gt;guard = 0;              // Release the guard lock        park();                     // Put the thread to sleep    }}// Release the lockvoid unlock(lock_t *m) {    while (TestAndSet(&amp;m-&gt;guard, 1) == 1)        ; // Acquire guard lock by spinning    if (queue_empty(m-&gt;q))        m-&gt;flag = 0; // Release the lock; no one wants it    else        unpark(queue_remove(m-&gt;q)); // Wake up the next waiting thread    m-&gt;guard = 0; // Release the guard lock}Addressing Race ConditionsThe race condition can occurs just before the park call. For example,  Thread-1: m-&gt;guard = 0;  Thread-2: unlock(Thread-1);  Thread-1: park();To ensure unpark is called before park, the thread does not sleep unnecessarily by adding setpark() before m-&gt;guard = 0.queue_add(m-&gt;q, gettid());setpark(); // new codem-&gt;guard = 0;Or place the guard directly within the kernel, allowing for atomic operations in lock release and thread dequeuing."
  },
  
  {
    "title": "Thread API",
    "url": "/posts/thread-api/",
    "categories": "OS",
    "tags": "OS",
    "date": "2024-02-03 00:00:00 +0800",
    





    
    "snippet": "Thread Creation in POSIXFunctions  create thread: pthread_create  wait a thread to complete: pthread_joinExample Code#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;pthread.h&gt;// De...",
    "content": "Thread Creation in POSIXFunctions  create thread: pthread_create  wait a thread to complete: pthread_joinExample Code#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;pthread.h&gt;// Define structures for argument and return valuestypedef struct {    int a;    int b;} myarg_t;typedef struct {    int x;    int y;} myret_t;// Thread function to perform a task and return valuesvoid *mythread(void *arg) {    myret_t *rvals = (myret_t *)malloc(sizeof(myret_t));  // Allocate memory for return values    // Simulate some work and set return values    rvals-&gt;x = 1;    rvals-&gt;y = 2;    return (void *)rvals;  // Return the allocated struct}int main(int argc, char *argv[]) {    pthread_t p;          // Thread object    myret_t *rvals;       // Pointer to store returned values    myarg_t args = {10, 20};  // Example argument values    // Create a new thread with the specified function and arguments    pthread_create(&amp;p, NULL, mythread, &amp;args);    // Wait for the thread to finish and retrieve its return values    pthread_join(p, (void **) &amp;rvals);    // Print the returned values    printf(\"returned %d %d\\n\", rvals-&gt;x, rvals-&gt;y);    free(rvals);  // Free the allocated memory    return 0;}Mutual Exclusion with POSIX ThreadsMutexes are essential for protecting critical sections of code to ensure correct operation.Usage Examplepthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER; // Static Initialization to set mutex to default state.// int rc = pthread_mutex_init(&amp;lock, NULL); // Dynamic Initialization with NULL indicating default attributes.// assert(rc == 0); // Check for successful initializationpthread_mutex_lock(&amp;lock); // Acquire the lock. If the lock is already held by another thread, the calling thread will block until it can acquire the lock.x = x + 1; // Critical sectionpthread_mutex_unlock(&amp;lock); // Release the lockpthread_mutex_destroy(&amp;lock); // Clean upAdvanced Mutex Operations  Timed Locks: Acquire a lock with a timeout, useful in scenarios where avoiding deadlock is crucial.  Try Locks: Non-blocking lock attempts that can be useful in certain advanced programming scenarios.Condition variablesThey are used when threads need to signal each other to proceed with their tasks.Example C++ Code#include &lt;iostream&gt;#include &lt;pthread.h&gt;#include &lt;unistd.h&gt; // For sleep()pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER; // Mutexpthread_cond_t cond = PTHREAD_COND_INITIALIZER;  // Condition variable// Shared variablebool ready = false;// Thread function that waits for the conditionvoid* wait_thread(void* arg) {    pthread_mutex_lock(&amp;lock);    while (!ready) { // Using while loop for spurious wake-ups        std::cout &lt;&lt; \"Waiting thread is waiting for the condition...\" &lt;&lt; std::endl;        pthread_cond_wait(&amp;cond, &amp;lock);    }    std::cout &lt;&lt; \"Waiting thread received the signal.\" &lt;&lt; std::endl;    pthread_mutex_unlock(&amp;lock);    return NULL;}// Thread function that signals the conditionvoid* signal_thread(void* arg) {    sleep(1); // Sleep for demonstration purposes    pthread_mutex_lock(&amp;lock);    ready = true;    std::cout &lt;&lt; \"Signaling thread is signaling the condition...\" &lt;&lt; std::endl;    pthread_cond_signal(&amp;cond);    pthread_mutex_unlock(&amp;lock);    return NULL;}int main() {    pthread_t waitThread, signalThread;    // Create threads    pthread_create(&amp;waitThread, NULL, wait_thread, NULL);    pthread_create(&amp;signalThread, NULL, signal_thread, NULL);    // Wait for threads to finish    pthread_join(waitThread, NULL);    pthread_join(signalThread, NULL);    // Clean up    pthread_mutex_destroy(&amp;lock);    pthread_cond_destroy(&amp;cond);    return 0;}  Hold Lock During Signaling: Always hold the lock when signaling or modifying the global condition variable to avoid race conditions.  Lock Handling in Wait and Signal: The wait function requires the lock as it releases it when putting the thread to sleep. The lock is re-acquired before pthread_cond_wait returns. The signal function only needs the condition variable.  Rechecking the Condition: Use a while loop rather than an if statement to recheck the condition. This is because some implementations may wake up threads spuriously, without the condition actually being met.Caution Against Using Flags for Synchronization// Waiting codewhile (ready == 0) ; // Spin// Signaling codeready = 1;This approach can lead to excessive CPU usage (busy-waiting) and is prone to synchronization errors."
  },
  
  {
    "title": "Concurrency and Threads",
    "url": "/posts/concurrency-and-threads/",
    "categories": "OS",
    "tags": "OS",
    "date": "2024-02-03 00:00:00 +0800",
    





    
    "snippet": "Threads  Each thread is like an independent worker, but they all share the same memory space.  Each thread has its own Program Counter(PC) to keeps track of where a thread is in its execution.  Jus...",
    "content": "Threads  Each thread is like an independent worker, but they all share the same memory space.  Each thread has its own Program Counter(PC) to keeps track of where a thread is in its execution.  Just like a process has a control block to keep track of its state, each thread has its own Thread Control Block(TCB).  Each thread has its own stack. This means each thread can handle its own functions and data, keeping them in its personal stack.  Context switch is allowed by saving and loading different sets of registers for each thread.Why use threads?They are useful on parallelism and handling I/O (Input/Output) operations.ParallelismIn multi-processor system, we can speed up the process by dividing the task(e.g. adding numbers in a huge array) among multiple threads, with each processor handling a part of the job.Avoid delays caused by I/O operationsIf a program only used one thread and had to wait for an I/O operation to complete, it couldn’t do anything else during that time. By using multiple threads, one thread can handle the I/O operation while others continue processing or initiating more I/O requests.Why not just use multiple processes instead of threads?Threads have a special advantage: they share the same address space. This makes data sharing between threads straightforward and efficient, especially suitable for tasks that are closely related.Race Condition  When multiple threads access and modify the same data, unpredictable outcomes can occur because of the uncontrollable threads scheduling.  For example, in order to increment an counter, 3 machine codes will be generated by compiler.          load counter value from memory to register(e.g. eax).      increment the register.      store back the updated register value to counter memory location.        Two threads (Thread 1 and Thread 2) executing this sequence concurrently so that the code executed twice, but the counter incremented only once.          Thread 1: loading the counter value(10) into eax and increments it to 11.      Context Switch to Thread 2.      Thread 2: loading the counter value(10) into eax and increments it to 11, store its eax value(11) back to memory.      Context Switch to Thread 1.      Thread 1: store its eax value(11) back to memory.      Critical SectionFrom the above code sequence example, we can see that increment an counter is an critical section that the code that accesses a shared resource and should not be executed by more than one thread at a time.Mutual ExclusionTo prevent race conditions, we need mutual exclusion in critical sections. If one thread operates within the critical part, the others are stopped.Atomicity  In multi-threading, a major challenge is ensuring that certain operations are executed without interruption, maintaining consistency in shared data.  add instruction is one of an atomic instructions.  In practice, we often don’t have such atomic instructions for complex operations in regular instruction set and we have to break it down to multiple instructions.  Since we can’t rely on hardware for atomicity, we turn to synchronization primitives."
  },
  
  {
    "title": "VMS Lazy Optimizations",
    "url": "/posts/vms-lazy-optimizations/",
    "categories": "OS",
    "tags": "OS",
    "date": "2024-02-02 00:00:00 +0800",
    





    
    "snippet": "Demand Page Zeroing  Consider adding a page to the address space, say in the heap.  The OS have to zero the page for security reason so that the process cannot know how this page used for.  Instead...",
    "content": "Demand Page Zeroing  Consider adding a page to the address space, say in the heap.  The OS have to zero the page for security reason so that the process cannot know how this page used for.  Instead of the OS zeros the page before maps it into your address space, the OS  the OS just adds an entry to the page table marking it unavailable.  When the process reads or writes the page, it traps the OS. A demand-zero page is identified by the OS using certain bits designated in the “reserved for OS” portion of the page table entry.  The OS then finds a physical page, zeroes it, and maps it into the process’s address space.  This task(zeros the page) is avoided if the process never accesses the page.Copy-On-Write  Instead of copying a page from one address space to another, the OS can map it into the target address space and declare it read-only in both address spaces.  If both address spaces just read the page, no data is moved.  A page write attempt from one of the address spaces will be logged into the OS. The OS then allocate a new page, fill it with data, and map it into the address space of the faulting process.  So it saves unnecessary copying.References  VMS - https://en.wikipedia.org/wiki/OpenVMS"
  },
  
  {
    "title": "Page Swapping",
    "url": "/posts/page-swapping/",
    "categories": "OS",
    "tags": "OS",
    "date": "2024-01-30 00:00:00 +0800",
    





    
    "snippet": "What is page swappingSwap pages to disk so that the running programs to use more RAM than is physically accessible.Swap Space  Swap space is reserved space on the disk for moving pages between memo...",
    "content": "What is page swappingSwap pages to disk so that the running programs to use more RAM than is physically accessible.Swap Space  Swap space is reserved space on the disk for moving pages between memory and the file system.  This assumes the OS can read and write to swap space in page-sized units.The free CommandThe free command displays amount of free and used memory in the system.               total        used        free      shared  buff/cache   availableMem:         8086120     2908832      577556       56540     4599732     4815504Swap:        2097148          12     2097136  In the “Mem” or memory row, there is more “available” space than “free” because there are pages the system knows it can get rid of if needed.  The “Swap” row which reports the usage of the swap space as distinct from your memory.Swapping Mechanism  In a software-managed TLB architecture, the OS determines if a page exists in physical memory using a new piece of information in each page-table entry called the present bit.  A page fault occurs when a program accesses a page that isn’t in physical memory.  A page fault will require the OS to swap in a page from disk.          use the PTE’s data bits, like the page’s PFN, to store a disk address.      Once the page is located on the disk, it is swapped into memory via I/O. The process will be blocked while the I/O is running, so the OS can run other ready processes while the page fault is handled.      The OS will update the page table to reflect the new page, update the PFN field of the page-table entry (PTE) to reflect the new page’s address in memory, and retry the instruction.      When to swap out(swap page to disk)?High/Low WatermarkWhen the OS detects that there are more pages in memory than the high watermark (HW), a background process called the swap daemon  starts to evict pages from memory until the number of pages is less than the low watermark (LW). The daemon then sleeps until the HW is reached again.Invoke by ProcessThe swap can also be awoken by a process if there are no free pages available; Once the daemon has freed up some pages, it will re-awaken the original thread, which will then be able to page in the appropriate page and continue working.Performancement OptimizationMany systems, will cluster or group a number of pages and write them out to the swap partition all at once.Other useful commands  vmstatImplementing LRUProblemScanning a wide array of times to discover the least-recently-used page is expensive.Approximating LRU  When a page is referenced (read or written), the hardware sets the use bit to 1.  The system’s pages organized in a circle.  Initially, a clock hand points to any page.  When replacing a page, the OS checks if the use bit is 1 or 0.          If 1, page P was recently used. The usage bit for P is cleared (set to 0), and the clock hand is advanced one page (P + 1).      If the use bit is set to 0, the page is evicted (in the worst case, all pages have been recently used and we have now searched through the entire set of pages, clearing all the bits).      Dirty Pages  The clock algorithm may be altered to look for pages that are both unused and clean to evict first; if those aren’t found, then look for unused dirty pages, and so on.  Because if a page has been updated and is thus unclean, it must be evicted by writing it back to disk, which is costly.  The eviction is free if it has not been updated; the physical frame can simply be reused for other purposes without further I/O.  A modified/dirty bit should be included in the hardware to accommodate this behavior.Thrashing  Thrashing is used to describe the system is continuously paging because the memory demands of the operating processes simply outnumber the physical memory available.  The methods to address thrashing          not to execute a subset of them in the hopes that the pages of the reduced set of processes will fit in memory, allowing progress.      launch an out-of-memory killer; this daemon selects a memory-intensive process and kills it.      "
  },
  
  {
    "title": "Memory Space Management with paging",
    "url": "/posts/memory-space-management-with-paging/",
    "categories": "OS",
    "tags": "OS",
    "date": "2024-01-28 00:00:00 +0800",
    





    
    "snippet": "What is PagingPaging is another memory space management approach that dividing memory into fixed size of chuncks called pages. In contrast to segmentation, paging does not have external fragmentati...",
    "content": "What is PagingPaging is another memory space management approach that dividing memory into fixed size of chuncks called pages. In contrast to segmentation, paging does not have external fragmentation and support the abstraction of an address space effectively, regardless of how a process uses the address space since it won’t make assumptions about the way the heap and stack grow and how they are use.Address TranslationTo translate the virtual address the process generates:  We have to break the resulting virtual address into two parts:          The virtual page number (VPN) and      The offset within the page.        Using our VPN, we can now index our page table and find out which physical frame virtual page lives in.Page Table  The page table is a data structure that maps virtual addresses (or virtual page numbers) into physical addresses (physical frame numbers).  Each process has its own page table.Linear Page TableLinear Page table is an array.  VPN is an index of the array.  Each page table entry(PTE) contains PFN and other useful bits.The steps of address translation by hardware// Extract the VPN from the virtual addressVPN = (VirtualAddress &amp; VPN_MASK) &gt;&gt; SHIFT// Form the address of the page-table entry (PTE)PTEAddr = PTBR + (VPN * sizeof(PTE))// Fetch the PTEPTE = AccessMemory(PTEAddr)// Check if process can access the pageif (PTE.Valid == False)    RaiseException(SEGMENTATION_FAULT)else if (CanAccess(PTE.ProtectBits) == False)    RaiseException(PROTECTION_FAULT)else    // Access is OK: form physical address and fetch itoffset = VirtualAddress &amp; OFFSET_MASKPhysAddr = (PTE.PFN &lt;&lt; PFN_SHIFT) | offsetRegister = AccessMemory(PhysAddr)"
  },
  
  {
    "title": "Advanced Page Table",
    "url": "/posts/advanced-page-table/",
    "categories": "OS",
    "tags": "OS",
    "date": "2024-01-28 00:00:00 +0800",
    





    
    "snippet": "Problem of Linear Page Table  The size of the page table is too big.  We need to allocate the physical memory for the page table entries that there is no physical frame as well.Larger Page size  On...",
    "content": "Problem of Linear Page Table  The size of the page table is too big.  We need to allocate the physical memory for the page table entries that there is no physical frame as well.Larger Page size  One way to make the page table smaller is make the page size larger because it makes the number of page table entries to be decreased.  The major drawback to this strategy is that large pages result in waste within each page, a problem known as internal fragmentation. Applications allocate pages but only use small portions of each, and memory quickly fills up with these excessively large pages.Combining Paging and segmentation  The goal of combining paging and segmentation is removing the erroneous entries between the heap, and stack segments(unallocated pages between the stack and the heap are no longer needed) to reduce the size of page table.  Instead of one page table for the process’s whole address space, we have one page table for each segments(code, heap, and stack) so we have three page tables.  The base register holds the physical address of the segment’s page table.  The limits register indicates the page table’s end (i.e., how many valid pages it has).  During a context switch, these registers must be updated to reflect the new process’s page tables.  On a TLB miss (assuming a hardware-managed TLB), the hardware utilizes the segment bits (SN) to identify which base and bounds pair to use. The hardware then combines the physical address with the VPN to generate the page table entry (PTE) address.SN           = (VirtualAddress &amp; SEG_MASK) &gt;&gt; SN_SHIFTVPN          = (VirtualAddress &amp; VPN_MASK) &gt;&gt; VPN_SHIFTAddressOfPTE = Base[SN] + (VPN * sizeof(PTE))Virtual Address:|****SN*****|*****VPN******|*****OFFSET******|  The downsides of this approach:          Use segmentation so assumes a fixed address space utilization pattern; a huge but sparsely used heap      External fragmentation: page tables can now be any size (in multiples of PTEs). Finding memory space for them is more difficult.      Multi-Level Page Tables  This is approach to get rid of all those incorrect sections in the page table without using segmentation.  It first divide the page table into page-sized units.  If all page-table entries (PTEs) of that page are invalid, then do not assign that page of the page table at all.  So, it’s generally compact and supports sparse address spaces.  To know the memory location of the pages of the page table and their validities, it use the new data structure called page directory.  When the OS wants to allocate or grow a page table, it may simply grab the next free page-sized unit(the size is much smaller than the size of page table).  But, on a TLB miss, two loads from memory are necessary to acquire the proper translation information from the page table (one for the page directory, and one for the PTE itself).  For 2-level page table, to find out the page table entry, we can use base pointer + PD.index * sizeof(page directory) to find out the address of page-sized unit = PD.PFN , then we use PD.PFN + PT.index * sizeof(PTE) to find out the PTE address.|******** VPN **********************|****** Offset *********|| 13 | 12 | 11 | 10 | 9 | 8 | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0 ||**** PD.Index *****|** PT.index ***|Inverted Page Table  Rather than having many page tables (one for each system process), we have a single page table with an item for each physical page of the system.  This entry indicates which process uses this page and which virtual page of that process corresponds to this physical page.  A hash table is frequently added on top of the underlying structure to speed up lookups.Swapping the Page Tables to DiskSome systems store page tables in kernel virtual memory, allowing the system to swap portions of these page tables to disk if memory becomes scarce."
  },
  
  {
    "title": "Transaction Lookaside Buffer",
    "url": "/posts/Transaction-Loodaside-Buffer/",
    "categories": "OS",
    "tags": "OS",
    "date": "2024-01-28 00:00:00 +0800",
    





    
    "snippet": "Problem of PagingPaging needs an additional memory lookup in order to translate each virtual address, but it takes too long to obtain translation information before every instruction fetch, load, o...",
    "content": "Problem of PagingPaging needs an additional memory lookup in order to translate each virtual address, but it takes too long to obtain translation information before every instruction fetch, load, or store.What is Transaction Lookaside Buffer?In order to speed up the process of address translation, we use the hardware cache for the address translation. This cache is called Transaction Lookaside Buffer(TLF) which is part of the MMU.TLB EntryTLBs generally include 32 or 64 of TLB entries. A few are for the OS (using the G bit). The OS can set a wired register to instruct the hardware how many TLB slots to reserve for it. The OS uses these reserved mappings for code and data it needs to access during key moments when a TLB miss would be troublesome (e.g., in the TLB miss handler).TLB Entry:  Virtual Page Number (VPN)  Process ID (PID) or Address Space ID(ASID)  Page Frame Number (PFN)  V bit: Valid bit - indicates whether the entry has a valid translation or not  G bit: Global bit - If set, TLB does not check PID for translation  …What if TLB missHardware approach  If the virtual address does not in the TLB entries, we have to check the page table to find the translation.  The hardware has to know the exact location of the page tables in memory (through a page-table base register)OS approach  The hardware mimics an exception, pausing the current instruction stream, switching to kernel mode, and jumping to a trap handler.  Returning from a TLB miss-handling trap causes the hardware to retry the instruction, resulting in a TLB hit.  OS must avoid creating endless loops of TLB misses by keeping the TLB miss handler in physical memory.          reserve some TLB entries for always valid transaction. Or      unmapped and not subject to address translation.        OS can use any data structure it wants to implement the page table.Array Access ExampleA memory array of 10 4-byte integers.The page size is 16 bytes.            VPN      Offset 0-4      Offset 5-8      Offset 9-12      Offset 13-16                  VPN 0                           arr[0]              VPN 1      arr[1]      arr[2]      arr[3]      arr[4]              VPN 2      arr[5]      arr[6]      arr[7]      arr[8]              VPN 3      arr[9]                           The TLB hit rate the first time the array is accessedThe hit rate is 60%.  arr[0]: Miss (VPN 0 stored in TLB)  arr[1]: Miss (VPN 1 stored in TLB)  arr[2]: Hit (VPN 1)  arr[3]: Hit (VPN 1)  arr[4]: Hit (VPN 1)  arr[5]: Miss (VPN 2 stored in TLB)  arr[6]: Hit (VPN 2)  arr[7]: Hit (VPN 2)  arr[8]: Hit (VPN 2)  arr[9]: Miss (VPN 3 stored in TLB)The TLB hit rate the second time the array is accessedThe hit rate is 100% because VPN 0-4 stored in TLB already in the first time access.Context SwitchingHow to make sure the process does not reuse the TLB entries of the old process?  flushing: clears the TLB by setting all valid bits to 0.  ASID: TLBs include an address space identifier (ASID) field. The ASID is a Process ID (PID) with less bits. So the TLB can hold several processes’ translations.Two entries for two processes with two VPNs point to the same physical pageWhen two processes share a page (for example, a code page), this can occur. Also, it reduces memory overheads by reducing the number of physical pages needed.            VPN      PFN      ASID      Prot-bit      Valid-bit                  VPN 0      PFN 100      1      r-x      1              VPN 5      PFN 100      2      r-x      1      TLB Replacement Policy  LRU  Random"
  },
  
  {
    "title": "Memory Segmentation",
    "url": "/posts/memory-segmentation/",
    "categories": "OS",
    "tags": "OS",
    "date": "2024-01-27 00:00:00 +0800",
    





    
    "snippet": "What is Memory Segmentation?  It allows the segments(code, stack, heap) of the address space can be stored in different physical memory locations so that we do not need to allocate the physical mem...",
    "content": "What is Memory Segmentation?  It allows the segments(code, stack, heap) of the address space can be stored in different physical memory locations so that we do not need to allocate the physical memory for the “free” segment.  Each segment has its own base/bound registers.Which segment the virtual memory address related to?Explicit Approach  we divide the address space into segments based on the first few bits of the virtual address.  the top 2 most bits represent which segment the address corresponds to.  the other bits represent the offset.// get top 2 bits of 14-bit VASegment = (VirtualAddress &amp; SEG_MASK) &gt;&gt; SEG_SHIFT// now get offsetOffset  = VirtualAddress &amp; OFFSET_MASKif (Offset &gt;= Bounds[Segment])  RaiseException(PROTECTION_FAULT)else  PhysAddr = Base[Segment] + Offset  Register = AccessMemory(PhysAddr)Implicit Approach  determines the segment by examining the address.  If the address came from the program counter (i.e., an instruction fetch), it’s in the code segment.  If it came from the stack or base pointer, it’s in the stack segment.  All others are in the heap.How to handle stack  The difference between the stack and the other segment is it now grows backwards (towards lower addresses).  We need more hardware support so that the hardware knows the segment grows positive or negative from the base address.  We can get the correct physical address by base address + offset - max segment size.            Segment      Base      Size (Max 4K)      Grows Positive?                  Code      32K      2K      1              Heap      34K      3K      1              Code      28K      2K      0      Segmentation presents new challenges for the OS  The segment registers must be saved and restored becase each process has its own virtual address space for context switch.  Able to update the segment size register to the new (larger/smaller) size.  Able to find physical memory space for new address spaces.  handle external fragmentation: physical memory soon fills up with pockets of free space, making it impossible to assign new segments or expand old ones."
  },
  
  {
    "title": "Why Memory Virtualisation?",
    "url": "/posts/why-memory-virtualisation/",
    "categories": "OS",
    "tags": "OS",
    "date": "2024-01-26 00:00:00 +0800",
    





    
    "snippet": "  Transparency: user program dont need to worry about the physical memory address.  Protection: each process only can manipulate its own address space.",
    "content": "  Transparency: user program dont need to worry about the physical memory address.  Protection: each process only can manipulate its own address space."
  },
  
  {
    "title": "Multilevel Feedback Queue Scheduling",
    "url": "/posts/mlfq-scheduling-policy/",
    "categories": "OS",
    "tags": "OS",
    "date": "2024-01-25 00:00:00 +0800",
    





    
    "snippet": "Example RunEnter the number of processes: 3Enter duration for process 1: 10Process 1 enqueued in queue with time quantum 2Enter duration for process 2: 20Process 2 enqueued in queue with time quant...",
    "content": "Example RunEnter the number of processes: 3Enter duration for process 1: 10Process 1 enqueued in queue with time quantum 2Enter duration for process 2: 20Process 2 enqueued in queue with time quantum 2Enter duration for process 3: 30Process 3 enqueued in queue with time quantum 2Process 1 dequeued from queue with time quantum 2Process 1 is running in high priority queueProcess 1 enqueued in queue with time quantum 4Process 2 dequeued from queue with time quantum 2Process 2 is running in high priority queueProcess 2 enqueued in queue with time quantum 4Process 3 dequeued from queue with time quantum 2Process 3 is running in high priority queueProcess 3 enqueued in queue with time quantum 4Process 1 dequeued from queue with time quantum 4Process 1 is running in medium priority queueProcess 1 enqueued in queue with time quantum 8Process 2 dequeued from queue with time quantum 4Process 2 is running in medium priority queueProcess 2 enqueued in queue with time quantum 8Process 3 dequeued from queue with time quantum 4Process 3 is running in medium priority queueProcess 3 enqueued in queue with time quantum 8Process 1 dequeued from queue with time quantum 8Process 1 is running in low priority queueProcess 1 finished executionProcess 2 dequeued from queue with time quantum 8Process 2 is running in low priority queueProcess 2 enqueued in queue with time quantum 8Process 3 dequeued from queue with time quantum 8Process 3 is running in low priority queueProcess 3 enqueued in queue with time quantum 8Process 2 dequeued from queue with time quantum 8Process 2 is running in low priority queueProcess 2 finished executionProcess 3 dequeued from queue with time quantum 8Process 3 is running in low priority queueProcess 3 enqueued in queue with time quantum 8Process 3 dequeued from queue with time quantum 8Process 3 is running in low priority queueProcess 3 finished executionProcess Duration        Waiting Time    Turnaround Time1       10      12      222       20      24      443       30      30      60Example C Code#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;typedef struct {  int id;  int duration;  int remaining_time;  int waiting_time;  int turnaround_time;} Process;typedef struct {  Process* processes;  int front;  int rear;  int time_quantum;} Queue;void init_queues(Queue* high_q, Queue* mid_q, Queue* low_q){  high_q-&gt;processes = malloc(10*sizeof(Process));  mid_q-&gt;processes = malloc(10*sizeof(Process));  low_q-&gt;processes = malloc(10*sizeof(Process));  high_q-&gt;time_quantum = 2;  mid_q-&gt;time_quantum = 4;  low_q-&gt;time_quantum = 8;  high_q-&gt;front = -1;  mid_q-&gt;front = -1;  low_q-&gt;front = -1;  high_q-&gt;rear = -1;  mid_q-&gt;rear = -1;  low_q-&gt;rear = -1;}void enqueue(Queue* q, Process* p){  printf(\"Process %d enqueued in queue with time quantum %d\\n\", p-&gt;id, q-&gt;time_quantum);  if(q-&gt;front &gt; 9) { return; }  q-&gt;front += 1;  q-&gt;processes[q-&gt;front].id = p-&gt;id;  q-&gt;processes[q-&gt;front].duration = p-&gt;duration;  q-&gt;processes[q-&gt;front].remaining_time = p-&gt;remaining_time;  q-&gt;processes[q-&gt;front].waiting_time = p-&gt;waiting_time;  q-&gt;processes[q-&gt;front].turnaround_time = p-&gt;turnaround_time;}Process* dequeue(Queue* q){  if(q-&gt;rear &gt;= q-&gt;front) {     q-&gt;rear = -1;    q-&gt;front = -1;    return NULL;  }  q-&gt;rear += 1;  printf(\"Process %d dequeued from queue with time quantum %d\\n\", q-&gt;processes[q-&gt;rear].id, q-&gt;time_quantum);  return &amp;q-&gt;processes[q-&gt;rear];}void mlfq(Process* processes, int n, Queue* high_q, Queue* mid_q, Queue* low_q) {  Process* current_p;  int total_turnaround_time = 0;  if (n&lt;= 0) { return; }  while(1){    current_p = dequeue(high_q);    if(current_p != NULL){      printf(\"Process %d is running in high priority queue\\n\", current_p-&gt;id);      if (current_p-&gt;duration &gt; high_q-&gt;time_quantum) {        current_p-&gt;duration -= high_q-&gt;time_quantum;        enqueue(mid_q, current_p);        total_turnaround_time += high_q-&gt;time_quantum;      } else {        total_turnaround_time += current_p-&gt;duration;        processes[current_p-&gt;id-1].turnaround_time = total_turnaround_time;        processes[current_p-&gt;id-1].waiting_time = total_turnaround_time - processes[current_p-&gt;id-1].duration;      }      continue;    }    current_p = dequeue(mid_q);    if(current_p != NULL){      printf(\"Process %d is running in medium priority queue\\n\", current_p-&gt;id);      if (current_p-&gt;duration &gt; mid_q-&gt;time_quantum) {        current_p-&gt;duration -= mid_q-&gt;time_quantum;        enqueue(low_q, current_p);        total_turnaround_time += mid_q-&gt;time_quantum;      }       else {        total_turnaround_time += current_p-&gt;duration;        processes[current_p-&gt;id-1].turnaround_time = total_turnaround_time;        processes[current_p-&gt;id-1].waiting_time = total_turnaround_time - processes[current_p-&gt;id-1].duration;        printf(\"Process %d finished execution\\n\", current_p-&gt;id);      }      continue;    }    current_p = dequeue(low_q);      if(current_p != NULL){      printf(\"Process %d is running in low priority queue\\n\", current_p-&gt;id);      if (current_p-&gt;duration &gt; low_q-&gt;time_quantum) {        current_p-&gt;duration -= low_q-&gt;time_quantum;        enqueue(low_q, current_p);        total_turnaround_time += low_q-&gt;time_quantum;      }       else {        total_turnaround_time += current_p-&gt;duration;        processes[current_p-&gt;id-1].turnaround_time = total_turnaround_time;        processes[current_p-&gt;id-1].waiting_time = total_turnaround_time - processes[current_p-&gt;id-1].duration;        printf(\"Process %d finished execution\\n\", current_p-&gt;id);      }      continue;    }    printf(\"\\n\");    break;    }}int main(){  int n;  float total_waiting_time=0, total_turnaround_time=0;  Queue* high_q = malloc(sizeof(Queue));  Queue* mid_q = malloc(sizeof(Queue));  Queue* low_q = malloc(sizeof(Queue));    printf(\"Enter the number of processes: \");  scanf(\"%d\", &amp;n);  Process* processes = malloc(n*sizeof(Process));    init_queues(high_q, mid_q, low_q);  for(int i=0;i&lt;n;i++){    processes[i].id = i+1;    printf(\"Enter duration for process %d: \", i+1);    scanf(\"%d\", &amp;processes[i].duration);    enqueue(high_q, &amp;processes[i]);  }  mlfq(processes, n, high_q, mid_q, low_q);  printf(\"Process\\tDuration\\tWaiting Time\\tTurnaround Time\\n\");  for(int i=0; i&lt;n; i++){    printf(\"%d\\t%d\\t%d\\t%d\\n\", processes[i].id, processes[i].duration, processes[i].waiting_time, processes[i].turnaround_time);    total_turnaround_time += processes[i].turnaround_time;    total_waiting_time += processes[i].waiting_time;  }  free(processes);  return 0;}"
  }
  
]

