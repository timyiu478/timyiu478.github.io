[
  
  {
    "title": "Multi CPU Scheduling",
    "url": "/posts/multi-cpu-scheduling/",
    "categories": "OS",
    "tags": "OS",
    "date": "2024-02-10 00:00:00 +0800",
    





    
    "snippet": "Key Questions  How to Schedule Jobs on Multiple CPUs? Strategies and mechanisms for effectively distributing tasks across multiple processing units.  Emerging Challenges in Multiprocessor Schedulin...",
    "content": "Key Questions  How to Schedule Jobs on Multiple CPUs? Strategies and mechanisms for effectively distributing tasks across multiple processing units.  Emerging Challenges in Multiprocessor Scheduling: Identifying and addressing unique issues that arise when transitioning from single to multiple CPU systems.  Load Balancing in Multi-Queue Multiprocessor Schedulers: Methods to achieve optimal workload distribution among CPUs, ensuring efficient utilization of resources.Implications for Applications  Conventional Applications: Standard applications, like a C program, typically utilize only one CPU. Therefore, adding more CPUs doesn’t inherently boost their performance.  Multi-Threaded Applications: These applications can distribute their workload across multiple CPUs, achieving significant performance gains.Role of Caches in Multiprocessor Systems  Caches are small, fast memory units that store copies of data from the system’s main memory, crucial for enhancing processing speed.  Each processor has its own chache but several processors share a single main memory.Challenges in Multiprocessor CachingHow data is shared and cached across multiple CPUs, affecting data access patterns and efficiency.Cache Coherence ProblemThe problem of cache coherence becomes evident when processes migrate between CPUs or when multiple CPUs access the same memory block.  Process Migration: If a program initially running on CPU A switches to CPU B, the latter CPU lacks immediate access to the previously cached data in CPU A’s cache.  Inefficient Data Access: CPU B, lacking the needed data in its cache, must access the main memory, leading to inefficiencies and potential data inconsistencies.  Inconsistency Risks: The situation becomes even more complicated if both CPUs attempt to modify the same memory location, leading to potential data inconsistencies between their caches and the main memory.Addressing Cache Coherence  Modern multiprocessor systems implement hardware-level mechanisms to ensure cache coherence. These mechanisms can detect and manage data that is simultaneously accessed or modified by multiple CPUs.  Operating systems and applications can also be designed to minimize cache coherence issues, for instance, by managing process allocation to CPUs or optimizing memory access patterns.Synchronization in Multiprocessor SystemsEven with cache coherence protocols in place, challenges persist in ensuring consistent and correct data manipulation.Case Study: Shared Linked ListImagine a scenario where two CPU threads concurrently execute a routine to remove an element from a shared linked list:  Thread 1 executes the first line, storing the current head value in its tmp variable.  Thread 2 also executes the same line, capturing the same head value in its separate tmp variable (since tmp is stack-allocated and thus private to each thread).Both threads attempt to remove the same element, leading to potential data corruption or other unintended consequences.To prevent the concurrent access issues, we use mutex locks for the critical section (data manipulation part).Cache Affinity  A multiprocessor scheduler should consider Cache affinity when scheduling, and thus if possible keeping a process on the same CPU.  When a process runs on the same CPU repeatedly, benefiting from the state information accumulated in that CPU’s caches  When a process is frequently shifted between different CPUs, it faces a performance hit. Each CPU switch forces the process to reload its state into a new cache, which is a slower operation.Single-Queue Multiprocessor Scheduling (SQMS)SQMS involves placing all jobs in a single queue, adopting a structure similar to single-processor scheduling.  The use of locks to access the single queue can lead to performance bottlenecks, especially as the number of CPUs increases. The competition for the single lock results in increased overhead and reduced job execution time.  In SQMS, jobs tend to move between CPUs, which goes against the principle of cache affinity.Multi-Queue Multiprocessor Scheduling (MQMS)MQMS addresses the limitations of SQMS by assigning each CPU its own scheduling queue.  Each queue operates independently, significantly reducing the need for synchronization and information sharing across CPUs.  Jobs are more likely to remain on the same CPU across executions, allowing them to benefit from cache affinity  While MQMS supports cache affinity, it requires effective load balancing strategies to ensure that no single CPU is overwhelmed or underutilized.          A technique that move jobs around CPUs which we call migration.      Work stealing is one basic method the system used to decide how to perform such a migration.      Work Stealing  When work-stealing takes place, a queue that is low on jobs occasionally peeks into another queue, to determine how full it is.  The source to “steal” one or more jobs from a target queue to balance the load but this strategy creates friction.  Overlooking other queues causes significant overhead and scaling issues, which was the whole point of introducing MQMS.  Conversely, if you don’t glance at other queues often, you risk significant load imbalances.  Finding the proper threshold is a dark art in system policy design."
  },
  
  {
    "title": "Concurrency Bugs",
    "url": "/posts/concurrency-bugs/",
    "categories": "OS",
    "tags": "OS",
    "date": "2024-02-10 00:00:00 +0800",
    





    
    "snippet": "Types of Bugs  Non-Deadlock          Atomicity violation bugs.      Order violation bugs.        DeadlockAtomicity violation bugsA code region is intended to be atomic but the atomicity is not enfo...",
    "content": "Types of Bugs  Non-Deadlock          Atomicity violation bugs.      Order violation bugs.        DeadlockAtomicity violation bugsA code region is intended to be atomic but the atomicity is not enforced during execution. For example,Thread 1::if (thd-&gt;proc_info) {    fputs(thd-&gt;proc_info, ...);}Thread 2::thd-&gt;proc_info = NULL;Execution Order:  Thread 1: thd-&gt;proc_info  Thread 2: thd-&gt;proc_info = NULL;  Thread 1: fputs(thd-&gt;proc_info, ...);  Thread 1: crash as a NULL pointer will be dereferenced by fputsThe fix to this type of bug is generally to acquire this lock before accesses the shared structure.Order violation bugsThe desired order between two (groups of) memory accesses is flipped. For example,Thread 1::void init() {      mThread = PR_CreateThread(mMain, ...);}Thread 2::void mMain(...) {    mState = mThread-&gt;State;}Execution Order:  Thread 2: mState = mThread-&gt;State;  Thread 2: crash with NULL-pointer dereferenceThe fix to this type of bug is generally to enforce the order(e.g. use condition variables).DeadlockConditions for deadlock  Mutual exclusion: Threads claim exclusive control of resources that they require (e.g., a thread grabs a lock).  Hold-and-wait: Threads hold resources allocated to them(e.g., locks that they have already acquired) while waiting for additional resources (e.g., locks that they wish to acquire).  No preemption: Resources (e.g., locks) cannot be forcibly removed from threads that are holding them.  Circular wait: There exists a circular chain of threads such that each thread holds one or more resources (e.g., locks) that are being requested by the next thread in the chain.If any of these four conditions are not met, deadlock cannot occur.PreventionCircular wait  To prevent curcular wait, Write your locking code such that you never induce a circular wait.  One way to do that is provide a total ordering on lock acquisition(e.g. always acquire lock 1 before lock 2).  In complext system that involves many locks, partial ordering is useful as well.  But odering just a convention that programmer can be ingored and it requires deep understanding to the code base.Hold-and-wait  To prevent hold-and-wait, acquiring all locks at once, atomically.  It requires that any thread grabs a lock any time it first acquires the global prevention lock.  This approach requires us to know exactly which locks must be held and to acquire them ahead of time.  This technique also is likely to decrease concurrency as all locks must be acquired early on (at once) instead of when they are truly needed.No preemption  To prevent no preemption, we either grabs the lock (if it is available) and returns success or returns an error code indicating the lock is held; in the latter case, you can try again later if you want to grab that lock.  The try-lock approach to allow a developer to back out of lock ownership (i.e., preempt their own ownership) in a graceful way.top:    pthread_mutex_lock(L1);    if (pthread_mutex_trylock(L2) != 0) {        pthread_mutex_unlock(L1);        goto top;    }  But livelock can be occurred that two threads could both be repeatedly attempting this sequence and repeatedly failing to acquire both locks. For example, the livelock is happended if the following execution order keep repeat.  Thread 1: lock(L1)  Thread 2: lock(L2)  Thread 1: try_lock(L2) != 0  Thread 2: try_lock(L1) != 0  Thread 1: unlock(L1)  Thread 2: unlock(L2)  To address the livelock problem, one could add a random delay before looping back and trying the entire thing over again, thus decreasing the odds of repeated interference among competing threads.Mutual Exclusion  To prevent mutual exclusion, we avoid the need for mutual exclusion at all.  E.g. design data structures without lock at all(lock-free or wait-free) with the help of powerful hardware instructions.  But design a lock-freee data structure is non-trival.Avoidance via SchedulingAvoidance requires some global knowledge of which locks various threads might grab during their execution, and subsequently, schedules said threads in a way as to guarantee no deadlock can occur.Detection And Recovery  Allow deadlocks to occur occasionally, and then take some action once such a deadlock has been detected.  Many database systems employ deadlock detection and recovery techniques. A deadlock detector runs periodically, building a resource graph and checking it for cycles. In the event of a cycle (deadlock), the system needs to be restarted.  If more intricate repair of data structures is first required, a human being may be involved to ease the process."
  },
  
  {
    "title": "Semaphore",
    "url": "/posts/semaphore/",
    "categories": "OS",
    "tags": "OS",
    "date": "2024-02-09 00:00:00 +0800",
    





    
    "snippet": "What is Semaphore?Semaphores serve as a unified primitive, adept at handling synchronization tasks traditionally managed by both locks and condition variables.Essence of a Semaphore  Semaphores ope...",
    "content": "What is Semaphore?Semaphores serve as a unified primitive, adept at handling synchronization tasks traditionally managed by both locks and condition variables.Essence of a Semaphore  Semaphores operate on a integer value, manipulated by two pivotal routines:          sem_wait(): Decreases the semaphore value by one. If this action would lead to a negative value, the thread must wait.      sem_post(): Increments the semaphore value by one, with the potential to wake a waiting thread if such a thread exists.        A semaphore is initialized with a value that will dictate its forthcoming behavior e.g 1 is promising mutual exclusivity. For threads of a single process, the second argument to remains 0, keeping the semaphore’s scope localized in its process.  It is an atomic operation by leveraging locks and condition variables.Semaphores For OrderingA parent thread must wait for a child thread to completesem_t s;void *child(void *arg) {    printf(\"child\\n\");    sem_post(&amp;s); // Signal: child is done    return NULL;}int main(int argc, char *argv[]) {    sem_init(&amp;s, 0, 0); // Initialize semaphore to 0    printf(\"parent: begin\\n\");    pthread_t c;    Pthread_create(&amp;c, NULL, child, NULL);    sem_wait(&amp;s); // Parent waits for the child    printf(\"parent: end\\n\");    return 0;}Evaluations  If the parent thread executes sem_wait(&amp;s) before the child runs, the semaphore decrement will cause the parent to wait, given its initial value of 0.  If the child complete first and invoke sem_post(&amp;s), the semaphore’s value becomes 1, allowing the parent to bypass waiting and continue execution upon reaching sem_wait(&amp;s).  In both cases, the semaphore’s initial value of 0 ensures that the parent thread will only proceed after the child has signaled its completion.The producer/consumer or bounded buffer problemSemaphores can act as versatile tools to handle this problem, functioning as locks when initialized to 1 and as signaling mechanisms when initialized to 0(a thread called sem_wait() must have to wait another thread to call sem_post() to wake it up). The initialization value of a semaphore typically reflects the count of resources available for distribution at startup.Requirements  Mutual Exclusion: Only one producer can write to a buffer slot at a time, and only one consumer can read from a buffer slot at a time.  Correct Ordering: Producers must wait for available space, and consumers must wait for available data.C++ Implementation#include &lt;stdio.h&gt;#include &lt;pthread.h&gt;#include &lt;semaphore.h&gt;#define MAX 10 // Define the size of the buffer#define loops 20 // Define the number of iterationsint buffer[MAX]; // Shared bufferint fill = 0; // Index for the next item to be producedint use = 0; // Index for the next item to be consumedsem_t empty; // Semaphore indicating the number of empty slotssem_t full; // Semaphore indicating the number of full slotssem_t mutex; // Binary semaphore used as a mutexvoid put(int value) {    buffer[fill] = value;    fill = (fill + 1) % MAX;}int get() {    int tmp = buffer[use];    use = (use + 1) % MAX;    return tmp;}void *producer(void *arg) {    for (int i = 0; i &lt; loops; i++) {        sem_wait(&amp;empty); // Wait for an empty slot        sem_wait(&amp;mutex); // Acquire mutex        put(i); // Produce an item        sem_post(&amp;mutex); // Release mutex        sem_post(&amp;full); // Signal that a new item is available    }    return NULL;}void *consumer(void *arg) {    for (int i = 0; i &lt; loops; i++) {        sem_wait(&amp;full); // Wait for a full slot        sem_wait(&amp;mutex); // Acquire mutex        int tmp = get(); // Consume an item        sem_post(&amp;mutex); // Release mutex        sem_post(&amp;empty); // Signal that a slot is now empty        printf(\"Consumed: %d\\n\", tmp);    }    return NULL;}int main(int argc, char *argv[]) {    // Initialize semaphores    sem_init(&amp;empty, 0, MAX);    sem_init(&amp;full, 0, 0);    sem_init(&amp;mutex, 0, 1);    // Create producer and consumer threads    pthread_t p, c;    pthread_create(&amp;p, NULL, producer, NULL);    pthread_create(&amp;c, NULL, consumer, NULL);    // Wait for threads to complete    pthread_join(p, NULL);    pthread_join(c, NULL);    // Clean up    sem_destroy(&amp;empty);    sem_destroy(&amp;full);    sem_destroy(&amp;mutex);    return 0;}Evaluations  The mutex semaphore is used to protect critical sections within put() and get() for preventing simultaneous access by multiple producers or consumers.  The mutex semaphore is acquired only after successfully passing the empty or full semaphore wait for avoiding the deadlock issue.  If the mutex semaphore is acquired before passing the empty or full semaphore wait, the deadlock occurs when the consumer acquires the mutex and is then blocked waiting for a full signal. Meanwhile, the producer is blocked that it cannot put data and signal full because it cannot acquire the mutex held by the consumer.  The semaphores full and empty are still used to signal the availability of data and space in the buffer.Reader-Writer LockThe need of reader-writer lock  operations like data insertions alter the structure and necessitate exclusive access.  while read operations can often proceed in parallel without such restrictions.C++ Implementation#include &lt;stdio.h&gt;#include &lt;pthread.h&gt;#include &lt;semaphore.h&gt;typedef struct _rwlock_t {    sem_t lock; // Binary semaphore (basic lock)    sem_t writelock; // Allow ONE writer/MANY readers    int readers; // Number of readers in critical section} rwlock_t;void rwlock_init(rwlock_t *rw) {    rw-&gt;readers = 0;    sem_init(&amp;rw-&gt;lock, 0, 1);    sem_init(&amp;rw-&gt;writelock, 0, 1);}void rwlock_acquire_readlock(rwlock_t *rw) {    sem_wait(&amp;rw-&gt;lock);    rw-&gt;readers++;    if (rw-&gt;readers == 1) // First reader gets writelock        sem_wait(&amp;rw-&gt;writelock);    sem_post(&amp;rw-&gt;lock);    printf(\"Reader acquired read lock\\n\");}void rwlock_release_readlock(rwlock_t *rw) {    sem_wait(&amp;rw-&gt;lock);    rw-&gt;readers--;    if (rw-&gt;readers == 0) // Last reader lets it go        sem_post(&amp;rw-&gt;writelock);    sem_post(&amp;rw-&gt;lock);    printf(\"Reader released read lock\\n\");}void rwlock_acquire_writelock(rwlock_t *rw) {    sem_wait(&amp;rw-&gt;writelock);    printf(\"Writer acquired write lock\\n\");}void rwlock_release_writelock(rwlock_t *rw) {    sem_post(&amp;rw-&gt;writelock);    printf(\"Writer released write lock\\n\");}void *reader(void *arg) {    rwlock_t *rw = (rwlock_t *)arg;    rwlock_acquire_readlock(rw);    // Simulate reading operation    rwlock_release_readlock(rw);    return NULL;}void *writer(void *arg) {    rwlock_t *rw = (rwlock_t *)arg;    rwlock_acquire_writelock(rw);    // Simulate writing operation    rwlock_release_writelock(rw);    return NULL;}int main() {    rwlock_t rwlock;    rwlock_init(&amp;rwlock);    pthread_t r1, r2, w1, w2;    pthread_create(&amp;r1, NULL, reader, &amp;rwlock);    pthread_create(&amp;r2, NULL, reader, &amp;rwlock);    pthread_create(&amp;w1, NULL, writer, &amp;rwlock);    pthread_create(&amp;w2, NULL, writer, &amp;rwlock);    pthread_join(r1, NULL);    pthread_join(r2, NULL);    pthread_join(w1, NULL);    pthread_join(w2, NULL);    return 0;}Evaluations  a risk that readers could continually access the data structure, starving writers who are waiting for an opportunity to acquire the write lock.  don’t always result in performance improvement over simpler, faster locking primitives.Thread throttlingThread throttling is a semaphore application in concurrency control, where the goal is to limit the number of threads running simultaneously to prevent system slowdowns.Semaphore as a Solutionsem_t semaphore;sem_init(&amp;semaphore, 0, threshold); // threshold is the max allowed threadsvoid memory_intensive_operation() {    sem_wait(&amp;semaphore); // Enter region    // Perform memory-intensive work    sem_post(&amp;semaphore); // Leave region}Implementing SemaphoresUnlike traditional semaphores, where the negative value indicates the number of waiting threads, in this implementation, the semaphore value never goes below zero. This approach simplifies the implementation and aligns with current practices like those in Linux.typedef struct __Zem_t {    int value;    pthread_cond_t cond;    pthread_mutex_t lock;} Zem_t;// Initialize the semaphorevoid Zem_init(Zem_t *s, int value) {    s-&gt;value = value;    pthread_cond_init(&amp;s-&gt;cond, NULL);    pthread_mutex_init(&amp;s-&gt;lock, NULL);}// Semaphore wait operationvoid Zem_wait(Zem_t *s) {    pthread_mutex_lock(&amp;s-&gt;lock);    while (s-&gt;value &lt;= 0)        pthread_cond_wait(&amp;s-&gt;cond, &amp;s-&gt;lock);    s-&gt;value--;    pthread_mutex_unlock(&amp;s-&gt;lock);}// Semaphore post operationvoid Zem_post(Zem_t *s) {    pthread_mutex_lock(&amp;s-&gt;lock);    s-&gt;value++;    pthread_cond_signal(&amp;s-&gt;cond);    pthread_mutex_unlock(&amp;s-&gt;lock);}Use Zem_t to throttle threadsvoid *memory_intensive_operation(void *arg) {    Zem_t *sem = (Zem_t *)arg;    Zem_wait(sem);    printf(\"Thread entered memory-intensive region\\n\");    sleep(1);    // Simulate memory-intensive work    Zem_post(sem);    printf(\"Thread exited memory-intensive region\\n\");    return NULL;}int main() {    const int MAX_THREADS = 5; // Maximum number of concurrent threads in the region    Zem_t sem;    Zem_init(&amp;sem, 3); // Allowing 3 threads to enter the region simultaneously    pthread_t threads[MAX_THREADS];    for (int i = 0; i &lt; MAX_THREADS; i++) {        pthread_create(&amp;threads[i], NULL, memory_intensive_operation, &amp;sem);    }    for (int i = 0; i &lt; MAX_THREADS; i++) {        pthread_join(threads[i], NULL);    }    return 0;}"
  },
  
  {
    "title": "Condition Variables in Threading",
    "url": "/posts/condition-variables-in-threading/",
    "categories": "OS",
    "tags": "OS",
    "date": "2024-02-09 00:00:00 +0800",
    





    
    "snippet": "What is Condition Variables?Condition variables in threading allow threads to wait for certain conditions to be true. When the condition changes, threads can be woken up by signaling the condition ...",
    "content": "What is Condition Variables?Condition variables in threading allow threads to wait for certain conditions to be true. When the condition changes, threads can be woken up by signaling the condition variable.Why we need Condition Variables?It allows a thread to wait for a specific condition without wasting CPU cycles.Mesa vs. Hoare Semantics  In Mesa semantics, signaling indicates a state change without guaranteeing the state upon the thread’s activation.  Hoare semantics ensures immediate action post-signaling but is harder to implement. Most systems opt for Mesa semantics.The Producer/Consumer (Bounded Buffer) ProblemThis problem involves producer threads creating data items and storing them in a share buffer, and consumer threads taking these items from the buffer for use.Synchronization requirementProducers only put data into the buffer when count is buffer is empty and consumers only retrieve data when count is buffer is full.Initial Implementation in C++int loops; // Initialized elsewherecond_t cond;mutex_t mutex;void *producer(void *arg) { for (int i = 0; i &lt; loops; i++) {  Pthread_mutex_lock(&amp;mutex);   if (count == 1)   Pthread_cond_wait(&amp;cond, &amp;mutex);  put(i);  Pthread_cond_signal(&amp;cond);  Pthread_mutex_unlock(&amp;mutex); }}void *consumer(void *arg) { for (int i = 0; i &lt; loops; i++) {  Pthread_mutex_lock(&amp;mutex);   if (count == 0)   Pthread_cond_wait(&amp;cond, &amp;mutex);  int tmp = get();  Pthread_cond_signal(&amp;cond);  Pthread_mutex_unlock(&amp;mutex);  printf(\"%d\\n\", tmp); }}Problem of Initial ImplementationThe issue stems from the buffer state changing after Consumer thread Tc1 is signaled but before it acts since the system use Mesa semantics.Assume there are 1 producer Tp and two consumers Tc1 and Tc2.  Tc1 checked count = 0 so it waits.  Tp puts item since count = 0 and then signals Tc1 to Ready state.  Before Tc1 can consume, Tc2 intervenes and consumes the buffer value.  When Tc1 resumes, it finds an empty buffer, leading to an assertion failure.Implementing a While Loop for Condition VariablesReplacing if statements with while loops in both producer and consumer functions. This ensures that whenever a thread wakes up, it rechecks the shared variable’s state.Unresolved Issue: DeadLock  Both consumers go to sleep as the buffer is empty.  Tp fills the buffer and signals, waking Tc1.  Tp producer keeps take control the CPU and try to add more data, it finds the buffer full and sleeps.  Tc1 consumes the buffer and signals, but if it accidentally wakes Tc2.  Tc2 finds an empty buffer and sleeps again.  Tp also asleep, leaves the system in a deadlock.Implementing Dual Condition Variables  The root of the problem lies in using only one condition variable cond for both full and empty states of the buffer. This can lead to incorrect signaling and subsequent deadlocks in a multi-threaded environment.  So we use two condition variables instead: empty and fill.  Producer threads wait on the empty condition. Once a buffer space is available (empty), they proceed to fill the buffer and then signal fill, indicating the buffer is no longer empty.  Consumer threads wait for the fill condition, meaning the buffer has data. After consuming, they signal empty, indicating space availability in the buffer.Multiple buffer slotsTo further optimize for concurrency and efficiency, we introduce multiple buffer slots. This enhancement allows for multiple values to be produced and consumed without frequent sleeping, reducing context switches and boosting efficiency.Enhanced C++ Implementation#include &lt;stdio.h&gt;#include &lt;pthread.h&gt;int buffer[MAX];int fill_ptr = 0; // Points to where to fill nextint use_ptr = 0;  // Points to where to use nextint count = 0;    // Number of items in the buffervoid put(int value) { buffer[fill_ptr] = value; fill_ptr = (fill_ptr + 1) % MAX; // Circular increment count++;}int get() { int tmp = buffer[use_ptr]; use_ptr = (use_ptr + 1) % MAX; // Circular increment count--; return tmp;}cond_t empty, fill; // Condition variables for synchronizationmutex_t mutex; // Mutex for protecting shared datavoid *producer(void *arg) {    int i;    for (i = 0; i &lt; loops; i++) {        Pthread_mutex_lock(&amp;mutex); // p1: Acquire the mutex for exclusive access        while (count == MAX) // p2: Check if the buffer is full (use a while loop to handle spurious wake-ups)            Pthread_cond_wait(&amp;empty, &amp;mutex); // p3: Wait for the buffer to have space        put(i); // p4: Put the value into the buffer        Pthread_cond_signal(&amp;fill); // p5: Signal that the buffer is no longer empty        Pthread_mutex_unlock(&amp;mutex); // p6: Release the mutex    }}void *consumer(void *arg) {    int i;    for (i = 0; i &lt; loops; i++) {        Pthread_mutex_lock(&amp;mutex); // c1: Acquire the mutex for exclusive access        while (count == 0) // c2: Check if the buffer is empty (use a while loop to handle spurious wake-ups)            Pthread_cond_wait(&amp;fill, &amp;mutex); // c3: Wait for the buffer to have data        int tmp = get(); // c4: Get a value from the buffer        Pthread_cond_signal(&amp;empty); // c5: Signal that the buffer is no longer full        Pthread_mutex_unlock(&amp;mutex); // c6: Release the mutex        printf(\"%d\\n\", tmp); // Print the retrieved value    }}"
  },
  
  {
    "title": "Concurrent Data Structures",
    "url": "/posts/concurrent-data-structure/",
    "categories": "OS",
    "tags": "OS",
    "date": "2024-02-07 00:00:00 +0800",
    





    
    "snippet": "Challenges of Concurrency in Data Structures  What are the strategies for adding locks to a data structure to ensure correct, concurrent access?  How can locks be applied in a way that maintains th...",
    "content": "Challenges of Concurrency in Data Structures  What are the strategies for adding locks to a data structure to ensure correct, concurrent access?  How can locks be applied in a way that maintains the speed and allows simultaneous access by multiple threads?Ideal Scenario: Perfect ScalingPerfect scaling is achieved when the time taken for threads to complete tasks on multiple processors is as fast as on a single processor, despite the increase in workload.Concurrent Counter Data Structure by adding lockC++ Implementation#include &lt;stdio.h&gt;#include &lt;pthread.h&gt;// Counter with locktypedef struct __counter_t {    int value;    pthread_mutex_t lock;} counter_t;void init(counter_t *c) {    c-&gt;value = 0;    pthread_mutex_init(&amp;c-&gt;lock, NULL);}void increment(counter_t *c) {    pthread_mutex_lock(&amp;c-&gt;lock);    c-&gt;value++;    printf(\"Incremented: %d\\n\", c-&gt;value);    pthread_mutex_unlock(&amp;c-&gt;lock);}void decrement(counter_t *c) {    pthread_mutex_lock(&amp;c-&gt;lock);    c-&gt;value--;    printf(\"Decremented: %d\\n\", c-&gt;value);    pthread_mutex_unlock(&amp;c-&gt;lock);}int get(counter_t *c) {    pthread_mutex_lock(&amp;c-&gt;lock);    int rc = c-&gt;value;    pthread_mutex_unlock(&amp;c-&gt;lock);    return rc;}void* threadFunction(void* arg) {    counter_t *c = (counter_t*)arg;    for (int i = 0; i &lt; 5; ++i) {        increment(c);    }    for (int i = 0; i &lt; 3; ++i) {        decrement(c);    }    printf(\"Final value in thread: %d\\n\", get(c));    return NULL;}int main() {    counter_t counter;    init(&amp;counter);    pthread_t threads[2];    for (int i = 0; i &lt; 2; ++i) {        pthread_create(&amp;threads[i], NULL, threadFunction, &amp;counter);    }    for (int i = 0; i &lt; 2; ++i) {        pthread_join(threads[i], NULL);    }    printf(\"Final value in main: %d\\n\", get(&amp;counter));    return 0;}Evaluations  Single Lock Bottleneck: A single lock might not be sufficient for high-performance needs. Further optimizations may be required, which will be the focus of the rest of the chapter.  Sufficiency for Basic Needs: If performance isn’t a critical issue, a simple lock might suffice, and no further complexity is necessary.The Approximation Counter Approach  The logical counter is represented by a global counter and one local counter for each CPU core.  Each local counter is synchronized with its own local lock, and a global lock is used for the global counter.  When a thread wants to increment the counter, it updates its corresponding local counter, which is efficient due to reduced contention across CPUs.  Regularly, the value from a local counter is transferred to the global counter by acquiring the global lock. This process involves incrementing the global counter based on the local counter’s value and then resetting the local counter to zero.  The frequency of local-to-global updates is determined by a threshold S. A smaller S makes the counter behave more like a non-scalable counter, while a larger S enhances scalability at the expense of accuracy in the global count.Concurrent Operations in a Linked ListC++ Implementation#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;pthread.h&gt;// Define the structure for a linked list nodetypedef struct node {    int key;    struct node *next;} node_t;// Define the structure for the linked listtypedef struct list {    node_t *head;    pthread_mutex_t lock;} list_t;// Initialize the linked listvoid List_Init(list_t *L) {    L-&gt;head = NULL;    pthread_mutex_init(&amp;L-&gt;lock, NULL);}// Insert a new node with the given key at the beginning of the listvoid List_Insert(list_t *L, int key) {    // Allocate memory for a new node    node_t *new = malloc(sizeof(node_t));    if (new == NULL) {        perror(\"malloc\");        return;    }    new-&gt;key = key;    // Lock the critical section    pthread_mutex_lock(&amp;L-&gt;lock);    // Update the pointers to insert the new node at the beginning    new-&gt;next = L-&gt;head;    L-&gt;head = new;    // Unlock the critical section    pthread_mutex_unlock(&amp;L-&gt;lock);}// Lookup a key in the linked list and return 0 if found, -1 if not foundint List_Lookup(list_t *L, int key) {    int rv = -1;    // Lock the critical section    pthread_mutex_lock(&amp;L-&gt;lock);    // Traverse the linked list to find the key    node_t *curr = L-&gt;head;    while (curr) {        if (curr-&gt;key == key) {            rv = 0; // Key found            break;        }        curr = curr-&gt;next;    }    // Unlock the critical section    pthread_mutex_unlock(&amp;L-&gt;lock);    return rv; // Return 0 for success and -1 for failure}Evaluation  The lock is only held around the critical region of the insert or lookup operations. This makes the implementation more robust.  Assumed malloc() is thread-safe that can be called without a lock, reducing the duration for which the lock is held.  The lookup function has a single exit path for decreasing the likelihood of errors like forgetting to unlock before returning.Hand-Over-Hand Locking in Linked Lists  This approach involves adding a lock to each node of the list. As one traverses the list, they acquire the next node’s lock before releasing the current one.  Conceptually, hand-over-hand locking increases parallelism in list operations. However, in practice, the overhead of locking each node can be prohibitive, often making it less efficient than a single lock, especially for extensive lists and numerous threads. A hybrid method where a lock is acquired for every few nodes might be a more practical solution.Concurrent Queues  A queue involves two separate locks: one for the head and another for the tail.  Allows enqueue operations to primarily use the tail lock and dequeue operations to use the head lock, enabling concurrent execution of these functions.  A dummy node, allocated during the queue’s initialization, separates the head and tail operations, further enhancing concurrency.Concurrent Non-resizable Hash Table  Each hash bucket, represented by a list, has its own lock. This differs from using a single lock for the entire table.  By allocating a lock per hash bucket, the hash table allows multiple operations to occur simultaneously, significantly enhancing its performance."
  },
  
  {
    "title": "Threads Locks",
    "url": "/posts/threads-locks/",
    "categories": "OS",
    "tags": "OS",
    "date": "2024-02-05 00:00:00 +0800",
    





    
    "snippet": "What is a LockLock is a tool for allowing running critical section code atomically by only allow one thread to be execute that part of code.How does a lock work?  Lock Variable: e.g. mutex. It hold...",
    "content": "What is a LockLock is a tool for allowing running critical section code atomically by only allow one thread to be execute that part of code.How does a lock work?  Lock Variable: e.g. mutex. It holds the lock state.  It indicates whether the lock is free or acquired by a thread.  Acquiring a Lock: When a thread executes lock(&amp;mutex), it acquires the lock if it’s free, allowing the thread to enter the critical section.  Waiting Threads: If there are threads waiting (blocked in the lock() function), the lock won’t be set to free immediately. Instead, one of the waiting threads will enter the critical section.  Releasing a Lock: Using unlock(&amp;mutex) releases the lock, making it available for other threads.Supports for implementing lock  Hardware: special hardware instructions for implementing locks that are both efficient and effective.  OS: complements these hardware capabilities by providing the necessary context and controls to ensure that locks are used appropriately within the system.Key Criteria for Assessing Lock Efficacy  Basic Functionality: The primary function of a lock is to ensure mutual exclusion, keeping multiple threads out of a critical section simultaneously.  Equal Opportunity: Fairness in locks means each competing thread should have a reasonable chance of acquiring the lock once it’s available.  Avoiding Starvation: Assess whether any thread consistently fails to acquire the lock, leading to starvation.  Overhead Assessment: Consider the time overheads incurred by using the lock in different scenarios:          Single CPU, Single Thread      Single CPU, Multiple Threads      Multiple CPUs, Multiple Threads      Achieving mutual exclusion on single-processor systems by controlling interrupts.Idea  Before entering a critical section, interrupts are disabled to prevent interruption.  After exiting the critical section, interrupts are re-enabled.ProsSimple.Cons  Trust the programs.  Cant ensure mutual exclusion on multi-processor systems.  Prolonged disabling of interrupts might lead to missed signals, causing system issues.  Interrupt masking and unmasking operations can be slow on modern CPUs.Implementing a Flag-Based LockLock Structure and Functionstypedef struct __lock_t { int flag; } lock_t;void init(lock_t *mutex) {    // 0 -&gt; lock is available, 1 -&gt; held    mutex-&gt;flag = 0;}void lock(lock_t *mutex) {    while (mutex-&gt;flag == 1) // TEST the flag        ; // spin-wait (do nothing)    mutex-&gt;flag = 1; // now SET it!}void unlock(lock_t *mutex) {    mutex-&gt;flag = 0;}Issues  Potential for Race Conditions: a critical window between the check while (mutex-&gt;flag == 1) and the set mutex-&gt;flag = 1; so that more than one thread could acquire the lock. For example,          Thread 1: unlock()      Thread 2: mutex-&gt;flag == 1 is false      Thread 3: mutex-&gt;flag == 1 is false      Thread 2: mutex-&gt;flag = 1      Thread 3: mutex-&gt;flag = 1        Spin-wait: consume CPU with do nothing.Spin Lock with Test-And-SetTest-And-SetThe test-and-set instruction is a fundamental piece of hardware support for locking which is the atomic instruction.int TestAndSet(int *old_ptr, int new) {    // logical implementation that should be executed atomically in reality    int old = *old_ptr; // Fetch old value    *old_ptr = new;     // Store 'new' into old_ptr    return old;         // Return the old value}Lock Acquisition  If a thread calls lock() and no other thread holds the lock, TestAndSet(flag, 1) will return 0 (the old value), and the thread acquires the lock while setting the flag to 1.  If another thread already holds the lock (flag is 1), TestAndSet(flag, 1) will return 1, causing the thread to enter a spin-wait loop until the lock is released.Releasing the LockThe unlocking thread sets the flag back to 0, allowing other threads to acquire the lock.Evaluation  achieve mutual exclustion  potential for starvation  waste CPU cycles in the spin-wait loopSpin Lock with Compare-And-SwapCompare-And-Swap (CAS), known as Compare-And-Exchange on x86 architectures, is a hardware primitive provided by some systems to aid in concurrent programming.CAS Functionality in C Pseudocodeint CompareAndSwap(int *ptr, int expected, int new) {    int original = *ptr;    if (original == expected)        *ptr = new;    return original;}  CAS returns the original value at ptr, allowing the calling code to determine whether the update was successful.  atomically updates ptr with new if the value at the address specified by ptr equals expected.CAS Lock Implementionvoid lock(lock_t *lock) {    while (CompareAndSwap(&amp;lock-&gt;flag, 0, 1) == 1)        ; // Spin-wait}Compare with Test-And-Set  Lock acquisition and releasing are the same.  CAS is more flexible than test-and-set, opening possibilities for more advanced synchronization techniques like lock-free synchronization.Spin Lock with Load-Linked and Store-ConditionalLoad-Linked and Store-ConditionalLoad-Linked (LL) and Store-Conditional (SC) are pairs of instructions used on platforms like MIPS, ARM, and PowerPC to create locks and other concurrent structures  Load-Linked (LL): This instruction loads a value from a memory location into a register. It prepares for a subsequent conditional store.  Store-Conditional (SC): This instruction attempts to store a value to the memory location if no updates have been made to that location since the last load-linked operation. It returns 0 on failure, leaving the value unchanged.Lock Implementation in Pseudo C Codevoid lock(lock_t *lock) {    while (LoadLinked(&amp;lock-&gt;flag) ||            !StoreConditional(&amp;lock-&gt;flag, 1))        ; // Spin-wait}When two threads attempt to acquire the lock simultaneously after an LL operation, only one will succeed with the SC. The other thread’s SC will fail, forcing it to retry.Ticket Lock with Fetch-And-AddFetch-And-Add Representation in CThe fetch-and-add instruction atomically increments a value at a specific address while returning the old value.int FetchAndAdd(int *ptr) {    int old = *ptr;    *ptr = old + 1;    return old;}Ticket Lock Representation in C++class TicketLock {  private:    std::atomic&lt;int&gt; ticketCounter;    std::atomic&lt;int&gt; turn;  public:    TicketLock() : ticketCounter(0), turn(0) {}    void lock() {        int myTicket = ticketCounter.fetch_add(1); // Fetch-And-Add        while (turn.load(std::memory_order_relaxed) != myTicket) {            ; // Spin-wait        }    }    void unlock() {        turn.fetch_add(1); // Move to next ticket    }};  A thread acquires a ticket through an atomic fetch-and-add on the ticket counter. This ticket number (ticketCOunter) represents the thread’s place in the queue.  A thread enters the critical section when its ticket number matches the current turn (ticketCounter == turn).  To release the lock, the thread simply increments the turn variable, allowing the next thread in line to enter the critical section.Advantages of Ticket Lock  Guarantee Progress: ensures that all threads make progress. Each thread is assigned a ticket, and they enter the critical section in the order of their tickets.  Fairness: guarantees that each thread will eventually get its turn to enter the critical section.Yield to Avoid Spin WaitingBasic IdeaInstead of spinning, a thread could yield the CPU to another thread through a basic yield() system call, where the yielding thread de-schedules itself, changing its state from running to ready.Challenges  In a system with many threads competing for a lock, yielding can lead to frequent context switches. If one thread acquires the lock and is preempted, the other threads will sequentially find the lock held, yield, and enter a run-and-yield cycle.  The starvation problem is unaddressed.Sleep and Waiting QueueSolaris Implementation in C// Define a structure for the locktypedef struct __lock_t {    int flag;       // 0 if lock is available, 1 if locked    int guard;      // To prevent concurrent guard lock    queue_t *q;     // Queue to hold waiting threads} lock_t;// Initialize the lockvoid lock_init(lock_t *m) {    m-&gt;flag = 0;     // Lock is initially available    m-&gt;guard = 0;    // Guard is initially free    queue_init(m-&gt;q); // Initialize the queue for waiting threads}// Acquire the lockvoid lock(lock_t *m) {    while (TestAndSet(&amp;m-&gt;guard, 1) == 1)        ; // Acquire guard lock by spinning    if (m-&gt;flag == 0) {        m-&gt;flag = 1;   // Lock is acquired        m-&gt;guard = 0;  // Release the guard lock    } else {        queue_add(m-&gt;q, gettid()); // Add current thread to the waiting queue        m-&gt;guard = 0;              // Release the guard lock        park();                     // Put the thread to sleep    }}// Release the lockvoid unlock(lock_t *m) {    while (TestAndSet(&amp;m-&gt;guard, 1) == 1)        ; // Acquire guard lock by spinning    if (queue_empty(m-&gt;q))        m-&gt;flag = 0; // Release the lock; no one wants it    else        unpark(queue_remove(m-&gt;q)); // Wake up the next waiting thread    m-&gt;guard = 0; // Release the guard lock}Addressing Race ConditionsThe race condition can occurs just before the park call. For example,  Thread-1: m-&gt;guard = 0;  Thread-2: unlock(Thread-1);  Thread-1: park();To ensure unpark is called before park, the thread does not sleep unnecessarily by adding setpark() before m-&gt;guard = 0.queue_add(m-&gt;q, gettid());setpark(); // new codem-&gt;guard = 0;Or place the guard directly within the kernel, allowing for atomic operations in lock release and thread dequeuing."
  },
  
  {
    "title": "Thread API",
    "url": "/posts/thread-api/",
    "categories": "OS",
    "tags": "OS",
    "date": "2024-02-03 00:00:00 +0800",
    





    
    "snippet": "Thread Creation in POSIXFunctions  create thread: pthread_create  wait a thread to complete: pthread_joinExample Code#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;pthread.h&gt;// De...",
    "content": "Thread Creation in POSIXFunctions  create thread: pthread_create  wait a thread to complete: pthread_joinExample Code#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;pthread.h&gt;// Define structures for argument and return valuestypedef struct {    int a;    int b;} myarg_t;typedef struct {    int x;    int y;} myret_t;// Thread function to perform a task and return valuesvoid *mythread(void *arg) {    myret_t *rvals = (myret_t *)malloc(sizeof(myret_t));  // Allocate memory for return values    // Simulate some work and set return values    rvals-&gt;x = 1;    rvals-&gt;y = 2;    return (void *)rvals;  // Return the allocated struct}int main(int argc, char *argv[]) {    pthread_t p;          // Thread object    myret_t *rvals;       // Pointer to store returned values    myarg_t args = {10, 20};  // Example argument values    // Create a new thread with the specified function and arguments    pthread_create(&amp;p, NULL, mythread, &amp;args);    // Wait for the thread to finish and retrieve its return values    pthread_join(p, (void **) &amp;rvals);    // Print the returned values    printf(\"returned %d %d\\n\", rvals-&gt;x, rvals-&gt;y);    free(rvals);  // Free the allocated memory    return 0;}Mutual Exclusion with POSIX ThreadsMutexes are essential for protecting critical sections of code to ensure correct operation.Usage Examplepthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER; // Static Initialization to set mutex to default state.// int rc = pthread_mutex_init(&amp;lock, NULL); // Dynamic Initialization with NULL indicating default attributes.// assert(rc == 0); // Check for successful initializationpthread_mutex_lock(&amp;lock); // Acquire the lock. If the lock is already held by another thread, the calling thread will block until it can acquire the lock.x = x + 1; // Critical sectionpthread_mutex_unlock(&amp;lock); // Release the lockpthread_mutex_destroy(&amp;lock); // Clean upAdvanced Mutex Operations  Timed Locks: Acquire a lock with a timeout, useful in scenarios where avoiding deadlock is crucial.  Try Locks: Non-blocking lock attempts that can be useful in certain advanced programming scenarios.Condition variablesThey are used when threads need to signal each other to proceed with their tasks.Example C++ Code#include &lt;iostream&gt;#include &lt;pthread.h&gt;#include &lt;unistd.h&gt; // For sleep()pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER; // Mutexpthread_cond_t cond = PTHREAD_COND_INITIALIZER;  // Condition variable// Shared variablebool ready = false;// Thread function that waits for the conditionvoid* wait_thread(void* arg) {    pthread_mutex_lock(&amp;lock);    while (!ready) { // Using while loop for spurious wake-ups        std::cout &lt;&lt; \"Waiting thread is waiting for the condition...\" &lt;&lt; std::endl;        pthread_cond_wait(&amp;cond, &amp;lock);    }    std::cout &lt;&lt; \"Waiting thread received the signal.\" &lt;&lt; std::endl;    pthread_mutex_unlock(&amp;lock);    return NULL;}// Thread function that signals the conditionvoid* signal_thread(void* arg) {    sleep(1); // Sleep for demonstration purposes    pthread_mutex_lock(&amp;lock);    ready = true;    std::cout &lt;&lt; \"Signaling thread is signaling the condition...\" &lt;&lt; std::endl;    pthread_cond_signal(&amp;cond);    pthread_mutex_unlock(&amp;lock);    return NULL;}int main() {    pthread_t waitThread, signalThread;    // Create threads    pthread_create(&amp;waitThread, NULL, wait_thread, NULL);    pthread_create(&amp;signalThread, NULL, signal_thread, NULL);    // Wait for threads to finish    pthread_join(waitThread, NULL);    pthread_join(signalThread, NULL);    // Clean up    pthread_mutex_destroy(&amp;lock);    pthread_cond_destroy(&amp;cond);    return 0;}  Hold Lock During Signaling: Always hold the lock when signaling or modifying the global condition variable to avoid race conditions.  Lock Handling in Wait and Signal: The wait function requires the lock as it releases it when putting the thread to sleep. The lock is re-acquired before pthread_cond_wait returns. The signal function only needs the condition variable.  Rechecking the Condition: Use a while loop rather than an if statement to recheck the condition. This is because some implementations may wake up threads spuriously, without the condition actually being met.Caution Against Using Flags for Synchronization// Waiting codewhile (ready == 0) ; // Spin// Signaling codeready = 1;This approach can lead to excessive CPU usage (busy-waiting) and is prone to synchronization errors."
  },
  
  {
    "title": "Concurrency and Threads",
    "url": "/posts/concurrency-and-threads/",
    "categories": "OS",
    "tags": "OS",
    "date": "2024-02-03 00:00:00 +0800",
    





    
    "snippet": "Threads  Each thread is like an independent worker, but they all share the same memory space.  Each thread has its own Program Counter(PC) to keeps track of where a thread is in its execution.  Jus...",
    "content": "Threads  Each thread is like an independent worker, but they all share the same memory space.  Each thread has its own Program Counter(PC) to keeps track of where a thread is in its execution.  Just like a process has a control block to keep track of its state, each thread has its own Thread Control Block(TCB).  Each thread has its own stack. This means each thread can handle its own functions and data, keeping them in its personal stack.  Context switch is allowed by saving and loading different sets of registers for each thread.Why use threads?They are useful on parallelism and handling I/O (Input/Output) operations.ParallelismIn multi-processor system, we can speed up the process by dividing the task(e.g. adding numbers in a huge array) among multiple threads, with each processor handling a part of the job.Avoid delays caused by I/O operationsIf a program only used one thread and had to wait for an I/O operation to complete, it couldn’t do anything else during that time. By using multiple threads, one thread can handle the I/O operation while others continue processing or initiating more I/O requests.Why not just use multiple processes instead of threads?Threads have a special advantage: they share the same address space. This makes data sharing between threads straightforward and efficient, especially suitable for tasks that are closely related.Race Condition  When multiple threads access and modify the same data, unpredictable outcomes can occur because of the uncontrollable threads scheduling.  For example, in order to increment an counter, 3 machine codes will be generated by compiler.          load counter value from memory to register(e.g. eax).      increment the register.      store back the updated register value to counter memory location.        Two threads (Thread 1 and Thread 2) executing this sequence concurrently so that the code executed twice, but the counter incremented only once.          Thread 1: loading the counter value(10) into eax and increments it to 11.      Context Switch to Thread 2.      Thread 2: loading the counter value(10) into eax and increments it to 11, store its eax value(11) back to memory.      Context Switch to Thread 1.      Thread 1: store its eax value(11) back to memory.      Critical SectionFrom the above code sequence example, we can see that increment an counter is an critical section that the code that accesses a shared resource and should not be executed by more than one thread at a time.Mutual ExclusionTo prevent race conditions, we need mutual exclusion in critical sections. If one thread operates within the critical part, the others are stopped.Atomicity  In multi-threading, a major challenge is ensuring that certain operations are executed without interruption, maintaining consistency in shared data.  add instruction is one of an atomic instructions.  In practice, we often don’t have such atomic instructions for complex operations in regular instruction set and we have to break it down to multiple instructions.  Since we can’t rely on hardware for atomicity, we turn to synchronization primitives."
  },
  
  {
    "title": "VMS Lazy Optimizations",
    "url": "/posts/vms-lazy-optimizations/",
    "categories": "OS",
    "tags": "OS",
    "date": "2024-02-02 00:00:00 +0800",
    





    
    "snippet": "Demand Page Zeroing  Consider adding a page to the address space, say in the heap.  The OS have to zero the page for security reason so that the process cannot know how this page used for.  Instead...",
    "content": "Demand Page Zeroing  Consider adding a page to the address space, say in the heap.  The OS have to zero the page for security reason so that the process cannot know how this page used for.  Instead of the OS zeros the page before maps it into your address space, the OS  the OS just adds an entry to the page table marking it unavailable.  When the process reads or writes the page, it traps the OS. A demand-zero page is identified by the OS using certain bits designated in the “reserved for OS” portion of the page table entry.  The OS then finds a physical page, zeroes it, and maps it into the process’s address space.  This task(zeros the page) is avoided if the process never accesses the page.Copy-On-Write  Instead of copying a page from one address space to another, the OS can map it into the target address space and declare it read-only in both address spaces.  If both address spaces just read the page, no data is moved.  A page write attempt from one of the address spaces will be logged into the OS. The OS then allocate a new page, fill it with data, and map it into the address space of the faulting process.  So it saves unnecessary copying.References  VMS - https://en.wikipedia.org/wiki/OpenVMS"
  },
  
  {
    "title": "Page Swapping",
    "url": "/posts/page-swapping/",
    "categories": "OS",
    "tags": "OS",
    "date": "2024-01-30 00:00:00 +0800",
    





    
    "snippet": "What is page swappingSwap pages to disk so that the running programs to use more RAM than is physically accessible.Swap Space  Swap space is reserved space on the disk for moving pages between memo...",
    "content": "What is page swappingSwap pages to disk so that the running programs to use more RAM than is physically accessible.Swap Space  Swap space is reserved space on the disk for moving pages between memory and the file system.  This assumes the OS can read and write to swap space in page-sized units.The free CommandThe free command displays amount of free and used memory in the system.               total        used        free      shared  buff/cache   availableMem:         8086120     2908832      577556       56540     4599732     4815504Swap:        2097148          12     2097136  In the “Mem” or memory row, there is more “available” space than “free” because there are pages the system knows it can get rid of if needed.  The “Swap” row which reports the usage of the swap space as distinct from your memory.Swapping Mechanism  In a software-managed TLB architecture, the OS determines if a page exists in physical memory using a new piece of information in each page-table entry called the present bit.  A page fault occurs when a program accesses a page that isn’t in physical memory.  A page fault will require the OS to swap in a page from disk.          use the PTE’s data bits, like the page’s PFN, to store a disk address.      Once the page is located on the disk, it is swapped into memory via I/O. The process will be blocked while the I/O is running, so the OS can run other ready processes while the page fault is handled.      The OS will update the page table to reflect the new page, update the PFN field of the page-table entry (PTE) to reflect the new page’s address in memory, and retry the instruction.      When to swap out(swap page to disk)?High/Low WatermarkWhen the OS detects that there are more pages in memory than the high watermark (HW), a background process called the swap daemon  starts to evict pages from memory until the number of pages is less than the low watermark (LW). The daemon then sleeps until the HW is reached again.Invoke by ProcessThe swap can also be awoken by a process if there are no free pages available; Once the daemon has freed up some pages, it will re-awaken the original thread, which will then be able to page in the appropriate page and continue working.Performancement OptimizationMany systems, will cluster or group a number of pages and write them out to the swap partition all at once.Other useful commands  vmstatImplementing LRUProblemScanning a wide array of times to discover the least-recently-used page is expensive.Approximating LRU  When a page is referenced (read or written), the hardware sets the use bit to 1.  The system’s pages organized in a circle.  Initially, a clock hand points to any page.  When replacing a page, the OS checks if the use bit is 1 or 0.          If 1, page P was recently used. The usage bit for P is cleared (set to 0), and the clock hand is advanced one page (P + 1).      If the use bit is set to 0, the page is evicted (in the worst case, all pages have been recently used and we have now searched through the entire set of pages, clearing all the bits).      Dirty Pages  The clock algorithm may be altered to look for pages that are both unused and clean to evict first; if those aren’t found, then look for unused dirty pages, and so on.  Because if a page has been updated and is thus unclean, it must be evicted by writing it back to disk, which is costly.  The eviction is free if it has not been updated; the physical frame can simply be reused for other purposes without further I/O.  A modified/dirty bit should be included in the hardware to accommodate this behavior.Thrashing  Thrashing is used to describe the system is continuously paging because the memory demands of the operating processes simply outnumber the physical memory available.  The methods to address thrashing          not to execute a subset of them in the hopes that the pages of the reduced set of processes will fit in memory, allowing progress.      launch an out-of-memory killer; this daemon selects a memory-intensive process and kills it.      "
  },
  
  {
    "title": "Memory Space Management with paging",
    "url": "/posts/memory-space-management-with-paging/",
    "categories": "OS",
    "tags": "OS",
    "date": "2024-01-28 00:00:00 +0800",
    





    
    "snippet": "What is PagingPaging is another memory space management approach that dividing memory into fixed size of chuncks called pages. In contrast to segmentation, paging does not have external fragmentati...",
    "content": "What is PagingPaging is another memory space management approach that dividing memory into fixed size of chuncks called pages. In contrast to segmentation, paging does not have external fragmentation and support the abstraction of an address space effectively, regardless of how a process uses the address space since it won’t make assumptions about the way the heap and stack grow and how they are use.Address TranslationTo translate the virtual address the process generates:  We have to break the resulting virtual address into two parts:          The virtual page number (VPN) and      The offset within the page.        Using our VPN, we can now index our page table and find out which physical frame virtual page lives in.Page Table  The page table is a data structure that maps virtual addresses (or virtual page numbers) into physical addresses (physical frame numbers).  Each process has its own page table.Linear Page TableLinear Page table is an array.  VPN is an index of the array.  Each page table entry(PTE) contains PFN and other useful bits.The steps of address translation by hardware// Extract the VPN from the virtual addressVPN = (VirtualAddress &amp; VPN_MASK) &gt;&gt; SHIFT// Form the address of the page-table entry (PTE)PTEAddr = PTBR + (VPN * sizeof(PTE))// Fetch the PTEPTE = AccessMemory(PTEAddr)// Check if process can access the pageif (PTE.Valid == False)    RaiseException(SEGMENTATION_FAULT)else if (CanAccess(PTE.ProtectBits) == False)    RaiseException(PROTECTION_FAULT)else    // Access is OK: form physical address and fetch itoffset = VirtualAddress &amp; OFFSET_MASKPhysAddr = (PTE.PFN &lt;&lt; PFN_SHIFT) | offsetRegister = AccessMemory(PhysAddr)"
  },
  
  {
    "title": "Advanced Page Table",
    "url": "/posts/advanced-page-table/",
    "categories": "OS",
    "tags": "OS",
    "date": "2024-01-28 00:00:00 +0800",
    





    
    "snippet": "Problem of Linear Page Table  The size of the page table is too big.  We need to allocate the physical memory for the page table entries that there is no physical frame as well.Larger Page size  On...",
    "content": "Problem of Linear Page Table  The size of the page table is too big.  We need to allocate the physical memory for the page table entries that there is no physical frame as well.Larger Page size  One way to make the page table smaller is make the page size larger because it makes the number of page table entries to be decreased.  The major drawback to this strategy is that large pages result in waste within each page, a problem known as internal fragmentation. Applications allocate pages but only use small portions of each, and memory quickly fills up with these excessively large pages.Combining Paging and segmentation  The goal of combining paging and segmentation is removing the erroneous entries between the heap, and stack segments(unallocated pages between the stack and the heap are no longer needed) to reduce the size of page table.  Instead of one page table for the process’s whole address space, we have one page table for each segments(code, heap, and stack) so we have three page tables.  The base register holds the physical address of the segment’s page table.  The limits register indicates the page table’s end (i.e., how many valid pages it has).  During a context switch, these registers must be updated to reflect the new process’s page tables.  On a TLB miss (assuming a hardware-managed TLB), the hardware utilizes the segment bits (SN) to identify which base and bounds pair to use. The hardware then combines the physical address with the VPN to generate the page table entry (PTE) address.SN           = (VirtualAddress &amp; SEG_MASK) &gt;&gt; SN_SHIFTVPN          = (VirtualAddress &amp; VPN_MASK) &gt;&gt; VPN_SHIFTAddressOfPTE = Base[SN] + (VPN * sizeof(PTE))Virtual Address:|****SN*****|*****VPN******|*****OFFSET******|  The downsides of this approach:          Use segmentation so assumes a fixed address space utilization pattern; a huge but sparsely used heap      External fragmentation: page tables can now be any size (in multiples of PTEs). Finding memory space for them is more difficult.      Multi-Level Page Tables  This is approach to get rid of all those incorrect sections in the page table without using segmentation.  It first divide the page table into page-sized units.  If all page-table entries (PTEs) of that page are invalid, then do not assign that page of the page table at all.  So, it’s generally compact and supports sparse address spaces.  To know the memory location of the pages of the page table and their validities, it use the new data structure called page directory.  When the OS wants to allocate or grow a page table, it may simply grab the next free page-sized unit(the size is much smaller than the size of page table).  But, on a TLB miss, two loads from memory are necessary to acquire the proper translation information from the page table (one for the page directory, and one for the PTE itself).  For 2-level page table, to find out the page table entry, we can use base pointer + PD.index * sizeof(page directory) to find out the address of page-sized unit = PD.PFN , then we use PD.PFN + PT.index * sizeof(PTE) to find out the PTE address.|******** VPN **********************|****** Offset *********|| 13 | 12 | 11 | 10 | 9 | 8 | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0 ||**** PD.Index *****|** PT.index ***|Inverted Page Table  Rather than having many page tables (one for each system process), we have a single page table with an item for each physical page of the system.  This entry indicates which process uses this page and which virtual page of that process corresponds to this physical page.  A hash table is frequently added on top of the underlying structure to speed up lookups.Swapping the Page Tables to DiskSome systems store page tables in kernel virtual memory, allowing the system to swap portions of these page tables to disk if memory becomes scarce."
  },
  
  {
    "title": "Transaction Lookaside Buffer",
    "url": "/posts/Transaction-Loodaside-Buffer/",
    "categories": "OS",
    "tags": "OS",
    "date": "2024-01-28 00:00:00 +0800",
    





    
    "snippet": "Problem of PagingPaging needs an additional memory lookup in order to translate each virtual address, but it takes too long to obtain translation information before every instruction fetch, load, o...",
    "content": "Problem of PagingPaging needs an additional memory lookup in order to translate each virtual address, but it takes too long to obtain translation information before every instruction fetch, load, or store.What is Transaction Lookaside Buffer?In order to speed up the process of address translation, we use the hardware cache for the address translation. This cache is called Transaction Lookaside Buffer(TLF) which is part of the MMU.TLB EntryTLBs generally include 32 or 64 of TLB entries. A few are for the OS (using the G bit). The OS can set a wired register to instruct the hardware how many TLB slots to reserve for it. The OS uses these reserved mappings for code and data it needs to access during key moments when a TLB miss would be troublesome (e.g., in the TLB miss handler).TLB Entry:  Virtual Page Number (VPN)  Process ID (PID) or Address Space ID(ASID)  Page Frame Number (PFN)  V bit: Valid bit - indicates whether the entry has a valid translation or not  G bit: Global bit - If set, TLB does not check PID for translation  …What if TLB missHardware approach  If the virtual address does not in the TLB entries, we have to check the page table to find the translation.  The hardware has to know the exact location of the page tables in memory (through a page-table base register)OS approach  The hardware mimics an exception, pausing the current instruction stream, switching to kernel mode, and jumping to a trap handler.  Returning from a TLB miss-handling trap causes the hardware to retry the instruction, resulting in a TLB hit.  OS must avoid creating endless loops of TLB misses by keeping the TLB miss handler in physical memory.          reserve some TLB entries for always valid transaction. Or      unmapped and not subject to address translation.        OS can use any data structure it wants to implement the page table.Array Access ExampleA memory array of 10 4-byte integers.The page size is 16 bytes.            VPN      Offset 0-4      Offset 5-8      Offset 9-12      Offset 13-16                  VPN 0                           arr[0]              VPN 1      arr[1]      arr[2]      arr[3]      arr[4]              VPN 2      arr[5]      arr[6]      arr[7]      arr[8]              VPN 3      arr[9]                           The TLB hit rate the first time the array is accessedThe hit rate is 60%.  arr[0]: Miss (VPN 0 stored in TLB)  arr[1]: Miss (VPN 1 stored in TLB)  arr[2]: Hit (VPN 1)  arr[3]: Hit (VPN 1)  arr[4]: Hit (VPN 1)  arr[5]: Miss (VPN 2 stored in TLB)  arr[6]: Hit (VPN 2)  arr[7]: Hit (VPN 2)  arr[8]: Hit (VPN 2)  arr[9]: Miss (VPN 3 stored in TLB)The TLB hit rate the second time the array is accessedThe hit rate is 100% because VPN 0-4 stored in TLB already in the first time access.Context SwitchingHow to make sure the process does not reuse the TLB entries of the old process?  flushing: clears the TLB by setting all valid bits to 0.  ASID: TLBs include an address space identifier (ASID) field. The ASID is a Process ID (PID) with less bits. So the TLB can hold several processes’ translations.Two entries for two processes with two VPNs point to the same physical pageWhen two processes share a page (for example, a code page), this can occur. Also, it reduces memory overheads by reducing the number of physical pages needed.            VPN      PFN      ASID      Prot-bit      Valid-bit                  VPN 0      PFN 100      1      r-x      1              VPN 5      PFN 100      2      r-x      1      TLB Replacement Policy  LRU  Random"
  },
  
  {
    "title": "Memory Segmentation",
    "url": "/posts/memory-segmentation/",
    "categories": "OS",
    "tags": "OS",
    "date": "2024-01-27 00:00:00 +0800",
    





    
    "snippet": "What is Memory Segmentation?  It allows the segments(code, stack, heap) of the address space can be stored in different physical memory locations so that we do not need to allocate the physical mem...",
    "content": "What is Memory Segmentation?  It allows the segments(code, stack, heap) of the address space can be stored in different physical memory locations so that we do not need to allocate the physical memory for the “free” segment.  Each segment has its own base/bound registers.Which segment the virtual memory address related to?Explicit Approach  we divide the address space into segments based on the first few bits of the virtual address.  the top 2 most bits represent which segment the address corresponds to.  the other bits represent the offset.// get top 2 bits of 14-bit VASegment = (VirtualAddress &amp; SEG_MASK) &gt;&gt; SEG_SHIFT// now get offsetOffset  = VirtualAddress &amp; OFFSET_MASKif (Offset &gt;= Bounds[Segment])  RaiseException(PROTECTION_FAULT)else  PhysAddr = Base[Segment] + Offset  Register = AccessMemory(PhysAddr)Implicit Approach  determines the segment by examining the address.  If the address came from the program counter (i.e., an instruction fetch), it’s in the code segment.  If it came from the stack or base pointer, it’s in the stack segment.  All others are in the heap.How to handle stack  The difference between the stack and the other segment is it now grows backwards (towards lower addresses).  We need more hardware support so that the hardware knows the segment grows positive or negative from the base address.  We can get the correct physical address by base address + offset - max segment size.            Segment      Base      Size (Max 4K)      Grows Positive?                  Code      32K      2K      1              Heap      34K      3K      1              Code      28K      2K      0      Segmentation presents new challenges for the OS  The segment registers must be saved and restored becase each process has its own virtual address space for context switch.  Able to update the segment size register to the new (larger/smaller) size.  Able to find physical memory space for new address spaces.  handle external fragmentation: physical memory soon fills up with pockets of free space, making it impossible to assign new segments or expand old ones."
  },
  
  {
    "title": "Why Memory Virtualisation?",
    "url": "/posts/why-memory-virtualisation/",
    "categories": "OS",
    "tags": "OS",
    "date": "2024-01-26 00:00:00 +0800",
    





    
    "snippet": "  Transparency: user program dont need to worry about the physical memory address.  Protection: each process only can manipulate its own address space.",
    "content": "  Transparency: user program dont need to worry about the physical memory address.  Protection: each process only can manipulate its own address space."
  },
  
  {
    "title": "Multilevel Feedback Queue Scheduling",
    "url": "/posts/mlfq-scheduling-policy/",
    "categories": "OS",
    "tags": "OS",
    "date": "2024-01-25 00:00:00 +0800",
    





    
    "snippet": "Example RunEnter the number of processes: 3Enter duration for process 1: 10Process 1 enqueued in queue with time quantum 2Enter duration for process 2: 20Process 2 enqueued in queue with time quant...",
    "content": "Example RunEnter the number of processes: 3Enter duration for process 1: 10Process 1 enqueued in queue with time quantum 2Enter duration for process 2: 20Process 2 enqueued in queue with time quantum 2Enter duration for process 3: 30Process 3 enqueued in queue with time quantum 2Process 1 dequeued from queue with time quantum 2Process 1 is running in high priority queueProcess 1 enqueued in queue with time quantum 4Process 2 dequeued from queue with time quantum 2Process 2 is running in high priority queueProcess 2 enqueued in queue with time quantum 4Process 3 dequeued from queue with time quantum 2Process 3 is running in high priority queueProcess 3 enqueued in queue with time quantum 4Process 1 dequeued from queue with time quantum 4Process 1 is running in medium priority queueProcess 1 enqueued in queue with time quantum 8Process 2 dequeued from queue with time quantum 4Process 2 is running in medium priority queueProcess 2 enqueued in queue with time quantum 8Process 3 dequeued from queue with time quantum 4Process 3 is running in medium priority queueProcess 3 enqueued in queue with time quantum 8Process 1 dequeued from queue with time quantum 8Process 1 is running in low priority queueProcess 1 finished executionProcess 2 dequeued from queue with time quantum 8Process 2 is running in low priority queueProcess 2 enqueued in queue with time quantum 8Process 3 dequeued from queue with time quantum 8Process 3 is running in low priority queueProcess 3 enqueued in queue with time quantum 8Process 2 dequeued from queue with time quantum 8Process 2 is running in low priority queueProcess 2 finished executionProcess 3 dequeued from queue with time quantum 8Process 3 is running in low priority queueProcess 3 enqueued in queue with time quantum 8Process 3 dequeued from queue with time quantum 8Process 3 is running in low priority queueProcess 3 finished executionProcess Duration        Waiting Time    Turnaround Time1       10      12      222       20      24      443       30      30      60Example C Code#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;typedef struct {  int id;  int duration;  int remaining_time;  int waiting_time;  int turnaround_time;} Process;typedef struct {  Process* processes;  int front;  int rear;  int time_quantum;} Queue;void init_queues(Queue* high_q, Queue* mid_q, Queue* low_q){  high_q-&gt;processes = malloc(10*sizeof(Process));  mid_q-&gt;processes = malloc(10*sizeof(Process));  low_q-&gt;processes = malloc(10*sizeof(Process));  high_q-&gt;time_quantum = 2;  mid_q-&gt;time_quantum = 4;  low_q-&gt;time_quantum = 8;  high_q-&gt;front = -1;  mid_q-&gt;front = -1;  low_q-&gt;front = -1;  high_q-&gt;rear = -1;  mid_q-&gt;rear = -1;  low_q-&gt;rear = -1;}void enqueue(Queue* q, Process* p){  printf(\"Process %d enqueued in queue with time quantum %d\\n\", p-&gt;id, q-&gt;time_quantum);  if(q-&gt;front &gt; 9) { return; }  q-&gt;front += 1;  q-&gt;processes[q-&gt;front].id = p-&gt;id;  q-&gt;processes[q-&gt;front].duration = p-&gt;duration;  q-&gt;processes[q-&gt;front].remaining_time = p-&gt;remaining_time;  q-&gt;processes[q-&gt;front].waiting_time = p-&gt;waiting_time;  q-&gt;processes[q-&gt;front].turnaround_time = p-&gt;turnaround_time;}Process* dequeue(Queue* q){  if(q-&gt;rear &gt;= q-&gt;front) {     q-&gt;rear = -1;    q-&gt;front = -1;    return NULL;  }  q-&gt;rear += 1;  printf(\"Process %d dequeued from queue with time quantum %d\\n\", q-&gt;processes[q-&gt;rear].id, q-&gt;time_quantum);  return &amp;q-&gt;processes[q-&gt;rear];}void mlfq(Process* processes, int n, Queue* high_q, Queue* mid_q, Queue* low_q) {  Process* current_p;  int total_turnaround_time = 0;  if (n&lt;= 0) { return; }  while(1){    current_p = dequeue(high_q);    if(current_p != NULL){      printf(\"Process %d is running in high priority queue\\n\", current_p-&gt;id);      if (current_p-&gt;duration &gt; high_q-&gt;time_quantum) {        current_p-&gt;duration -= high_q-&gt;time_quantum;        enqueue(mid_q, current_p);        total_turnaround_time += high_q-&gt;time_quantum;      } else {        total_turnaround_time += current_p-&gt;duration;        processes[current_p-&gt;id-1].turnaround_time = total_turnaround_time;        processes[current_p-&gt;id-1].waiting_time = total_turnaround_time - processes[current_p-&gt;id-1].duration;      }      continue;    }    current_p = dequeue(mid_q);    if(current_p != NULL){      printf(\"Process %d is running in medium priority queue\\n\", current_p-&gt;id);      if (current_p-&gt;duration &gt; mid_q-&gt;time_quantum) {        current_p-&gt;duration -= mid_q-&gt;time_quantum;        enqueue(low_q, current_p);        total_turnaround_time += mid_q-&gt;time_quantum;      }       else {        total_turnaround_time += current_p-&gt;duration;        processes[current_p-&gt;id-1].turnaround_time = total_turnaround_time;        processes[current_p-&gt;id-1].waiting_time = total_turnaround_time - processes[current_p-&gt;id-1].duration;        printf(\"Process %d finished execution\\n\", current_p-&gt;id);      }      continue;    }    current_p = dequeue(low_q);      if(current_p != NULL){      printf(\"Process %d is running in low priority queue\\n\", current_p-&gt;id);      if (current_p-&gt;duration &gt; low_q-&gt;time_quantum) {        current_p-&gt;duration -= low_q-&gt;time_quantum;        enqueue(low_q, current_p);        total_turnaround_time += low_q-&gt;time_quantum;      }       else {        total_turnaround_time += current_p-&gt;duration;        processes[current_p-&gt;id-1].turnaround_time = total_turnaround_time;        processes[current_p-&gt;id-1].waiting_time = total_turnaround_time - processes[current_p-&gt;id-1].duration;        printf(\"Process %d finished execution\\n\", current_p-&gt;id);      }      continue;    }    printf(\"\\n\");    break;    }}int main(){  int n;  float total_waiting_time=0, total_turnaround_time=0;  Queue* high_q = malloc(sizeof(Queue));  Queue* mid_q = malloc(sizeof(Queue));  Queue* low_q = malloc(sizeof(Queue));    printf(\"Enter the number of processes: \");  scanf(\"%d\", &amp;n);  Process* processes = malloc(n*sizeof(Process));    init_queues(high_q, mid_q, low_q);  for(int i=0;i&lt;n;i++){    processes[i].id = i+1;    printf(\"Enter duration for process %d: \", i+1);    scanf(\"%d\", &amp;processes[i].duration);    enqueue(high_q, &amp;processes[i]);  }  mlfq(processes, n, high_q, mid_q, low_q);  printf(\"Process\\tDuration\\tWaiting Time\\tTurnaround Time\\n\");  for(int i=0; i&lt;n; i++){    printf(\"%d\\t%d\\t%d\\t%d\\n\", processes[i].id, processes[i].duration, processes[i].waiting_time, processes[i].turnaround_time);    total_turnaround_time += processes[i].turnaround_time;    total_waiting_time += processes[i].waiting_time;  }  free(processes);  return 0;}"
  }
  
]

